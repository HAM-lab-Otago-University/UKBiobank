{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "import skbold\n",
    "import textwrap\n",
    "from skbold.preproc import ConfoundRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_confounds = pd.read_csv('/ML_DATASETS/paper/brain/DTI/dti_confounds_final_set.csv')\n",
    "dti = pd.read_csv('/BRAIN/CSV_brain/dti_names_nona_names.csv')\n",
    "dti_confounds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_confounds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ConfoundRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: skbold\n",
    "def confound_regressor_skbold(features_train, features_test, confounds_train, confounds_test):\n",
    "    # Scale features (train and test sets)\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train_scaled = scaler_features.fit_transform(features_train)\n",
    "    features_test_scaled = scaler_features.transform(features_test)\n",
    "    \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train_scaled = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test_scaled = scaler_confounds.transform(confounds_test)\n",
    "\n",
    "    # Convert full sets into np.array\n",
    "    features_full_scaled_np = np.array(pd.concat([pd.DataFrame(features_train_scaled, columns = features_train.columns), pd.DataFrame(features_test_scaled, columns = features_test.columns)], axis=0))\n",
    "    confounds_full_scaled_np = np.array(pd.concat([pd.DataFrame(confounds_train_scaled, columns = confounds_train.columns), pd.DataFrame(confounds_test_scaled, columns = confounds_test.columns)], axis=0))\n",
    "    \n",
    "    # Define ConfoundRegressor on a FULL set (train and test)\n",
    "    cfr = ConfoundRegressor(confound=confounds_full_scaled_np, X=features_full_scaled_np)\n",
    "    features_train_corrected = cfr.fit_transform(features_train_scaled)\n",
    "    features_test_corrected = cfr.transform(features_test_scaled)\n",
    "\n",
    "\n",
    "    return features_train_corrected, features_test_corrected, features_train_scaled, features_test_scaled, scaler_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: Linear Model\n",
    "def confound_regressor(features_train, features_test, confounds_train, confounds_test):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train = scaler_features.fit_transform(features_train)\n",
    "    features_test = scaler_features.transform(features_test)\n",
    "        \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test = scaler_confounds.transform(confounds_test)\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(confounds_train, features_train)\n",
    "    features_train_pred = model.predict(confounds_train)\n",
    "    features_train_res = features_train - features_train_pred\n",
    "\n",
    "    features_test_pred = model.predict(confounds_test)\n",
    "    features_test_res = features_test - features_test_pred\n",
    "\n",
    "    return features_train_res, features_test_res, features_train, features_test, scaler_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract modalities from DTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FA**\n",
    "- TBSS\n",
    "\n",
    "fa_tbss_train\n",
    "\n",
    "fa_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "fa_prob_train\n",
    "\n",
    "fa_prob_test\n",
    "\n",
    "**MD**\n",
    "- TBSS\n",
    "\n",
    "md_tbss_train\n",
    "\n",
    "md_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "md_prob_train\n",
    "\n",
    "md_prob_test\n",
    "\n",
    "**L1**\n",
    "- TBSS\n",
    "\n",
    "l1_tbss_train\n",
    "\n",
    "l1_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "l1_prob_train\n",
    "\n",
    "l1_prob_test\n",
    "\n",
    "**L2**\n",
    "- TBSS\n",
    "\n",
    "l2_tbss_train\n",
    "\n",
    "l2_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "l2_prob_train\n",
    "\n",
    "l2_prob_test\n",
    "\n",
    "**L3**\n",
    "- TBSS\n",
    "\n",
    "l3_tbss_train\n",
    "\n",
    "l3_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "l3_prob_train\n",
    "\n",
    "l3_prob_test\n",
    "\n",
    "**MO**\n",
    "- TBSS\n",
    "\n",
    "mo_tbss_train\n",
    "\n",
    "mo_tbss_test\n",
    "\n",
    "- Probabilistic tractographisovf_tbssy_four_train_dti_trainy\n",
    "\n",
    "mo_prob_train\n",
    "\n",
    "mo_prob_test\n",
    "\n",
    "**OD**\n",
    "- TBSS\n",
    "\n",
    "od_tbss_train\n",
    "\n",
    "od_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "od_prob_train\n",
    "\n",
    "od_prob_test\n",
    "\n",
    "**ICVF**\n",
    "- TBSS\n",
    "\n",
    "icvf_tbss_train\n",
    "\n",
    "icvf_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "icvf_prob_train\n",
    "\n",
    "icvf_prob_test\n",
    "\n",
    "**ISOVF**\n",
    "- TBSS\n",
    "\n",
    "isovf_tbss_train\n",
    "\n",
    "isovf_tbss_test\n",
    "\n",
    "- Probabilistic tractography\n",
    "\n",
    "isovf_prob_train\n",
    "\n",
    "isovf_prob_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get columns containing individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns containing individual metrics\n",
    "## FA\n",
    "# TBSS\n",
    "fa_tbss = pd.concat([dti.filter(regex=\"Mean FA in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "fa_prob = pd.concat([dti.filter(regex=\"Weighted-mean FA in\"), dti['eid']], axis=1)\n",
    "\n",
    "## MD\n",
    "# TBSS\n",
    "md_tbss = pd.concat([dti.filter(regex=\"Mean MD in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "md_prob = pd.concat([dti.filter(regex=\"Weighted-mean MD in\"), dti['eid']], axis=1)\n",
    "\n",
    "## L1\n",
    "# TBSS\n",
    "l1_tbss = pd.concat([dti.filter(regex=\"Mean L1 in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "l1_prob = pd.concat([dti.filter(regex=\"Weighted-mean L1 in\"), dti['eid']], axis=1)\n",
    "\n",
    "\n",
    "## L2\n",
    "# TBSS\n",
    "l2_tbss = pd.concat([dti.filter(regex=\"Mean L2 in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "l2_prob = pd.concat([dti.filter(regex=\"Weighted-mean L2 in\"), dti['eid']], axis=1)\n",
    "\n",
    "\n",
    "## L3\n",
    "# TBSS\n",
    "l3_tbss = pd.concat([dti.filter(regex=\"Mean L3 in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "l3_prob = pd.concat([dti.filter(regex=\"Weighted-mean L3 in\"), dti['eid']], axis=1)\n",
    "\n",
    "\n",
    "## MO\n",
    "# TBSS\n",
    "mo_tbss = pd.concat([dti.filter(regex=\"Mean MO in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractographisovf_tbssy_four_train_dti_trainy\n",
    "mo_prob = pd.concat([dti.filter(regex=\"Weighted-mean MO in\"), dti['eid']], axis=1)\n",
    "\n",
    "\n",
    "## OD\n",
    "# TBSS\n",
    "od_tbss = pd.concat([dti.filter(regex=\"Mean OD in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "od_prob = pd.concat([dti.filter(regex=\"Weighted-mean OD in\"), dti['eid']], axis=1)\n",
    "\n",
    "\n",
    "## ICVF\n",
    "# TBSS\n",
    "icvf_tbss = pd.concat([dti.filter(regex=\"Mean ICVF in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "icvf_prob = pd.concat([dti.filter(regex=\"Weighted-mean ICVF in\"), dti['eid']], axis=1)\n",
    "\n",
    "## ISOVF\n",
    "# TBSS\n",
    "isovf_tbss = pd.concat([dti.filter(regex=\"Mean ISOVF in\"), dti['eid']], axis=1)\n",
    "# Probabilistic tractography\n",
    "isovf_prob = pd.concat([dti.filter(regex=\"Weighted-mean ISOVF in\"), dti['eid']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test set for g-factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = [\"fa_tbss\",  \"fa_prob\", \"md_tbss\", \"md_prob\",\n",
    "              \"l1_tbss\", \"l1_prob\", \"l2_tbss\", \"l2_prob\",\n",
    "              \"l3_tbss\", \"l3_prob\", \"mo_tbss\", \"mo_prob\",\n",
    "              \"od_tbss\", \"od_prob\", \"icvf_tbss\", \"icvf_prob\",\n",
    "              \"isovf_tbss\", \"isovf_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    \n",
    "# Match confounds to MRI\n",
    "for fold in folds:\n",
    "    train_id = pd.read_csv(f'/Cog-Ment/g_factor_5_folds_python/fold_{fold}/train_id_fold_{fold}.csv')\n",
    "    test_id = pd.read_csv(f'/Cog-Ment/g_factor_5_folds_python/fold_{fold}/test_id_fold_{fold}.csv')\n",
    "        \n",
    "    # Match g-factor to ID\n",
    "    g_train_full = pd.concat([pd.read_csv(f'/Cog-Ment/R/g_factor_5_folds/fold_{fold}/g_train_{fold}.csv'), train_id.astype(int)], axis=1).to_csv(f'/g_factor/g_train_with_id_fold_{fold}.csv', index=False)\n",
    "    g_test_full = pd.concat([pd.read_csv(f'/Cog-Ment/R/g_factor_5_folds/fold_{fold}/g_test_{fold}.csv'), test_id.astype(int)], axis=1).to_csv(f'/g_factor/g_test_with_id_fold_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds = pd.read_csv('/ML_DATASETS/paper/brain/DTI/dti_confounds_final_set.csv')\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "############## 1\n",
    "print('Matching mental health data to cognitive data')\n",
    "seed = 42\n",
    "\n",
    "for modality in modalities:\n",
    "\n",
    "    modality_data = globals()[modality]\n",
    "\n",
    "    pls_result = {}\n",
    "\n",
    "    folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    \n",
    "    # Match confounds to MRI\n",
    "    conf_to_brain_match = pd.merge(confounds, modality_data['eid'], on='eid')\n",
    "    brain_to_conf_match = pd.merge(conf_to_brain_match['eid'], modality_data, on='eid')\n",
    "\n",
    "    for fold in folds:\n",
    "        train_id = pd.read_csv(f'/Cog-Ment/g_factor_5_folds_python/fold_{fold}/train_id_fold_{fold}.csv')\n",
    "        test_id = pd.read_csv(f'/Cog-Ment/g_factor_5_folds_python/fold_{fold}/test_id_fold_{fold}.csv')\n",
    "        \n",
    "        # Upload g-factor with ID\n",
    "        g_train_full = pd.read_csv(f'/g_factor/g_train_with_id_fold_{fold}.csv')\n",
    "        g_test_full = pd.read_csv(f'/g_factor/g_test_with_id_fold_{fold}.csv')\n",
    "\n",
    "        # Match brain data to cognitive data\n",
    "        brain_train, brain_test, brain_train_id, brain_test_id = pd.merge(brain_to_conf_match, train_id, on='eid').drop(columns=['eid']), pd.merge(brain_to_conf_match, test_id, on='eid').drop(columns=['eid']), pd.merge(brain_to_conf_match, train_id, on='eid')['eid'], pd.merge(brain_to_conf_match, test_id, on='eid')['eid']\n",
    "\n",
    "        brain_train.to_csv(f'/brain/dti/dti_idp/{modality}_train_fold_{fold}.csv', index=False)\n",
    "        brain_test.to_csv(f'/brain/dti/dti_idp/{modality}_test_fold_{fold}.csv', index=False)\n",
    "        brain_train_id.to_csv(f'/brain/dti/dti_idp/{modality}_train_id_fold_{fold}.csv', index=False)\n",
    "        brain_test_id.to_csv(f'/brain/dti/dti_idp/{modality}_test_id_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        ############## 2\n",
    "        print(f'Matching confounds to {modality}')\n",
    "        \n",
    "        # Match confounds to MRI\n",
    "        brain_conf_train, brain_conf_test = pd.merge(conf_to_brain_match, brain_train_id, on='eid').drop(columns=['eid']), pd.merge(conf_to_brain_match, brain_test_id, on='eid').drop(columns=['eid'])\n",
    "        brain_conf_train.to_csv(f'/brain/dti/dti_idp/{modality}_conf_train_fold_{fold}.csv', index=False)\n",
    "        brain_conf_test.to_csv(f'/brain/dti/dti_idp/{modality}_conf_test_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        ############## 3\n",
    "        print(f'Matching g-factor to {modality}')\n",
    "        \n",
    "        # Match g-factor back to brain data\n",
    "        g_train, g_test, g_train_id, g_test_id = pd.merge(g_train_full, brain_train_id, on='eid').drop(columns=['eid']), pd.merge(g_test_full, brain_test_id, on='eid').drop(columns=['eid']), pd.merge(g_train_full, brain_train_id, on='eid')['eid'], pd.merge(g_test_full, brain_test_id, on='eid')['eid']\n",
    "        g_train.to_csv(f'/brain/dti/dti_idp/g_train_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        g_test.to_csv(f'/brain/dti/dti_idp/g_test_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        ############## 4\n",
    "        print('Applying ConfoundRegressor')\n",
    "        \n",
    "        # Apply ConfoundRegressor\n",
    "        features_train_corr, features_test_corr, features_train_scaled, features_test_scaled, scaler_features = confound_regressor_skbold(brain_train, brain_test, brain_conf_train, brain_conf_test)\n",
    "        pd.DataFrame(features_train_corr, columns = brain_train.columns).to_csv(f'/brain/dti/dti_idp/fold_{fold}/{modality}_train_corr_{fold}.csv', index=False)\n",
    "        pd.DataFrame(features_test_corr, columns = brain_train.columns).to_csv(f'/brain/dti/dti_idp/fold_{fold}/{modality}_test_corr_{fold}.csv', index=False)\n",
    "\n",
    "        pd.DataFrame(features_train_scaled, columns = brain_train.columns).to_csv(f'/brain/dti/dti_idp/fold_{fold}/{modality}_train_scaled_{fold}.csv', index=False)\n",
    "        pd.DataFrame(features_test_scaled, columns = brain_train.columns).to_csv(f'/brain/dti/dti_idp/fold_{fold}/{modality}_test_scaled_{fold}.csv', index=False)\n",
    "        \n",
    "        with open(f'/brain/dti/dti_idp/fold_{fold}/scaler_features_{modality}_fold_{fold}.pkl', \"wb\") as f:\n",
    "            pickle.dump(scaler_features, f)\n",
    "\n",
    "        # Initiate and run PLS\n",
    "        parameters = {'n_components': range(1, features_train_corr.shape[1]+1)}\n",
    "        pls = PLSRegression()\n",
    "        model = GridSearchCV(pls, parameters, scoring = 'neg_mean_absolute_error', cv=KFold(10, shuffle = True, random_state=seed), verbose=4, n_jobs = 8)\n",
    "        \n",
    "        \n",
    "        print(\"Fitting PLS\")\n",
    "        model.fit(features_train_corr, np.array(g_train)) #np.array(features_train_corr)\n",
    "        \n",
    "        print(f'Model parameters for fold {fold}:', model.cv_results_['params'])\n",
    "        print(f'Mean test score for fold {fold}:', model.cv_results_['mean_test_score'])\n",
    "        print(f'Rank test score for fold {fold}:', model.cv_results_['rank_test_score'])\n",
    "        print(model)\n",
    "        \n",
    "        print(f'Saving PLS model for {modality} fold {fold}')\n",
    "        with open(f'/brain/dti/dti_idp/fold_{fold}/models/{modality}_model_fold_{fold}.pkl', \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "            \n",
    "        print(f'Best params in fold {fold} = ', model.best_params_)\n",
    "        print(f'Best score (neg_mean_absolute_error) in fold {fold} = ', model.best_score_)\n",
    "            \n",
    "        # Predict the values\n",
    "        print(f'Predicting & saving g_test for {modality} fold {fold}')\n",
    "        g_pred_test = model.predict(np.array(features_test_corr))\n",
    "        pd.DataFrame(g_pred_test, columns=['g predicted test']).to_csv(f'/brain/dti/dti_idp/fold_{fold}/g_pred/{modality}_g_pred_test_fold_{fold}.csv')\n",
    "\n",
    "        g_pred_test_with_id = pd.concat([g_test_id.astype(int), pd.DataFrame(g_pred_test, columns=['g predicted test'])], axis=1).to_csv(f'/brain/dti/dti_idp/fold_{fold}/g_pred/{modality}_g_pred_test_id_fold_{fold}.csv')\n",
    "\n",
    "        \n",
    "        print(f'Predicting & saving g_train for {modality} fold {fold}')\n",
    "        g_pred_train = model.predict(np.array(features_train_corr))\n",
    "        pd.DataFrame(g_pred_train, columns=['g predicted train']).to_csv(f'/brain/dti/dti_idp/fold_{fold}/g_pred/{modality}_g_pred_train_fold_{fold}.csv')\n",
    "        \n",
    "\n",
    "        g_pred_train_with_id = pd.concat([g_train_id.astype(int), pd.DataFrame(g_pred_train, columns=['g predicted train'])], axis=1).to_csv(f'/brain/dti/dti_idp/fold_{fold}/g_pred/{modality}_g_pred_train_id_fold_{fold}.csv')\n",
    "\n",
    "            \n",
    "        print(f\"Fold = {fold}\")\n",
    "        print(\"----------\")\n",
    "        print(\"MSE = \", mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"MAE = \", mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"R2 = \", r2_score(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"Pearson's r = \", pearsonr(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"----------\")\n",
    "            \n",
    "        pls_result['fold'] = fold\n",
    "        pls_result['modality'] = modality\n",
    "        pls_result['n_components'] = model.best_params_\n",
    "        pls_result['MSE'] = mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['MAE'] = mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['R2'] = r2_score(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['Pearson r'] = pearsonr(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "            \n",
    "        with open(f'/brain/dti/dti_idp/fold_{fold}/models/{modality}_fold_{fold}_PLS_result.csv', 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=pls_result.keys())\n",
    "            writer.writerow(pls_result)\n",
    "            \n",
    "        pls_result.clear()\n",
    "        \n",
    "        corr, pval = stats.pearsonr(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        r2 = r2_score(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        mse = mean_squared_error(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        pd.DataFrame([modality, fold, corr, pval, r2, mse, model.best_params_], index=['Modality', 'Fold', 'Correlation', 'P-value', 'R2', 'MSE', 'n components'], columns=['Values']).to_csv(f'/brain/dti/dti_idp/fold_{fold}/models/{modality}_fold_{fold}_full_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decondounded and original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original values vs residuals (deconfounded)\n",
    "features_train_corr_check = pd.read_csv('/brain/dti/dti_idp/fold_0/fa_tbss_train_corr_0.csv')\n",
    "brain_train_orig_check =  pd.read_csv('/brain/dti/dti_idp/fa_tbss_train_fold_0.csv')\n",
    "cols = brain_train_orig_check.columns[:25]\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(cols):\n",
    "    sns.regplot(x=brain_train_orig_check[col], y=features_train_corr_check[col], line_kws={'color': 'red', 'linewidth': 1}, scatter=True, scatter_kws = {\"color\": \".7\", \"s\": 10, \"alpha\": 0.2}, ax=axes[i])\n",
    "    axes[i].set_xlabel(\"\\n\".join(textwrap.wrap(cols[i] + ' (Original)', 20)), fontsize=7)\n",
    "    axes[i].set_ylabel(\"\\n\".join(textwrap.wrap(cols[i] + ' (Residuals)', 20)), fontsize=7)\n",
    "    plt.yticks(fontsize=6)\n",
    "    plt.xticks(fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge results across 5 folds and average R2 and r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload individual pls results and merge them into one table\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "five_folds = []\n",
    "for modality in modalities:\n",
    "    for fold in folds:\n",
    "        pls = pd.read_csv(f'/brain/dti/dti_idp/fold_{fold}/models/{modality}_fold_{fold}_PLS_result.csv', header=None)\n",
    "        pls.columns = ['Fold', 'Modality', 'n components', 'MSE', 'MAE', 'R2', 'Pearson r']\n",
    "        five_folds.append(pls)\n",
    "        five_folds_all_modalities = pd.concat(five_folds, ignore_index=False)\n",
    "\n",
    "five_folds_all_modalities['Pearson r'] = five_folds_all_modalities['Pearson r'].astype(str).str.replace(r'PearsonRResult\\(statistic=|pvalue=|\\)', '', regex=True)\n",
    "five_folds_all_modalities[['Pearson r', 'p-value']] = five_folds_all_modalities['Pearson r'].str.split(',', expand=True).astype(float).round(decimals=3)\n",
    "five_folds_all_modalities = five_folds_all_modalities.round(decimals=3)\n",
    "#five_folds_all_modalities.to_csv('/brain/dti/pls_5_folds_all_modalities.csv', index=False)\n",
    "five_folds_all_modalities['n components'] = five_folds_all_modalities['n components'].astype(str).str.replace(r\"{'n_components':\", '', regex=True)\n",
    "five_folds_all_modalities['n components'] = five_folds_all_modalities['n components'].astype(str).str.replace(r\"}\", '', regex=True)\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(five_folds_all_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across folds\n",
    "five_folds_all_modalities_mean = five_folds_all_modalities[['R2', 'Pearson r', 'Modality', 'MSE', 'MAE']]\n",
    "five_folds_all_modalities_mean.groupby(['Modality']).mean().sort_values(by='R2', ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PLS loadings (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "simplefilter(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for modality in modalities:\n",
    "    for fold in folds:\n",
    "        with open(f'/brain/dti/dti_idp/fold_{fold}/models/{modality}_model_fold_{fold}.pkl', \"rb\") as f:\n",
    "              model = pickle.load(f)\n",
    "              r2_sum = 0\n",
    "              r2_vector = np.empty(model.best_estimator_.n_components)\n",
    "              \n",
    "              g_train = pd.read_csv(f'/brain/dti/dti_idp/g_train_{modality}_matched_fold_{fold}.csv')\n",
    "        \n",
    "        for i in range(0,model.best_estimator_.n_components):\n",
    "          Y_pred = np.dot(model.best_estimator_.x_scores_[:,i].reshape(-1,1), model.best_estimator_.y_loadings_[:,i].reshape(-1,1).T) * g_train.values.std(axis=0, ddof=1) + g_train['g'].mean(axis=0)\n",
    "          r2_sum += r2_score(g_train.values,Y_pred)\n",
    "          print('R2 for %d component: %g' %(i+1,r2_score(g_train.values, Y_pred)))\n",
    "          r2_vector[i] = r2_score(g_train.values,Y_pred)\n",
    "          \n",
    "          x_loading_by_r2 = model.best_estimator_.x_loadings_ *  r2_vector\n",
    "          x_loading_by_r2_scaled = stats.zscore(model.best_estimator_.x_loadings_) *  r2_vector\n",
    "          weighted_x_loading = np.sum(x_loading_by_r2, axis=1)\n",
    "          weighted_x_loading_scaled = np.sum(x_loading_by_r2_scaled, axis=1)\n",
    "        print(f'R2 for all components in {modality} fold {fold}: %g' %r2_sum.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all weighted loadings into one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "simplefilter(\"ignore\", category=InconsistentVersionWarning)\n",
    "###############################\n",
    "weighted_x_loadings = {}\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for modality in modalities:\n",
    "    weighted_x_loadings[modality] = {}\n",
    "    for fold in folds:\n",
    "        with open(f'/brain/dti/dti_idp/fold_{fold}/models/{modality}_model_fold_{fold}.pkl', \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        g_train = pd.read_csv(f'/brain/dti/dti_idp/g_train_{modality}_matched_fold_{fold}.csv')['g']\n",
    "        \n",
    "        # Initialize variables for R2 calculation\n",
    "        r2_vector = np.empty(model.best_estimator_.n_components)\n",
    "        \n",
    "        # Calculate R2 for each component\n",
    "        for i in range(model.best_estimator_.n_components):\n",
    "            Y_pred = np.dot(model.best_estimator_.x_scores_[:, i].reshape(-1, 1),\n",
    "                            model.best_estimator_.y_loadings_[:, i].reshape(-1, 1).T) * g_train.values.std(ddof=1) + g_train.mean()\n",
    "            r2_vector[i] = r2_score(g_train.values, Y_pred)\n",
    "        \n",
    "        # Calculate weighted x_loadings\n",
    "        x_loading_by_r2 = model.best_estimator_.x_loadings_ * r2_vector\n",
    "        x_loading_by_r2_scaled = stats.zscore(model.best_estimator_.x_loadings_) * r2_vector\n",
    "        weighted_x_loading = np.sum(x_loading_by_r2, axis=1)\n",
    "        weighted_x_loading_scaled = np.sum(x_loading_by_r2_scaled, axis=1)\n",
    "        \n",
    "        # Store the weighted_x_loadings in the dictionary\n",
    "        weighted_x_loadings[modality][fold] = {\n",
    "            'weighted_x_loading': weighted_x_loading,\n",
    "            'weighted_x_loading_scaled': weighted_x_loading_scaled\n",
    "        }\n",
    "\n",
    "# Print the accumulated weighted_x_loadings\n",
    "for modality in weighted_x_loadings:\n",
    "    for fold in weighted_x_loadings[modality]:\n",
    "        print(f\"Modality: {modality}, Fold: {fold}, Weighted X Loading: {weighted_x_loadings[modality][fold]['weighted_x_loading']}\")\n",
    "        print(f\"Modality: {modality}, Fold: {fold}, Weighted X Loading Scaled: {weighted_x_loadings[modality][fold]['weighted_x_loading_scaled']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract weighted loadings for each modality in the form of a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_dataframes = {modality: globals()[modality] for modality in modalities}\n",
    "weighted_x_loading_scaled_dfs = {}\n",
    "for modality in weighted_x_loadings:\n",
    "    dfs = []\n",
    "    for fold in weighted_x_loadings[modality]:\n",
    "        # Get the weighted_x_loading_scaled for the current fold\n",
    "        weighted_x_loading_scaled = weighted_x_loadings[modality][fold]['weighted_x_loading_scaled']\n",
    "        # Get feature names\n",
    "        feature_names = modality_dataframes[modality].drop(columns='eid').columns.tolist()\n",
    "        # Create a DataFrame with feature names and weighted_x_loading_scaled\n",
    "        df = pd.concat([pd.DataFrame(feature_names, columns=['Features']),\n",
    "                        pd.DataFrame(weighted_x_loading_scaled, columns=['Loadings'])], axis=1)\n",
    "        dfs.append(df)\n",
    "    # Concatenate all fold DataFrames for the current modality\n",
    "    weighted_x_loading_scaled_dfs[modality] = pd.concat(dfs, keys=folds, names=['Fold', 'Index'])\n",
    "\n",
    "# Print the DataFrames for each modality\n",
    "for modality in weighted_x_loading_scaled_dfs:\n",
    "    print(f\"DataFrame for {modality}:\")\n",
    "    print(weighted_x_loading_scaled_dfs[modality])\n",
    "\n",
    "# Save modality_dataframes\n",
    "with open(f'/brain/dti/dti_idp/dti_idp_modality_dataframes.pkl', \"wb\") as f:\n",
    "    pickle.dump(modality_dataframes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(weighted_x_loading_scaled_dfs[modality]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary\n",
    "with open(f'/brain/dti/dti_idp/dti_idp_weighted_x_loading_scaled_dfs.pkl', \"wb\") as f:\n",
    "    pickle.dump(weighted_x_loading_scaled_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modality in weighted_x_loading_scaled_dfs:\n",
    "    fold_0 = weighted_x_loading_scaled_dfs[modality].loc['0'].reset_index().drop(columns='Index')\n",
    "    \n",
    "    negative_load = fold_0[fold_0['Loadings'] < 0].sort_values(by='Loadings', ascending=False).reset_index(drop=True)\n",
    "    positive_load = fold_0[fold_0['Loadings'] >= 0].sort_values(by='Loadings', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    max_abs_value = max(abs(negative_load['Loadings'].min()), positive_load['Loadings'].max())\n",
    "\n",
    "    fig, ax_neg = plt.subplots(figsize=(12, 20))\n",
    "\n",
    "    negative_load['Loadings'].plot.barh(ax=ax_neg, color='purple')\n",
    "    ax_neg.set_xlim(-max_abs_value - 0.1, 0)\n",
    "    ax_neg.tick_params(axis='y', labelcolor='purple', labelleft=False, labelright=True, labelsize=15)\n",
    "    ax_neg.tick_params(axis='x', colors='purple', labelsize=15)\n",
    "    ax_neg.set_yticks(range(len(negative_load)))\n",
    "    ax_neg.set_yticklabels(negative_load['Features']) \n",
    "    ax_neg.yaxis.set_ticks_position('none')\n",
    "    \n",
    "    ax_pos = ax_neg.twiny().twinx()  # Create a secondary x-axis and y-axis\n",
    "    positive_load['Loadings'].plot.barh(ax=ax_pos, color='crimson')\n",
    "    ax_pos.set_xlim(0, max_abs_value + 0.1)\n",
    "    ax_pos.tick_params(axis='y', labelcolor='crimson', right=False, labelleft=True, labelsize=15)\n",
    "    ax_pos.set_yticks(range(len(positive_load)))\n",
    "    ax_pos.set_yticklabels(positive_load['Features'])\n",
    "    ax_pos.yaxis.set_ticks_position('none')\n",
    "    ax_pos.set_xticklabels([])\n",
    "    ax_pos.set_xticks([])\n",
    "    ax_pos.invert_yaxis()\n",
    "\n",
    "    # Set top x-axis labels\n",
    "    ax_pos_twin = ax_neg.twiny()\n",
    "    ax_pos_twin.set_xlim(0, max_abs_value + 0.1)\n",
    "    ax_pos_twin.tick_params(axis='x', labelcolor='crimson', labelsize=15)\n",
    "    \n",
    "    # Remove the positive loading labels from the right side\n",
    "    ax_pos.tick_params(axis='y', labelright=False)\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukbiobank_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
