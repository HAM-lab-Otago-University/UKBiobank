{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import warnings\n",
    "import skbold\n",
    "from skbold.preproc import ConfoundRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import ukbiobank.utils.utils\n",
    "from ukbiobank.utils import loadCsv\n",
    "from ukbiobank.utils import addFields\n",
    "from ukbiobank.utils.utils import fieldIdsToNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ConfoundRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: skbold\n",
    "def confound_regressor_skbold(features_train, features_test, confounds_train, confounds_test):\n",
    "    # Scale features (train and test sets)\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train_scaled = scaler_features.fit_transform(features_train)\n",
    "    features_test_scaled = scaler_features.transform(features_test)\n",
    "    \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train_scaled = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test_scaled = scaler_confounds.transform(confounds_test)\n",
    "\n",
    "    # Convert full sets into np.array\n",
    "    features_full_scaled_np = np.array(pd.concat([pd.DataFrame(features_train_scaled, columns = features_train.columns), pd.DataFrame(features_test_scaled, columns = features_test.columns)], axis=0))\n",
    "    confounds_full_scaled_np = np.array(pd.concat([pd.DataFrame(confounds_train_scaled, columns = confounds_train.columns), pd.DataFrame(confounds_test_scaled, columns = confounds_test.columns)], axis=0))\n",
    "    \n",
    "    # Define ConfoundRegressor on a FULL set (train and test)\n",
    "    cfr = ConfoundRegressor(confound=confounds_full_scaled_np, X=features_full_scaled_np)\n",
    "    features_train_corrected = cfr.fit_transform(features_train_scaled)\n",
    "    features_test_corrected = cfr.transform(features_test_scaled)\n",
    "\n",
    "\n",
    "    return features_train_corrected, features_test_corrected, features_train_scaled, features_test_scaled, scaler_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: Linear Model\n",
    "def confound_regressor(features_train, features_test, confounds_train, confounds_test):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train = scaler_features.fit_transform(features_train)\n",
    "    features_test = scaler_features.transform(features_test)\n",
    "        \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test = scaler_confounds.transform(confounds_test)\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(confounds_train, features_train)\n",
    "    features_train_pred = model.predict(confounds_train)\n",
    "    features_train_res = features_train - features_train_pred\n",
    "\n",
    "    features_test_pred = model.predict(confounds_test)\n",
    "    features_test_res = features_test - features_test_pred\n",
    "\n",
    "    return features_train_res, features_test_res, features_train, features_test, scaler_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data: T2w MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/ukbbdata/ukb.csv'\n",
    "ukb = ukbiobank.ukbio(ukb_csv=csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2\n",
    "\n",
    "\n",
    "- 24486\tTotal volume of deep white matter hyperintensities\n",
    "- 24485\tTotal volume of peri-ventricular white matter hyperintensities\n",
    "- 25781\tTotal volume of white matter hyperintensities (from T1 and T2_FLAIR images)\n",
    "\n",
    "#### Confounds:\n",
    "\n",
    "- 25926\tIntensity scaling for T2_FLAIR\n",
    "- 25736\tDiscrepancy between T2 FLAIR brain image and T1 brain image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_t2 = ukbiobank.utils.utils.loadCsv(ukbio=ukb, fields=['eid',\n",
    "24486,\t#Total volume of deep white matter hyperintensities - T2\n",
    "24485,\t#Total volume of peri-ventricular white matter hyperintensities - T2\n",
    "25781,\t#Total volume of white matter hyperintensities (from T1 and T2_FLAIR images) - T2\n",
    "], instance=2)\n",
    "add_t2_names = ukbiobank.utils.utils.fieldIdsToNames(ukbio=ukb, df=df_add_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get columns that could not be uploaded with ukb 'loadCsv' using 'usecols'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_t2 = pd.read_csv('/ukbbdata/ukb.csv', usecols=['eid','24486-2.0', '24485-2.0', '25781-2.0'])\n",
    "add_t2.columns = ['eid','Total volume of deep white matter hyperintensities from T2', 'Total volume of peri-ventricular white matter hyperintensities from T2', 'Total volume of white matter hyperintensities (from T1 and T2_FLAIR images)']\n",
    "add_t2_nona = add_t2.dropna(axis=0).reset_index(drop=True)\n",
    "add_t2_nona.to_csv('/ML_DATASETS/Brain/T1/additional_(t1-t2-fMRI)-T2_names_nona.csv', index=False)\n",
    "add_t2 = pd.read_csv('/ML_DATASETS/Brain/T1/additional_(t1-t2-fMRI)-T2_names_nona.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload confounds for T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Head size: 25000\n",
    "- Site: 54\n",
    "- Acquisition date: 53\n",
    "- STRUCT MOTION: 24419\n",
    "- Discrepancy between T1 brain image and standard-space brain template (linearly-aligned): 25731\n",
    "- Discrepancy between T1 brain image and standard-space brain template (nonlinearly-aligned): 25732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_t2_conf = ukbiobank.utils.utils.loadCsv(ukbio=ukb, fields=['eid',\n",
    "25926,\n",
    "25736,\n",
    "25000,\n",
    "54,\n",
    "53,\n",
    "24419], instance=2)\n",
    "add_t2_conf_names = ukbiobank.utils.utils.fieldIdsToNames(ukbio=ukb, df=df_add_t2_conf)\n",
    "add_t2_conf_names_nona = add_t2_conf_names.dropna(axis=0)\n",
    "add_t2_conf_names_nona.columns = add_t2_conf_names_nona.columns.str.replace('-2.0', '')\n",
    "add_t2_conf_names_nona.to_csv(r'/ML_DATASETS/Brain/T1/additional_(t1-t2-fMRI)-T2_CONF_RAW.csv', index=False)\n",
    "add_t2_conf_names_nona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 'Structural motion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_t2_conf = pd.read_csv('/ukbbdata/ukb.csv', usecols=['eid', '24419-2.0'])\n",
    "add_t2_conf.columns = ['eid', 'Struct.motion']\n",
    "add_t2_conf_nona = add_t2_conf.dropna(axis=0).reset_index(drop=True)\n",
    "add_t2_conf_full = pd.merge(add_t2_conf_names_nona, add_t2_conf_nona, on='eid')\n",
    "add_t2_conf_full.to_csv('/ML_DATASETS/Brain/T1/additional_(t1-t2-fMRI)-T2_CONF_RAW_FULL.csv', index=False)\n",
    "add_t2_conf_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date & site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date\n",
    "add_t2_conf_full = pd.read_csv('/ML_DATASETS/Brain/T1/additional_(t1-t2-fMRI)-T2_CONF_RAW_FULL.csv')\n",
    "import datetime\n",
    "add_t2_conf_unix = add_t2_conf_full.copy()\n",
    "add_t2_conf_unix['Date of attending assessment centre'] = pd.to_datetime(add_t2_conf_unix['Date of attending assessment centre'], format=\"%Y-%m-%d\")  #\"%m/%d/%Y\")\n",
    "add_t2_conf_unix['Date of attending assessment centre'] = add_t2_conf_unix['Date of attending assessment centre'].apply(datetime.datetime.timestamp)\n",
    "# Round values\n",
    "add_t2_conf_unix['Date of attending assessment centre'] = add_t2_conf_unix['Date of attending assessment centre'].apply(int)\n",
    "print(add_t2_conf_unix['Date of attending assessment centre'])\n",
    "# Dummy encode site\n",
    "add_t2_conf_unix_dummy = pd.get_dummies(add_t2_conf_unix, columns=['UK Biobank assessment centre'], dtype=int)\n",
    "add_t2_conf_unix_dummy = add_t2_conf_unix_dummy.drop(columns=['T2-FLAIR used (in addition to T1) to run FreeSurfer', 'Discrepancy between tfMRI brain image and T1 brain image'])\n",
    "add_t2_conf_unix_dummy.to_csv('/ML_DATASETS/Brain/t1_t2_tfmri/add_t2_conf_unix_dummy.csv', index=False)\n",
    "t2_conf = pd.read_csv('/media/hcs-sci-psy-narun/ML_DATASETS/Brain/t1_t2_tfmri/add_t2_conf_unix_dummy.csv')\n",
    "t2_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data: whole-brain T1w MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_t1 = pd.read_csv('/ML_DATASETS/Brain/T1/additional_(t1-t2-fMRI)-T1_names_nona.csv')\n",
    "add_t1_conf = pd.read_csv('/ML_DATASETS/Brain/T1/struct_conf_full_dummy.csv')\n",
    "add_t1_conf.to_csv('/PLS/brain/additional/orig/add_t1_conf.csv', index=False)\n",
    "add_t1.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_t2_combined = add_t1.merge(add_t2, on = 'eid')\n",
    "t1_t2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_t2_conf_combined = pd.read_csv('/PLS/brain/additional/orig/add_t1_conf.csv').merge(t2_conf[['Intensity scaling for T2_FLAIR', 'Discrepancy between T2 FLAIR brain image and T1 brain image', 'eid']], on='eid')\n",
    "t1_t2_conf_combined.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS on uncategorized (whole-brain) T1w and T2w MRI data combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "############## 1\n",
    "print('Started: Uploading')\n",
    "seed = 42\n",
    "file = t1_t2_combined.copy()\n",
    "confound = t1_t2_conf_combined.copy()\n",
    "\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"] \n",
    "pls_result = {}\n",
    "    \n",
    "# Match confounds to MRI\n",
    "print('Started: Match confounds to brain data')\n",
    "conf_to_brain_match = pd.merge(confound, file['eid'], on='eid')\n",
    "brain_to_conf_match = pd.merge(conf_to_brain_match['eid'], file, on='eid')\n",
    "\n",
    "for fold in folds:\n",
    "# Upload g-factor with ID\n",
    "    g_train_full = pd.read_csv(f'/PLS/g_factor/g_train_with_id_fold_{fold}.csv')\n",
    "    g_test_full = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv')\n",
    "\n",
    "    \n",
    "    # Brain data to cognitive data\n",
    "    brain_train, brain_test, brain_train_id, brain_test_id = pd.merge(brain_to_conf_match, g_train_full['eid'], on='eid').drop(columns=['eid']), pd.merge(brain_to_conf_match, g_test_full['eid'], on='eid').drop(columns=['eid']), pd.merge(brain_to_conf_match, g_train_full['eid'], on='eid')['eid'], pd.merge(brain_to_conf_match, g_test_full['eid'], on='eid')['eid']\n",
    "\n",
    "    brain_train.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_train_fold_{fold}.csv', index=False)\n",
    "    brain_test.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_test_fold_{fold}.csv', index=False)\n",
    "    brain_train_id.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_train_id_fold_{fold}.csv', index=False)\n",
    "    brain_test_id.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_test_id_fold_{fold}.csv', index=False)\n",
    "        \n",
    "    ############## 2\n",
    "    print(f'Matching confounds to T1_T2_whole_brain fold {fold}')\n",
    "        \n",
    "    # Match confounds to MRI\n",
    "    brain_conf_train, brain_conf_test = pd.merge(conf_to_brain_match, brain_train_id, on='eid').drop(columns=['eid']), pd.merge(conf_to_brain_match, brain_test_id, on='eid').drop(columns=['eid'])\n",
    "    brain_conf_train.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_conf_train_fold_{fold}.csv', index=False)\n",
    "    brain_conf_test.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_conf_test_fold_{fold}.csv', index=False)\n",
    "        \n",
    "    ############## 3\n",
    "    print(f'Matching g-factor to T1_T2_whole_brain')\n",
    "        \n",
    "    # Match g-factor back to MRI\n",
    "    g_train, g_test, g_train_id, g_test_id = pd.merge(g_train_full, brain_train_id, on='eid').drop(columns=['eid']), pd.merge(g_test_full, brain_test_id, on='eid').drop(columns=['eid']), pd.merge(g_train_full, brain_train_id, on='eid')['eid'], pd.merge(g_test_full, brain_test_id, on='eid')['eid']\n",
    "    g_train.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/g_train_T1_T2_whole_brain_matched_fold_{fold}.csv', index=False)\n",
    "    g_test.to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/g_test_T1_T2_whole_brain_matched_fold_{fold}.csv', index=False)\n",
    "        \n",
    "    ############## 4\n",
    "    print(f'Applying ConfoundRegressor to MRI data fold {fold}')\n",
    "        \n",
    "    # Apply ConfoundRegressor\n",
    "    features_corr_train, features_corr_test, features_scaled_train, features_scaled_test, scaler_features = confound_regressor(brain_train, brain_test, brain_conf_train, brain_conf_test)\n",
    "    pd.DataFrame(features_corr_train, columns = brain_train.columns).to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_train_corr_{fold}.csv', index=False)\n",
    "    pd.DataFrame(features_corr_test, columns = brain_train.columns).to_csv(f'/PLS/brain/additional/fold_{fold}/suppl/T1_T2_whole_brain_test_corr_{fold}.csv', index=False)\n",
    "    pd.DataFrame(features_scaled_train, columns = brain_train.columns).to_csv(f'/PLS/brain/additional/fold_{fold}/scaling/T1_T2_whole_brain_train_scaled_{fold}.csv', index=False)\n",
    "    pd.DataFrame(features_scaled_test, columns = brain_train.columns).to_csv(f'/PLS/brain/additional/fold_{fold}/scaling/T1_T2_whole_brain_test_scaled_{fold}.csv', index=False)\n",
    "\n",
    "        \n",
    "    with open(f'/PLS/brain/additional/fold_{fold}/scaling/scaler_features_T1_T2_whole_brain_fold_{fold}.pkl', \"wb\") as f:\n",
    "        pickle.dump(scaler_features, f)\n",
    "\n",
    "\n",
    "    # Initiate and run PLS\n",
    "    parameters = {'n_components': range(1, features_corr_train.shape[1]+1, 1)}\n",
    "    pls = PLSRegression()\n",
    "    model = GridSearchCV(pls, parameters, scoring = 'neg_mean_absolute_error', cv=KFold(10, shuffle = True, random_state=seed), verbose=4)\n",
    "        \n",
    "        \n",
    "    print(\"Fitting PLS\")\n",
    "    model.fit(features_corr_train, np.array(g_train))\n",
    "        \n",
    "    print(f'Model parameters for fold {fold}:', model.cv_results_['params'])\n",
    "    print(f'Mean test score for fold {fold}:', model.cv_results_['mean_test_score'])\n",
    "    print(f'Rank test score for fold {fold}:', model.cv_results_['rank_test_score'])\n",
    "    print(model)\n",
    "        \n",
    "    print(f'Saving PLS model for T1_T2_whole_brain fold {fold}')\n",
    "    with open(f'/PLS/brain/additional/fold_{fold}/models/pkl/T1_T2_whole_brain_model_fold_{fold}.pkl', \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "            \n",
    "    print(f'Best params in fold {fold} = ', model.best_params_)\n",
    "    print(f'Best score (neg_mean_absolute_error) in fold {fold} = ', model.best_score_)\n",
    "            \n",
    "    # Predict g-factor\n",
    "    print(f'Predicting & saving g_test for T1_T2_whole_brain fold {fold}')\n",
    "    g_pred_test = model.predict(np.array(features_corr_test))\n",
    "    pd.DataFrame(g_pred_test, columns=['g predicted test']).to_csv(f'/PLS/brain/additional/fold_{fold}/g_pred/T1_T2_whole_brain_g_pred_test_fold_{fold}.csv')\n",
    "\n",
    "    g_pred_test_with_id = pd.concat([g_test_id.astype(int), pd.DataFrame(g_pred_test, columns=['g predicted test'])], axis=1).to_csv(f'/PLS/brain/additional/fold_{fold}/g_pred/T1_T2_whole_brain_g_pred_test_id_fold_{fold}.csv')\n",
    "\n",
    "        \n",
    "    print(f'Predicting & saving g_train for T1_T2_whole_brain fold {fold}')\n",
    "    g_pred_train = model.predict(np.array(features_corr_train))\n",
    "    pd.DataFrame(g_pred_train, columns=['g predicted train']).to_csv(f'/PLS/brain/additional/fold_{fold}/g_pred/T1_T2_whole_brain_g_pred_train_fold_{fold}.csv')\n",
    "        \n",
    "\n",
    "    g_pred_train_with_id = pd.concat([g_train_id.astype(int), pd.DataFrame(g_pred_train, columns=['g predicted train'])], axis=1).to_csv(f'/PLS/brain/additional/fold_{fold}/g_pred/T1_T2_whole_brain_g_pred_train_id_fold_{fold}.csv')\n",
    "        \n",
    "            \n",
    "    print(f\"Fold = {fold}\")\n",
    "    print(\"----------\")\n",
    "    print(\"MSE = \", mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "    print(\"MAE = \", mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "    print(\"R2 = \", r2_score(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "    print(\"Pearson's r = \", pearsonr(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "    print(\"----------\")\n",
    "            \n",
    "    pls_result['fold'] = fold\n",
    "    pls_result['modality'] = 'T1_T2_whole_brain'\n",
    "    pls_result['n_components'] = model.best_params_\n",
    "    pls_result['MSE'] = mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "    pls_result['MAE'] = mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "    pls_result['R2'] = r2_score(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "    pls_result['Pearson r'] = pearsonr(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "            \n",
    "    with open(f'/PLS/brain/additional/fold_{fold}/models/csv/T1_T2_whole_brain_fold_{fold}_PLS_result.csv', 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=pls_result.keys())\n",
    "        writer.writerow(pls_result)\n",
    "            \n",
    "    pls_result.clear()\n",
    "        \n",
    "    corr, pval = stats.pearsonr(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "    r2 = r2_score(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "    mse = mean_squared_error(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "    result = pd.DataFrame(['T1_T2', fold, corr, pval, r2, mse, model.best_params_], index=['Modality', 'Fold', 'Correlation', 'P-value', 'R2', 'MSE', 'n components'], columns=['Values']).to_csv(f'/PLS/brain/additional/fold_{fold}/models/csv/T1_T2_whole_brain_fold_{fold}_full_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and average results across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_folds = []\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for fold in folds:\n",
    "    pls = pd.read_csv(f'/PLS/brain/additional/fold_{fold}/models/csv/T1_T2_whole_brain_fold_{fold}_PLS_result.csv', header=None)\n",
    "    pls.columns = ['Fold', 'Modality', 'n components', 'MSE', 'MAE', 'R2', 'Pearson r']\n",
    "    five_folds.append(pls)\n",
    "    five_folds_all_modalities = pd.concat(five_folds, ignore_index=False)\n",
    "\n",
    "five_folds_all_modalities['Pearson r'] = five_folds_all_modalities['Pearson r'].astype(str).str.replace(r'PearsonRResult\\(statistic=|pvalue=|\\)', '', regex=True)\n",
    "five_folds_all_modalities[['Pearson r', 'p-value']] = five_folds_all_modalities['Pearson r'].str.split(',', expand=True).astype(float).round(decimals=3)\n",
    "five_folds_all_modalities = five_folds_all_modalities.round(decimals=3)\n",
    "#five_folds_all_modalities.to_csv('/PLS/brain/dti/pls_5_folds_all_modalities.csv', index=False)\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(five_folds_all_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across folders\n",
    "five_folds_all_modalities_mean= five_folds_all_modalities[['R2', 'Pearson r', 'Modality', 'MSE', 'MAE']]\n",
    "five_folds_all_modalities_mean = five_folds_all_modalities_mean.groupby(['Modality']).mean().round(3).reset_index() #.sort_values(by='R2', ascending=False)\n",
    "five_folds_all_modalities_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukbiobank_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
