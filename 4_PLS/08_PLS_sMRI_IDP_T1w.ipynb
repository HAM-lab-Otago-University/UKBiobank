{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting, image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter, MultipleLocator, AutoMinorLocator, MaxNLocator, FixedLocator\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import textwrap \n",
    "import warnings\n",
    "import skbold\n",
    "from scipy.stats import pearsonr\n",
    "from skbold.preproc import ConfoundRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ConfoundRegressor with skbold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: skbold\n",
    "def confound_regressor_skbold(features_train, features_test, confounds_train, confounds_test):\n",
    "    # Scale features (train and test sets)\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train_scaled = scaler_features.fit_transform(features_train)\n",
    "    features_test_scaled = scaler_features.transform(features_test)\n",
    "    \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train_scaled = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test_scaled = scaler_confounds.transform(confounds_test)\n",
    "\n",
    "    # Convert full sets into np.array\n",
    "    features_full_scaled_np = np.array(pd.concat([pd.DataFrame(features_train_scaled, columns = features_train.columns), pd.DataFrame(features_test_scaled, columns = features_test.columns)], axis=0))\n",
    "    confounds_full_scaled_np = np.array(pd.concat([pd.DataFrame(confounds_train_scaled, columns = confounds_train.columns), pd.DataFrame(confounds_test_scaled, columns = confounds_test.columns)], axis=0))\n",
    "    \n",
    "    # Define ConfoundRegressor on a FULL set (train and test)\n",
    "    cfr = ConfoundRegressor(confound=confounds_full_scaled_np, X=features_full_scaled_np)\n",
    "    features_train_corrected = cfr.fit_transform(features_train_scaled)\n",
    "    features_test_corrected = cfr.transform(features_test_scaled)\n",
    "\n",
    "\n",
    "    return features_train_corrected, features_test_corrected, features_train_scaled, features_test_scaled, scaler_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if for some reason *skbold* does not work, a similar result can be obtained by applying LinearRegression model - they give IDENTICAL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: Linear Model\n",
    "def confound_regressor(features_train, features_test, confounds_train, confounds_test):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train = scaler_features.fit_transform(features_train)\n",
    "    features_test = scaler_features.transform(features_test)\n",
    "        \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test = scaler_confounds.transform(confounds_test)\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(confounds_train, features_train)\n",
    "    features_train_pred = model.predict(confounds_train)\n",
    "    features_train_res = features_train - features_train_pred\n",
    "\n",
    "    features_test_pred = model.predict(confounds_test)\n",
    "    features_test_res = features_test - features_test_pred\n",
    "\n",
    "    return features_train_res, features_test_res, features_train, features_test, scaler_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and prepare T1w files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_fast = pd.read_csv('/PLS/brain/t1/t1_idp/struct_fast_names_nona.csv')\n",
    "struct_sub_first = pd.read_csv('/PLS/brain/t1/t1_idp/struct_sub_first_names_nona.csv')\n",
    "struct_aseg_mean_intensity = pd.read_csv('/PLS/brain/t1/t1_idp/struct_aseg_mean_intensity_names_nona.csv')\n",
    "struct_aseg_volume = pd.read_csv('/PLS/brain/t1/t1_idp/struct_aseg_volume_names_nona.csv')\n",
    "struct_ba_exvivo_area = pd.read_csv('/PLS/brain/t1/t1_idp/struct_ba_exvivo_area_names_nona.csv')\n",
    "struct_ba_exvivo_mean_thickness = pd.read_csv('/PLS/brain/t1/t1_idp/struct_ba_exvivo_mean_thickness_names_nona.csv')\n",
    "struct_ba_exvivo_volume = pd.read_csv('/PLS/brain/t1/t1_idp/struct_ba_exvivo_volume_names_nona.csv')\n",
    "struct_a2009s_area = pd.read_csv('/PLS/brain/t1/t1_idp/struct_a2009s_area_names_nona.csv')\n",
    "struct_a2009s_mean_thickness = pd.read_csv('/PLS/brain/t1/t1_idp/struct_a2009s_mean_thickness_names_nona.csv')\n",
    "struct_a2009s_volume = pd.read_csv('/PLS/brain/t1/t1_idp/struct_a2009s_volume_names_nona.csv')\n",
    "struct_dkt_area = pd.read_csv('/PLS/brain/t1/t1_idp/struct_dkt_area_names_nona.csv')\n",
    "struct_dkt_mean_thickness = pd.read_csv('/PLS/brain/t1/t1_idp/struct_dkt_mean_thickness_names_nona.csv')\n",
    "struct_dkt_volume = pd.read_csv('/PLS/brain/t1/t1_idp/struct_dkt_volume_names_nona.csv')\n",
    "struct_desikan_gw = pd.read_csv('/PLS/brain/t1/t1_idp/struct_desikan_gw_names_nona.csv')\n",
    "struct_desikan_pial = pd.read_csv('/PLS/brain/t1/t1_idp/struct_desikan_pial_names_nona.csv')\n",
    "struct_desikan_white_area = pd.read_csv('/PLS/brain/t1/t1_idp/struct_desikan_white_area_names_nona.csv')\n",
    "struct_desikan_white_mean_thickness = pd.read_csv('/PLS/brain/t1/t1_idp/struct_desikan_white_mean_thickness_names_nona.csv')\n",
    "struct_desikan_white_volume = pd.read_csv('/PLS/brain/t1/t1_idp/struct_desikan_white_volume_names_nona.csv')\n",
    "struct_subsegmentation = pd.read_csv('/PLS/brain/t1/t1_idp/struct_subsegmentation_names_nona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['struct_fast', 'struct_sub_first', 'struct_aseg_mean_intensity', 'struct_aseg_volume', 'struct_ba_exvivo_area', \n",
    "          'struct_ba_exvivo_mean_thickness', 'struct_ba_exvivo_volume', 'struct_a2009s_area', 'struct_a2009s_mean_thickness', 'struct_a2009s_volume',\n",
    "           'struct_dkt_area', 'struct_dkt_mean_thickness', 'struct_dkt_volume', 'struct_desikan_gw', 'struct_desikan_pial',\n",
    "           'struct_desikan_white_area', 'struct_desikan_white_mean_thickness', 'struct_desikan_white_volume', 'struct_subsegmentation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns have 'Field ID...' in their names - remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/PLS/brain/t1/t1_idp/'\n",
    "def clean_dataframe_columns(modality):\n",
    "    # Load the data frame\n",
    "    df = pd.read_csv(f\"{folder_path}{modality}_names_nona.csv\")\n",
    "\n",
    "    # Rename columns if they contain 'Field ID...'\n",
    "    #df.columns = [col.replace('Field ID', '').strip() for col in df.columns]\n",
    "    df.columns = [re.sub(r'\\(Field ID: \\d+\\)', '', col).strip() for col in df.columns]\n",
    "    \n",
    "    df.to_csv(f\"{folder_path}{modality}_renamed.csv\", index=False)\n",
    "    print(f\"Cleaned columns for {modality} and saved to {modality}_renamed.csv\")\n",
    "\n",
    "# Iterate over each modality and clean the columns\n",
    "for modality in modalities:\n",
    "    clean_dataframe_columns(modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds = pd.read_csv('/PLS/brain/t1/t1_idp/struct_conf_full_dummy.csv')\n",
    "\n",
    "############## 1\n",
    "print('Started: Uploading')\n",
    "\n",
    "for modality in modalities:\n",
    "\n",
    "    file = pd.read_csv(f'/PLS/brain/t1/t1_idp/{modality}_renamed.csv')\n",
    "    #modality_data = globals()[modality]\n",
    "\n",
    "    folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    \n",
    "    # Match confounds to MRI\n",
    "    print('Started: Match confounds to brain data')\n",
    "    conf_to_brain_match = pd.merge(confounds, file['eid'], on='eid')\n",
    "    brain_to_conf_match = pd.merge(conf_to_brain_match['eid'], file, on='eid')\n",
    "\n",
    "    for fold in folds:\n",
    "        train_id = pd.read_csv(f'/g_factor_5_folds_python/fold_{fold}/train_id_fold_{fold}.csv')\n",
    "        test_id = pd.read_csv(f'/g_factor_5_folds_python/fold_{fold}/test_id_fold_{fold}.csv')\n",
    "        \n",
    "        # Upload g-factor with ID\n",
    "        g_train_full = pd.read_csv(f'/PLS/g_factor/g_train_with_id_fold_{fold}.csv')\n",
    "        g_test_full = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv')\n",
    "\n",
    "    \n",
    "        # Match brain data to cognitive data\n",
    "        brain_train, brain_test, brain_train_id, brain_test_id = pd.merge(brain_to_conf_match, train_id, on='eid').drop(columns=['eid']), pd.merge(brain_to_conf_match, test_id, on='eid').drop(columns=['eid']), pd.merge(brain_to_conf_match, train_id, on='eid')['eid'], pd.merge(brain_to_conf_match, test_id, on='eid')['eid']\n",
    "\n",
    "        brain_train.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_train_fold_{fold}.csv', index=False)\n",
    "        brain_test.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_test_fold_{fold}.csv', index=False)\n",
    "        brain_train_id.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_train_id_fold_{fold}.csv', index=False)\n",
    "        brain_test_id.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_test_id_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        ############## 2\n",
    "        print(f'Matching confounds to {modality} fold {fold}')\n",
    "        \n",
    "        # Match confounds to MRI\n",
    "        brain_conf_train, brain_conf_test = pd.merge(conf_to_brain_match, brain_train_id, on='eid').drop(columns=['eid']), pd.merge(conf_to_brain_match, brain_test_id, on='eid').drop(columns=['eid'])\n",
    "        brain_conf_train.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_conf_train_fold_{fold}.csv', index=False)\n",
    "        brain_conf_test.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_conf_test_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        ############## 3\n",
    "        print(f'Matching g-factor to {modality}')\n",
    "        \n",
    "        # Match g-factor back to MRI\n",
    "        g_train, g_test, g_train_id, g_test_id = pd.merge(g_train_full, brain_train_id, on='eid').drop(columns=['eid']), pd.merge(g_test_full, brain_test_id, on='eid').drop(columns=['eid']), pd.merge(g_train_full, brain_train_id, on='eid')['eid'], pd.merge(g_test_full, brain_test_id, on='eid')['eid']\n",
    "        g_train.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_train_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        g_test.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_test_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "\n",
    "        g_train_id.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_train_id_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        g_test_id.to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_test_id_{modality}_matched_fold_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "############## 1\n",
    "seed = 42\n",
    "\n",
    "for modality in modalities:\n",
    "    \n",
    "    folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    pls_result = {}\n",
    "    \n",
    "\n",
    "    for fold in folds:\n",
    "        brain_train = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_train_fold_{fold}.csv')\n",
    "        brain_test = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_test_fold_{fold}.csv')\n",
    "        brain_conf_train = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_conf_train_fold_{fold}.csv')\n",
    "        brain_conf_test = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_conf_test_fold_{fold}.csv')\n",
    "        g_train_id = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_train_id_{modality}_matched_fold_{fold}.csv')\n",
    "        g_test_id = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_test_id_{modality}_matched_fold_{fold}.csv')\n",
    "        g_train = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_train_{modality}_matched_fold_{fold}.csv')\n",
    "        g_test = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_test_{modality}_matched_fold_{fold}.csv')\n",
    "       \n",
    "        # Apply ConfoundRegressor\n",
    "\n",
    "        features_train_corr, features_test_corr, features_train, features_test, scaler_features = confound_regressor(brain_train, brain_test, brain_conf_train, brain_conf_test)\n",
    "\n",
    "        pd.DataFrame(features_train_corr, columns = brain_train.columns).to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_train_corr_{fold}.csv', index=False)\n",
    "        pd.DataFrame(features_test_corr, columns = brain_test.columns).to_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_test_corr_{fold}.csv', index=False)\n",
    "        \n",
    "        with open(f'/PLS/brain/t1/fold_{fold}/suppl/scaler_features_{modality}_fold_{fold}.pkl', \"wb\") as f:\n",
    "            pickle.dump(scaler_features, f)\n",
    "\n",
    "        # Initiate and run PLS\n",
    "        parameters = {'n_components': range(1, features_train_corr.shape[1]+1, 1)}\n",
    "        pls = PLSRegression()\n",
    "        model = GridSearchCV(pls, parameters, scoring = 'neg_mean_absolute_error', cv=KFold(10, shuffle = True, random_state=seed), verbose=4, n_jobs = 8)\n",
    "        \n",
    "        \n",
    "        print(\"Fitting PLS\")\n",
    "        model.fit(features_train_corr, np.array(g_train))\n",
    "        \n",
    "        print(f'Model parameters for fold {fold}:', model.cv_results_['params'])\n",
    "        print(f'Mean test score for fold {fold}:', model.cv_results_['mean_test_score'])\n",
    "        print(f'Rank test score for fold {fold}:', model.cv_results_['rank_test_score'])\n",
    "        print(model)\n",
    "        \n",
    "        print(f'Saving PLS model for {modality} fold {fold}')\n",
    "        with open(f'/PLS/brain/t1/fold_{fold}/models/{modality}_model_fold_{fold}.pkl', \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "            \n",
    "        print(f'Best params in fold {fold} = ', model.best_params_)\n",
    "        print(f'Best score (neg_mean_absolute_error) in fold {fold} = ', model.best_score_)\n",
    "            \n",
    "        # Predict the values\n",
    "        print(f'Predicting & saving g_test for {modality} fold {fold}')\n",
    "        g_pred_test = model.predict(np.array(features_test_corr))\n",
    "        pd.DataFrame(g_pred_test, columns=['g predicted test']).to_csv(f'/PLS/brain/t1/fold_{fold}/g_pred/{modality}_g_pred_test_fold_{fold}.csv')\n",
    "\n",
    "        g_pred_test_with_id = pd.concat([g_test_id.astype(int), pd.DataFrame(g_pred_test, columns=['g predicted test'])], axis=1).to_csv(f'/PLS/brain/t1/fold_{fold}/g_pred/{modality}_g_pred_test_id_fold_{fold}.csv')\n",
    "\n",
    "        \n",
    "        print(f'Predicting & saving g_train for {modality} fold {fold}')\n",
    "        g_pred_train = model.predict(np.array(features_train_corr))\n",
    "        pd.DataFrame(g_pred_train, columns=['g predicted train']).to_csv(f'/PLS/brain/t1/fold_{fold}/g_pred/{modality}_g_pred_train_fold_{fold}.csv')\n",
    "        \n",
    "\n",
    "        g_pred_train_with_id = pd.concat([g_train_id.astype(int), pd.DataFrame(g_pred_train, columns=['g predicted train'])], axis=1).to_csv(f'/PLS/brain/t1/fold_{fold}/g_pred/{modality}_g_pred_train_id_fold_{fold}.csv')\n",
    "        \n",
    "            \n",
    "        print(f\"Fold = {fold}\")\n",
    "        print(\"----------\")\n",
    "        print(\"MSE = \", mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"MAE = \", mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"R2 = \", r2_score(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"Pearson's r = \", pearsonr(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"----------\")\n",
    "            \n",
    "        pls_result['fold'] = fold\n",
    "        pls_result['modality'] = modality\n",
    "        pls_result['n_components'] = model.best_params_\n",
    "        pls_result['MSE'] = mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['MAE'] = mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['R2'] = r2_score(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['Pearson r'] = pearsonr(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "            \n",
    "        with open(f'/PLS/brain/t1/fold_{fold}/models/{modality}_fold_{fold}_PLS_result.csv', 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=pls_result.keys())\n",
    "            writer.writerow(pls_result)\n",
    "            \n",
    "        pls_result.clear()\n",
    "        \n",
    "        corr, pval = stats.pearsonr(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        r2 = r2_score(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        mse = mean_squared_error(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        pd.DataFrame([modality, fold, corr, pval, r2, mse, model.best_params_], index=['Modality', 'Fold', 'Correlation', 'P-value', 'R2', 'MSE', 'n components'], columns=['Values']).to_csv(f'/PLS/brain/t1/fold_{fold}/models/{modality}_fold_{fold}_full_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload individual fold pls results and merge them into one table\n",
    "five_folds = []\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for modality in modalities:\n",
    "    for fold in folds:\n",
    "        pls = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/models/{modality}_fold_{fold}_PLS_result.csv', header=None)\n",
    "        pls.columns = ['Fold', 'Modality', 'n components', 'MSE', 'MAE', 'R2', 'Pearson r']\n",
    "        five_folds.append(pls)\n",
    "        five_folds_all_modalities = pd.concat(five_folds, ignore_index=False)\n",
    "\n",
    "five_folds_all_modalities['Pearson r'] = five_folds_all_modalities['Pearson r'].astype(str).str.replace(r'PearsonRResult\\(statistic=|pvalue=|\\)', '', regex=True)\n",
    "five_folds_all_modalities[['Pearson r', 'p-value']] = five_folds_all_modalities['Pearson r'].str.split(',', expand=True).astype(float).round(decimals=3)\n",
    "five_folds_all_modalities = five_folds_all_modalities.round(decimals=3)\n",
    "five_folds_all_modalities['n components'] = five_folds_all_modalities['n components'].astype(str).str.replace(r\"{'n_components':\", '', regex=True)\n",
    "five_folds_all_modalities['n components'] = five_folds_all_modalities['n components'].astype(str).str.replace(r\"}\", '', regex=True)\n",
    "five_folds_all_modalities.to_csv('/PLS/brain/t1/T1_PLS_result_five_folds_summary.csv', index=False)\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(five_folds_all_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "five_folds_all_modalities_mean = five_folds_all_modalities[['R2', 'Pearson r', 'Modality', 'MSE', 'MAE']].groupby(['Modality']).mean().round(3).reset_index()\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('struct_', '', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('fast', 'FAST', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('subsegmentation', 'Subcortical volumetric subsegmentation', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('first', 'FIRST', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('dkt', 'DKT', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('desikan', 'Desikan', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('aseg', 'ASEG', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('white', 'white matter', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('gw', 'grey/white matter intensity', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('_', ' ', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].astype(str).str.replace('sub ', '', regex=True)\n",
    "five_folds_all_modalities_mean['Modality'] = five_folds_all_modalities_mean['Modality'].apply(lambda x: x.replace('ba ', 'BA '))\n",
    "five_folds_all_modalities_mean.to_csv(f'/PLS/brain/t1/T1_PLS_result_averaged.csv', index=False)\n",
    "five_folds_all_modalities_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_folds_all_modalities_mean.sort_values(by='R2', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PLS loadings (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for modality in modalities:\n",
    "    for fold in folds:\n",
    "        with open(f'/PLS/brain/t1/fold_{fold}/models/{modality}_model_fold_{fold}.pkl', \"rb\") as f:\n",
    "              model = pickle.load(f)\n",
    "              r2_sum = 0\n",
    "              r2_vector = np.empty(model.best_estimator_.n_components)\n",
    "              \n",
    "              g_train = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_train_{modality}_matched_fold_{fold}.csv')\n",
    "        \n",
    "        for i in range(0,model.best_estimator_.n_components):\n",
    "          Y_pred = np.dot(model.best_estimator_.x_scores_[:,i].reshape(-1,1), model.best_estimator_.y_loadings_[:,i].reshape(-1,1).T) * g_train.values.std(axis=0, ddof=1) + g_train['g'].mean(axis=0)\n",
    "          r2_sum += r2_score(g_train.values,Y_pred)\n",
    "          print('R2 for %d component: %g' %(i+1,r2_score(g_train.values, Y_pred)))\n",
    "          r2_vector[i] = r2_score(g_train.values,Y_pred)\n",
    "          \n",
    "          x_loading_by_r2 = model.best_estimator_.x_loadings_ *  r2_vector\n",
    "          x_loading_by_r2_scaled = stats.zscore(model.best_estimator_.x_loadings_) *  r2_vector\n",
    "          weighted_x_loading = np.sum(x_loading_by_r2, axis=1)\n",
    "          weighted_x_loading_scaled = np.sum(x_loading_by_r2_scaled, axis=1)\n",
    "        print(f'R2 for all components in {modality} fold {fold}: %g' %r2_sum.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all weighted loadings into one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_x_loadings = {}\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for modality in modalities:\n",
    "    weighted_x_loadings[modality] = {}\n",
    "    for fold in folds:\n",
    "        with open(f'/PLS/brain/t1/fold_{fold}/models/{modality}_model_fold_{fold}.pkl', \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        g_train = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/g_train_{modality}_matched_fold_{fold}.csv')['g']\n",
    "        \n",
    "        # Initialize variables for R2 calculation\n",
    "        r2_vector = np.empty(model.best_estimator_.n_components)\n",
    "        \n",
    "        # Calculate R2 for each component\n",
    "        for i in range(model.best_estimator_.n_components):\n",
    "            Y_pred = np.dot(model.best_estimator_.x_scores_[:, i].reshape(-1, 1),\n",
    "                            model.best_estimator_.y_loadings_[:, i].reshape(-1, 1).T) * g_train.values.std(ddof=1) + g_train.mean()\n",
    "            r2_vector[i] = r2_score(g_train.values, Y_pred)\n",
    "        \n",
    "        # Calculate weighted x_loadings\n",
    "        x_loading_by_r2 = model.best_estimator_.x_loadings_ * r2_vector\n",
    "        x_loading_by_r2_scaled = stats.zscore(model.best_estimator_.x_loadings_) * r2_vector\n",
    "        weighted_x_loading = np.sum(x_loading_by_r2, axis=1)\n",
    "        weighted_x_loading_scaled = np.sum(x_loading_by_r2_scaled, axis=1)\n",
    "        \n",
    "        # Store the weighted_x_loadings in the dictionary\n",
    "        weighted_x_loadings[modality][fold] = {\n",
    "            'weighted_x_loading': weighted_x_loading,\n",
    "            'weighted_x_loading_scaled': weighted_x_loading_scaled\n",
    "        }\n",
    "\n",
    "# Print the accumulated weighted_x_loadings\n",
    "for modality in weighted_x_loadings:\n",
    "    for fold in weighted_x_loadings[modality]:\n",
    "        print(f\"Modality: {modality}, Fold: {fold}, Weighted X Loading: {weighted_x_loadings[modality][fold]['weighted_x_loading']}\")\n",
    "        print(f\"Modality: {modality}, Fold: {fold}, Weighted X Loading Scaled: {weighted_x_loadings[modality][fold]['weighted_x_loading_scaled']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract weighted loadings for each modality in the form of a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_dataframes = {modality: globals()[modality] for modality in modalities}\n",
    "weighted_x_loading_scaled_dfs = {}\n",
    "for modality in weighted_x_loadings:\n",
    "    dfs = []\n",
    "    for fold in weighted_x_loadings[modality]:\n",
    "        # Get the weighted_x_loading_scaled for the current fold\n",
    "        weighted_x_loading_scaled = weighted_x_loadings[modality][fold]['weighted_x_loading_scaled']\n",
    "        # Get feature names\n",
    "        feature_names = modality_dataframes[modality].drop(columns='eid').columns.tolist()\n",
    "        # Create a DataFrame with feature names and weighted_x_loading_scaled\n",
    "        df = pd.concat([pd.DataFrame(feature_names, columns=['Features']),\n",
    "                        pd.DataFrame(weighted_x_loading_scaled, columns=['Loadings'])], axis=1)\n",
    "        dfs.append(df)\n",
    "    # Concatenate all fold DataFrames for the current modality\n",
    "    weighted_x_loading_scaled_dfs[modality] = pd.concat(dfs, keys=folds, names=['Fold', 'Index'])\n",
    "\n",
    "# Print the DataFrames for each modality\n",
    "for modality in weighted_x_loading_scaled_dfs:\n",
    "    print(f\"DataFrame for {modality}:\")\n",
    "    print(weighted_x_loading_scaled_dfs[modality])\n",
    "\n",
    "# Save modality_dataframes\n",
    "with open(f'/PLS/brain/t1/t1_modality_dataframes.pkl', \"wb\") as f:\n",
    "    pickle.dump(modality_dataframes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary\n",
    "with open(f'/PLS/brain/t1/t1_weighted_x_loading_scaled_dfs.pkl', \"wb\") as f:\n",
    "    pickle.dump(weighted_x_loading_scaled_dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot original vs deconfounded features for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_folds = ['0']\n",
    "for modality in modalities:\n",
    "    for fold in plot_folds:\n",
    "        features_train_corr = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_train_corr_{fold}.csv')\n",
    "        brain_train = pd.read_csv(f'/PLS/brain/t1/fold_{fold}/suppl/{modality}_train_fold_{fold}.csv')\n",
    "        cols = brain_train.columns[:5]\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 5))\n",
    "        axes = axes.flatten()\n",
    "        for i, col in enumerate(cols):\n",
    "            sns.regplot(x=brain_train[col], y=features_train_corr[col], line_kws={'color': 'red', 'linewidth': 1}, scatter=True, scatter_kws = {\"color\": \".7\", \"s\": 10, \"alpha\": 0.2}, ax=axes[i])\n",
    "            axes[i].set_xlabel(\"\\n\".join(textwrap.wrap(cols[i] + ' (Original)', 20)), fontsize=7)\n",
    "            axes[i].set_ylabel(\"\\n\".join(textwrap.wrap(cols[i] + ' (Residuals)', 20)), fontsize=7)\n",
    "        plt.yticks(fontsize=6)\n",
    "        plt.xticks(fontsize=6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukbiobank_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
