{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import nilearn\n",
    "from nilearn import connectome\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import sklearn\n",
    "import warnings\n",
    "import skbold\n",
    "from skbold.preproc import ConfoundRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ConfoundRegressor: skbold\n",
    "def confound_regressor_skbold(features_train, features_test, confounds_train, confounds_test):\n",
    "    # Scale features (train and test sets)\n",
    "    scaler_features = StandardScaler()\n",
    "    features_train_scaled = scaler_features.fit_transform(features_train)\n",
    "    features_test_scaled = scaler_features.transform(features_test)\n",
    "    \n",
    "    # Scale confounds (train and test sets)\n",
    "    scaler_confounds = StandardScaler()\n",
    "    confounds_train_scaled = scaler_confounds.fit_transform(confounds_train)\n",
    "    confounds_test_scaled = scaler_confounds.transform(confounds_test)\n",
    "\n",
    "    # Convert full sets into np.array\n",
    "    features_full_scaled_np = np.array(pd.concat([pd.DataFrame(features_train_scaled, columns = features_train.columns), pd.DataFrame(features_test_scaled, columns = features_test.columns)], axis=0))\n",
    "    confounds_full_scaled_np = np.array(pd.concat([pd.DataFrame(confounds_train_scaled, columns = confounds_train.columns), pd.DataFrame(confounds_test_scaled, columns = confounds_test.columns)], axis=0))\n",
    "    \n",
    "    # Define ConfoundRegressor on a FULL set (train and test)\n",
    "    cfr = ConfoundRegressor(confound=confounds_full_scaled_np, X=features_full_scaled_np)\n",
    "    features_train_corrected = cfr.fit_transform(features_train_scaled)\n",
    "    features_test_corrected = cfr.transform(features_test_scaled)\n",
    "\n",
    "\n",
    "    return features_train_corrected, features_test_corrected, features_train_scaled, features_test_scaled, scaler_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get timeseries for 25 ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of folder paths\n",
    "folder_paths = [\"/Resting_State/rsfMRI_bulk_main/1-4000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/5000-14000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/14000_24000_split/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/14000-24000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/24000-34000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/24000-34000_split/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/34000-44000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/34000-44000_split/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/44000-54413/unzipped/\"]\n",
    "\n",
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list = []\n",
    "index_list = []\n",
    "skipped_folders = []\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "\n",
    "    print(f'Started {folder_path}')\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "\n",
    "        print(f'Started {subject_folder}')\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "\n",
    "        if os.path.isdir(os.path.join(subfolder_path, 'fMRI')):\n",
    "            subfolder_path = os.path.join(subfolder_path, 'fMRI')\n",
    "        else:\n",
    "            subfolder_path = subfolder_path\n",
    "\n",
    "        timeseries_file = os.path.join(subfolder_path, \"rfMRI_25.dr\", \"dr_stage1.txt\")\n",
    "\n",
    "        if not os.path.exists(timeseries_file):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        print('Appending files')\n",
    "        timeseries_25 = np.loadtxt(timeseries_file)\n",
    "        timeseries_list.append(timeseries_25)\n",
    "        index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders, sep='\\n')\n",
    "\n",
    "index_list_df = pd.DataFrame(index_list)\n",
    "index_list_df.columns = ['eid']\n",
    "index_list_df.sort_values(by='eid')\n",
    "\n",
    "print('Started instance 2')\n",
    "timeseries_instance_2 = []\n",
    "index_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list, timeseries_list):\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    if mid_part == '2':\n",
    "        timeseries_instance_2.append(timeseries)\n",
    "        index_instance_2.append(folder_name)\n",
    "\n",
    "index_25_df_full = pd.DataFrame(index_list, columns=['eid']).to_csv('/PLS/brain/rs/ica_tangent/files/index_25_ica_full.csv', index=False)\n",
    "index_instance_2_25_df = pd.DataFrame(index_instance_2, columns=['eid'])\n",
    "index_instance_2_25_df['eid'] = index_instance_2_25_df['eid'].str.replace('_20227_2_0', '').astype(int)\n",
    "index_instance_2_25_df.to_csv('/PLS/brain/rs/ica_tangent/files/index_25_ica_instance_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vectorize=True returned flattened lower triangular parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary and save it\n",
    "timeseries_25_dict = {}\n",
    "for index, timeseries in zip(index_instance_2_25_df['eid'], timeseries_instance_2):\n",
    "    timeseries_25_dict[index] = timeseries\n",
    "\n",
    "with open(f'/PLS/brain/rs/ica_tangent/files/timeseries_25_dict.pkl', \"wb\") as f:\n",
    "    pickle.dump(timeseries_25_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary\n",
    "with open('/PLS/brain/rs/ica_tangent/files/timeseries_25_dict.pkl', \"rb\") as a:\n",
    "    timeseries_25_dict = pickle.load(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract tangent matrices for 21 'good' components\n",
    "\n",
    "(outlined [here](https://www.fmrib.ox.ac.uk/ukbiobank/group_means/rfMRI_GoodComponents_d25_v1.txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for fold in folds:\n",
    "\n",
    "    print(\"__________________________________________\")\n",
    "\n",
    "    print(f\"Started fold {fold}\")\n",
    "\n",
    "    print('Setting ConnectivityMeasure model')\n",
    "\n",
    "    tangent_measure = ConnectivityMeasure(\n",
    "    kind=\"tangent\",\n",
    "    standardize=\"zscore_sample\",\n",
    "    vectorize = True,\n",
    "    discard_diagonal = True)\n",
    "\n",
    "    print(\"Uploading train and test id\")\n",
    "\n",
    "    train_id = pd.read_csv(f'/g_factor_5_folds_python/fold_{fold}/train_id_fold_{fold}.csv')\n",
    "    test_id = pd.read_csv(f'/g_factor_5_folds_python/fold_{fold}/test_id_fold_{fold}.csv')\n",
    "\n",
    "    tangent_25_train = []\n",
    "    tangent_25_test = []\n",
    "    \n",
    "    tangent_train_id = []\n",
    "    tangent_test_id = []\n",
    "    \n",
    "    print(\"Getting train set\")\n",
    "\n",
    "    for id_val in train_id['eid'].values:\n",
    "        if id_val in timeseries_25_dict.keys():\n",
    "            tangent_25_train.append(timeseries_25_dict[id_val])\n",
    "            tangent_train_id.append(id_val)\n",
    "\n",
    "    print(\"Get 21 components, fit, and transform, train set\")\n",
    "    tangent_21_train = np.array(tangent_25_train)[:, :, [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "    tangent_matrices_21_train = tangent_measure.fit_transform(tangent_21_train)\n",
    "    pd.DataFrame(tangent_matrices_21_train, columns = [f'Component {i+1} Tangent (21 IC)' for i in range(tangent_matrices_21_train.shape[1])], index=tangent_train_id).to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_21_train_fold_{fold}.csv')\n",
    "    \n",
    "    print(\"__________________________________________\")\n",
    "    print(\"Getting test set\")\n",
    "            \n",
    "    for id_val in test_id['eid'].values:\n",
    "        if id_val in timeseries_25_dict.keys():\n",
    "            tangent_25_test.append(timeseries_25_dict[id_val])\n",
    "            tangent_test_id.append(id_val)\n",
    "\n",
    "    print(\"Get 21 components and transform, test set\")\n",
    "    tangent_21_test = np.array(tangent_25_test)[:, :, [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "    tangent_matrices_21_test = tangent_measure.transform(tangent_21_test)\n",
    "    pd.DataFrame(tangent_matrices_21_test, columns = [f'Component {i+1} Tangent (21 IC)' for i in range(tangent_matrices_21_test.shape[1])], index=tangent_test_id).to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_21_test_fold_{fold}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate column names that will reflect connections between components (lower triangular part with diagonal discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_names(components):\n",
    "    column_names = []\n",
    "    for i in range(1, len(components)):\n",
    "        for j in range(i):\n",
    "            column_names.append(f'Component {components[i]} & Component {components[j]} Tangent (55 IC)')\n",
    "    return column_names\n",
    "\n",
    "# Original components\n",
    "good_21_orig = [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
    "\n",
    "# Generate column names\n",
    "column_names = generate_column_names(good_21_orig)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns to reflect connections between components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in folds:\n",
    "    print(f\"Renaming columns for fold {fold}\")\n",
    "\n",
    "    tangent_matrices_21_train = pd.read_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_21_train_fold_{fold}.csv')\n",
    "    tangent_matrices_21_test = pd.read_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_21_test_fold_{fold}.csv')\n",
    "\n",
    "    # Rename the columns\n",
    "    tangent_matrices_21_train.columns = ['Unnamed: 0'] + column_names\n",
    "    tangent_matrices_21_test.columns = ['Unnamed: 0'] + column_names\n",
    "\n",
    "    # Save the updated DataFrames back to CSV files\n",
    "    tangent_matrices_21_train.to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_renamed/tangent_matrices_21_train_fold_{fold}.csv', index=False)\n",
    "    tangent_matrices_21_test.to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_renamed/tangent_matrices_21_test_fold_{fold}.csv', index=False)\n",
    "\n",
    "    print(f\"Columns renamed for fold {fold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, convert a 1D array back into NxN matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangent_matrices_21_test = pd.read_csv('/PLS/brain/rs/ica_tangent/tangent_matrices_renamed/tangent_matrices_21_test_fold_0.csv')\n",
    "def vector_to_full_matrix(vector):\n",
    "    # Calculate the size of the original matrix N\n",
    "    k = len(vector)\n",
    "    N = int((1 + np.sqrt(1 + 8 * k)) / 2)\n",
    "    \n",
    "    # Initialize an NxN matrix with zeros\n",
    "    matrix = np.zeros((N, N))\n",
    "    \n",
    "    # Fill the lower triangular part (excluding the diagonal)\n",
    "    index = 0\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            matrix[i, j] = vector[index]\n",
    "            matrix[j, i] = vector[index]\n",
    "            index += 1\n",
    "    np.fill_diagonal(matrix, vector[index:index + N])\n",
    "    return matrix\n",
    "\n",
    "# Example usage\n",
    "matrix = vector_to_full_matrix(tangent_matrices_21_test.drop(columns='Unnamed: 0').iloc[0].values)\n",
    "matrices = [vector_to_full_matrix(row) for row in tangent_matrices_21_test.drop(columns='Unnamed: 0').iloc[:3].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get timeseries for 100 ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of folder paths\n",
    "folder_paths = [\"/Resting_State/rsfMRI_bulk_main/1-4000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/5000-14000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/14000_24000_split/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/14000-24000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/24000-34000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/24000-34000_split/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/34000-44000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/34000-44000_split/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_main/44000-54413/unzipped/\"]\n",
    "\n",
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list = []\n",
    "index_list = []\n",
    "skipped_folders = []\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "\n",
    "    print(f'Started {folder_path}')\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "\n",
    "        print(f'Started {subject_folder}')\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "\n",
    "        if os.path.isdir(os.path.join(subfolder_path, 'fMRI')):\n",
    "            subfolder_path = os.path.join(subfolder_path, 'fMRI')\n",
    "        else:\n",
    "            subfolder_path = subfolder_path\n",
    "\n",
    "        timeseries_file = os.path.join(subfolder_path, \"rfMRI_100.dr\", \"dr_stage1.txt\")\n",
    "\n",
    "        if not os.path.exists(timeseries_file):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        print('Appending files')\n",
    "        timeseries_100 = np.loadtxt(timeseries_file)\n",
    "        timeseries_list.append(timeseries_100)\n",
    "        index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders, sep='\\n')\n",
    "\n",
    "index_list_df = pd.DataFrame(index_list)\n",
    "index_list_df.columns = ['eid']\n",
    "index_list_df.sort_values(by='eid')\n",
    "\n",
    "print('Started instance 2')\n",
    "timeseries_instance_2 = []\n",
    "index_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list, timeseries_list):\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    if mid_part == '2':\n",
    "        timeseries_instance_2.append(timeseries)\n",
    "        index_instance_2.append(folder_name)\n",
    "\n",
    "index_100_df_full = pd.DataFrame(index_list, columns=['eid']).to_csv('/PLS/brain/rs/ica_tangent/files/index_100_ica_full.csv', index=False)\n",
    "index_instance_2_100_df = pd.DataFrame(index_instance_2, columns=['eid'])\n",
    "index_instance_2_100_df['eid'] = index_instance_2_100_df['eid'].str.replace('_20227_2_0', '').astype(int)\n",
    "index_instance_2_100_df.to_csv('/PLS/brain/rs/ica_tangent/files/index_100_ica_instance_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary and save it\n",
    "timeseries_100_dict = {}\n",
    "for index, timeseries in zip(index_instance_2_100_df['eid'], timeseries_instance_2):\n",
    "    timeseries_100_dict[index] = timeseries\n",
    "\n",
    "with open(f'/PLS/brain/rs/ica_tangent/files/timeseries_100_dict.pkl', \"wb\") as f:\n",
    "    pickle.dump(timeseries_100_dict, f)\n",
    "\n",
    "\n",
    "# Load the dictionary\n",
    "with open('/PLS/brain/rs/ica_tangent/files/timeseries_100_dict.pkl', \"rb\") as a:\n",
    "    timeseries_100_dict = pickle.load(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract tangent matrices for 55 'good' components\n",
    "\n",
    "(outlined [here](https://www.fmrib.ox.ac.uk/ukbiobank/group_means/rfMRI_GoodComponents_d100_v1.txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for fold in folds:\n",
    "\n",
    "    print(\"__________________________________________\")\n",
    "\n",
    "    print(f\"Started fold {fold}\")\n",
    "\n",
    "    print('Setting ConnectivityMeasure model')\n",
    "\n",
    "    tangent_measure = ConnectivityMeasure(\n",
    "    kind=\"tangent\",\n",
    "    standardize=\"zscore_sample\",\n",
    "    vectorize = True,\n",
    "    discard_diagonal = True)\n",
    "\n",
    "    # Extract 55 good ICA out of 100\n",
    "\n",
    "    good_55_orig = [2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
    "                40, 41, 42, 43, 45, 46, 48, 49, 50, 52, 53, 57, 58, 60, 63, 64, 93]\n",
    "    good_55 = [ i-1 for i in good_55_orig ]\n",
    "\n",
    "    print(\"Uploading train and test id\")\n",
    "\n",
    "    train_id = pd.read_csv(f'/g_factor_5_folds_python/fold_{fold}/train_id_fold_{fold}.csv')\n",
    "    test_id = pd.read_csv(f'/g_factor_5_folds_python/fold_{fold}/test_id_fold_{fold}.csv')\n",
    "\n",
    "    tangent_100_train = []\n",
    "    tangent_100_test = []\n",
    "    \n",
    "    tangent_train_id = []\n",
    "    tangent_test_id = []\n",
    "    \n",
    "    print(\"Getting train set\")\n",
    "\n",
    "    for id_val in train_id['eid'].values:\n",
    "        if id_val in timeseries_100_dict.keys():\n",
    "            tangent_100_train.append(timeseries_100_dict[id_val])\n",
    "            tangent_train_id.append(id_val)\n",
    "\n",
    "    print(\"Get 55 components, fit, and transform, train set\")\n",
    "\n",
    "    tangent_55_train = np.array(tangent_100_train)[:, :, good_55]\n",
    "    tangent_matrices_55_train = tangent_measure.fit_transform(tangent_55_train)\n",
    "    pd.DataFrame(tangent_matrices_55_train, columns = [f'Component {i+1} Tangent (55 IC)' for i in range(tangent_matrices_55_train.shape[1])], index=tangent_train_id).to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_55_train_fold_{fold}.csv')\n",
    "\n",
    "    print(\"__________________________________________\")\n",
    "    print(\"Getting test set\")\n",
    "            \n",
    "    for id_val in test_id['eid'].values:\n",
    "        if id_val in timeseries_100_dict.keys():\n",
    "            tangent_100_test.append(timeseries_100_dict[id_val])\n",
    "            tangent_test_id.append(id_val)\n",
    "\n",
    "    print(\"Get 55 components and transform, test set\")\n",
    "    tangent_55_test = np.array(tangent_100_test)[:, :, good_55]\n",
    "    tangent_matrices_55_test = tangent_measure.transform(tangent_55_test)\n",
    "    pd.DataFrame(tangent_matrices_55_test, columns = [f'Component {i+1} Tangent (55 IC)' for i in range(tangent_matrices_55_test.shape[1])], index=tangent_test_id).to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_55_test_fold_{fold}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate column names that will reflect connections between components (lower triangular part with diagonal discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_names(components):\n",
    "    column_names = []\n",
    "    for i in range(1, len(components)):\n",
    "        for j in range(i):\n",
    "            column_names.append(f'Component {components[i]} & Component {components[j]} Tangent (55 IC)')\n",
    "    return column_names\n",
    "\n",
    "# Original components\n",
    "good_55_orig = [2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
    "                40, 41, 42, 43, 45, 46, 48, 49, 50, 52, 53, 57, 58, 60, 63, 64, 93]\n",
    "\n",
    "# Generate column names\n",
    "column_names = generate_column_names(good_55_orig)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns to reflect connections between components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in folds:\n",
    "    print(f\"Renaming columns for fold {fold}\")\n",
    "\n",
    "    tangent_matrices_55_train = pd.read_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_55_train_fold_{fold}.csv')\n",
    "    tangent_matrices_55_test = pd.read_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_55_test_fold_{fold}.csv')\n",
    "\n",
    "    # Rename the columns\n",
    "    tangent_matrices_55_train.columns = ['Unnamed: 0'] + column_names\n",
    "    tangent_matrices_55_test.columns = ['Unnamed: 0'] + column_names\n",
    "\n",
    "    # Save the updated DataFrames back to CSV files\n",
    "    tangent_matrices_55_train.to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_renamed/tangent_matrices_55_train_fold_{fold}.csv', index=False)\n",
    "    tangent_matrices_55_test.to_csv(f'/PLS/brain/rs/ica_tangent/tangent_matrices_renamed/tangent_matrices_55_test_fold_{fold}.csv', index=False)\n",
    "\n",
    "    print(f\"Columns renamed for fold {fold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, convert a 1D array back into NxN matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangent_matrices_55_test = pd.read_csv('/PLS/brain/rs/ica_tangent/tangent_matrices_renamed/tangent_matrices_55_test_fold_0.csv')\n",
    "def vector_to_full_matrix(vector):\n",
    "    # Calculate the size of the original matrix N\n",
    "    k = len(vector)\n",
    "    N = int((1 + np.sqrt(1 + 8 * k)) / 2)\n",
    "    \n",
    "    # Initialize an NxN matrix with zeros\n",
    "    matrix = np.zeros((N, N))\n",
    "    \n",
    "    # Fill the lower triangular part (excluding the diagonal)\n",
    "    index = 0\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            matrix[i, j] = vector[index]\n",
    "            matrix[j, i] = vector[index]\n",
    "            index += 1\n",
    "    np.fill_diagonal(matrix, vector[index:index + N])\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Example usage\n",
    "matrix = vector_to_full_matrix(tangent_matrices_55_test.drop(columns='Unnamed: 0').iloc[0].values)\n",
    "matrices = [vector_to_full_matrix(row) for row in tangent_matrices_55_test.drop(columns='Unnamed: 0').iloc[:3].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS on RS ICA tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['tangent_matrices_21', 'tangent_matrices_55']\n",
    "confounds = pd.read_csv('/PLS/brain/rs/ica_main/data_tables/rs_confounds.csv')\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "############## 1\n",
    "seed = 42\n",
    "\n",
    "for modality in modalities:\n",
    "\n",
    "    print(f'Started {modality}', flush=True)\n",
    "\n",
    "    folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    pls_result = {}\n",
    "\n",
    "    for fold in folds:\n",
    "\n",
    "        tangent_train_id = pd.read_csv(f'/PLS/brain/rs/ica_tangent/{modality}_train_fold_{fold}.csv').rename(columns={'Unnamed: 0': 'eid'})\n",
    "        tangent_test_id = pd.read_csv(f'/PLS/brain/rs/ica_tangent/{modality}_test_fold_{fold}.csv').rename(columns={'Unnamed: 0': 'eid'})\n",
    "        \n",
    "        # Match confounds to MRI\n",
    "        print(f'Matching brain data to confounds in {modality} fold {fold}', flush=True)\n",
    "        \n",
    "        print('____Train____')\n",
    "        conf_to_brain_match_train = pd.merge(confounds, tangent_train_id['eid'], on='eid')\n",
    "        conf_to_brain_match_train.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_conf_to_brain_match_train_fold_{fold}.csv', index=False)\n",
    "\n",
    "        brain_to_conf_match_train = pd.merge(conf_to_brain_match_train['eid'], tangent_train_id, on='eid')\n",
    "        brain_to_conf_match_train.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_brain_to_conf_match_train_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        print('____Test___') \n",
    "        conf_to_brain_match_test = pd.merge(confounds, tangent_test_id['eid'], on='eid')\n",
    "        conf_to_brain_match_test.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_conf_to_brain_match_test_fold_{fold}.csv', index=False)\n",
    "\n",
    "        brain_to_conf_match_test = pd.merge(conf_to_brain_match_test['eid'], tangent_test_id, on='eid')\n",
    "        brain_to_conf_match_test.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_brain_to_conf_match_test_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        # Upload g-factor with ID\n",
    "        g_train_full = pd.read_csv(f'/PLS/g_factor/g_train_with_id_fold_{fold}.csv')\n",
    "        g_test_full = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv')\n",
    "\n",
    "        \n",
    "        ############## 2\n",
    "        print(f'Matching confounds to {modality} fold {fold}', flush=True)\n",
    "        \n",
    "        # Match confounds to MRI\n",
    "        print('Getting train and test without IDs')\n",
    "        brain_train, brain_test, conf_train, conf_test = brain_to_conf_match_train.drop(columns=['eid']), brain_to_conf_match_test.drop(columns=['eid']), conf_to_brain_match_train.drop(columns=['eid']), conf_to_brain_match_test.drop(columns=['eid'])\n",
    "        \n",
    "        ############## 3\n",
    "        print(f'Matching g-factor to {modality} fold {fold}', flush=True)\n",
    "        \n",
    "        # Match g-factor back to MRI\n",
    "        print('Metching g-factor to brain')\n",
    "        g_train, g_test, g_train_id, g_test_id = pd.merge(g_train_full, brain_to_conf_match_train['eid'], on='eid').drop(columns=['eid']), pd.merge(g_test_full, brain_to_conf_match_test['eid'], on='eid').drop(columns=['eid']), pd.merge(g_train_full, brain_to_conf_match_train['eid'], on='eid')['eid'], pd.merge(g_test_full, brain_to_conf_match_test['eid'], on='eid')['eid']\n",
    "        g_train.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/g_train_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        g_test.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/g_test_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        g_train_id.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/g_train_id_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        g_test_id.to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/g_test_id_{modality}_matched_fold_{fold}.csv', index=False)\n",
    "        \n",
    "        ############## 4\n",
    "        print(f'Applying ConfoundRegressor to {modality} fold {fold}', flush=True)\n",
    "        \n",
    "        # Apply ConfoundRegressor\n",
    "        features_train_corr, features_test_corr, features_train_scaled, features_test_scaled, scaler_features = confound_regressor_skbold(brain_train, brain_test, conf_train, conf_test)\n",
    "        \n",
    "        pd.DataFrame(features_train_corr, columns = brain_train.columns).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_train_corr_{fold}.csv', index=False)\n",
    "        pd.DataFrame(features_test_corr, columns = brain_test.columns).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_test_corr_{fold}.csv', index=False)\n",
    "        \n",
    "        pd.DataFrame(features_train_scaled, columns = brain_train.columns).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_train_scaled_{fold}.csv', index=False)\n",
    "        pd.DataFrame(features_test_scaled, columns = brain_test.columns).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/{modality}_test_scaled_{fold}.csv', index=False)\n",
    "        \n",
    "\n",
    "        with open(f'/PLS/brain/rs/ica_tangent/fold_{fold}/pls_output/scaler_features_{modality}_fold_{fold}.pkl', \"wb\") as f:\n",
    "            pickle.dump(scaler_features, f)\n",
    "            \n",
    "\n",
    "        # Initiate and run PLS\n",
    "        parameters = {'n_components': range(1, 36, 1)}\n",
    "        pls = PLSRegression()\n",
    "        model = GridSearchCV(pls, parameters, scoring = 'neg_mean_absolute_erro', cv=KFold(10, shuffle = True, random_state=seed), verbose=4, n_jobs = 8)\n",
    "        \n",
    "        \n",
    "        print(f'Fitting PLS to {modality} fold {fold}', flush=True)\n",
    "        model.fit(features_train_corr, np.array(g_train))\n",
    "        \n",
    "        print(f'Model parameters for fold {fold}:', model.cv_results_['params'])\n",
    "        print(f'Mean test score for fold {fold}:', model.cv_results_['mean_test_score'])\n",
    "        print(f'Rank test score for fold {fold}:', model.cv_results_['rank_test_score'])\n",
    "        print(model)\n",
    "        \n",
    "        print(f'Saving PLS model for {modality} fold {fold}')\n",
    "        with open(f'/PLS/brain/rs/ica_tangent/fold_{fold}/models/pkl/{modality}_model_fold_{fold}.pkl', \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "            \n",
    "        print(f'Best params in fold {fold} = ', model.best_params_)\n",
    "        print(f'Best score (neg_mean_absolute_error) in fold {fold} = ', model.best_score_)\n",
    "            \n",
    "        # Predict the values\n",
    "        print(f'Predicting & saving g_test for {modality} fold {fold}', flush=True)\n",
    "        g_pred_test = model.predict(np.array(features_test_corr))\n",
    "        pd.DataFrame(g_pred_test, columns=['g predicted test']).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/g_pred/{modality}_test_fold_{fold}.csv')\n",
    "\n",
    "        g_pred_test_with_id = pd.concat([g_test_id.astype(int), pd.DataFrame(g_pred_test, columns=['g predicted test'])], axis=1).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/g_pred/{modality}_g_pred_test_id_fold_{fold}.csv')\n",
    "\n",
    "        \n",
    "        print(f'Predicting & saving g_train for {modality} fold {fold}', flush=True)\n",
    "        g_pred_train = model.predict(np.array(features_train_corr))\n",
    "        pd.DataFrame(g_pred_train, columns=['g predicted train']).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/g_pred/{modality}_g_pred_train_fold_{fold}.csv')\n",
    "        \n",
    "\n",
    "        g_pred_train_with_id = pd.concat([g_train_id.astype(int), pd.DataFrame(g_pred_train, columns=['g predicted train'])], axis=1).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/g_pred/{modality}_g_pred_train_id_fold_{fold}.csv')\n",
    "        \n",
    "            \n",
    "        print(f\"Fold = {fold}\")\n",
    "        print(\"----------\")\n",
    "        print(\"MSE = \", mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"MAE = \", mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"R2 = \", r2_score(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"Pearson's r = \", pearsonr(np.array(g_test)[:,0], g_pred_test[:,0]))\n",
    "        print(\"----------\")\n",
    "            \n",
    "        pls_result['fold'] = fold\n",
    "        pls_result['modality'] = modality\n",
    "        pls_result['n_components'] = model.best_params_\n",
    "        pls_result['MSE'] = mean_squared_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['MAE'] = mean_absolute_error(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['R2'] = r2_score(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "        pls_result['Pearson '] = pearsonr(np.array(g_test)[:,0], g_pred_test[:,0])\n",
    "            \n",
    "        with open(f'/PLS/brain/rs/ica_tangent/fold_{fold}/models/csv/{modality}_fold_{fold}_PLS_result.csv', 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=pls_result.keys())\n",
    "            writer.writerow(pls_result)\n",
    "            \n",
    "        pls_result.clear()\n",
    "        \n",
    "        corr, pval = stats.pearsonr(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        r2 = r2_score(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        mse = mean_squared_error(np.squeeze(np.array(g_test)), np.squeeze(g_pred_test))\n",
    "        result = pd.DataFrame([modality, fold, corr, pval, r2, mse, model.best_params_], index=['Modality', 'Fold', 'Correlation', 'P-value', 'R2', 'MSE', 'n components'], columns=['Values']).to_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/models/csv/{modality}_fold_{fold}_full_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and average results across five folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_folds = []\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "for modality in modalities:\n",
    "    for fold in folds:\n",
    "        pls = pd.read_csv(f'/PLS/brain/rs/ica_tangent/fold_{fold}/models/csv/{modality}_fold_{fold}_PLS_result.csv', header=None)\n",
    "        pls.columns = ['Fold', 'Modality', 'n components', 'MSE', 'MAE', 'R2', 'Pearson ']\n",
    "        #pls.index = [modality] * len(pls)\n",
    "        five_folds.append(pls)\n",
    "        five_folds_all_modalities = pd.concat(five_folds, ignore_index=False)\n",
    "\n",
    "five_folds_all_modalities['Pearson '] = five_folds_all_modalities['Pearson '].astype(str).str.replace('PearsonRResult\\(statistic=|pvalue=|\\)', '', regex=True)\n",
    "five_folds_all_modalities[['Pearson ', 'p-value']] = five_folds_all_modalities['Pearson '].str.split(',', expand=True).astype(float).round(decimals=3)\n",
    "five_folds_all_modalities = five_folds_all_modalities.round(decimals=3)\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(five_folds_all_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across folds\n",
    "five_folds_all_modalities_mean = five_folds_all_modalities[['R2', 'Pearson ', 'Modality', 'MSE', 'MAE']]\n",
    "five_folds_all_modalities_mean.groupby(['Modality']).mean().round(3).sort_values(by='R2', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukbiobank_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
