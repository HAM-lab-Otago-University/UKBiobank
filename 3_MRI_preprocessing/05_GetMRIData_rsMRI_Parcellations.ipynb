{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31014_aparc_a2009s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\"/Resting_State/rsfMRI_bulk_timeseries/31014_aparc_a2009s/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31014_aparc_a2009s/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31014_aparc_a2009s/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31014_aparc_a2009s/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31014_aparc_a2009s/43367/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list = []\n",
    "index_list = []\n",
    "skipped_folders = []\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.aparc.a2009s.csv.gz\"\n",
    "\n",
    "        timeseries_file = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "            timeseries_list.append(timeseries)\n",
    "            index_list.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file}\")\n",
    "            continue\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index column and extract instance 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_df = pd.DataFrame(index_list)\n",
    "index_list_df.columns = ['ID']\n",
    "\n",
    "timeseries_instance_2 = []\n",
    "index_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list, timeseries_list): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_instance_2.append(timeseries)\n",
    "        index_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_instance_2_df = pd.DataFrame(index_instance_2)\n",
    "index_instance_2_df.columns = ['ID']\n",
    "\n",
    "# Get instance 2\n",
    "index_instance_2_df = index_instance_2_df['ID'].str.replace('_31014_2_0', '')\n",
    "index_instance_2_df = pd.DataFrame(index_instance_2_df, columns=['ID'])\n",
    "index_instance_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_aparc_2009 = index_instance_2_df.copy()\n",
    "id_aparc_2009.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_aparc_2009.csv', index = False)\n",
    "id_aparc_2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_aparc_2009 = timeseries_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_aparc_2009_dict = {}\n",
    "for index, timeseries in zip(id_aparc_2009['ID'], timeseries_aparc_2009):\n",
    "    timeseries_aparc_2009_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_aparc_2009_dict.npy', timeseries_aparc_2009_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full ID list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_aparc_2009_full = index_list_df.copy()\n",
    "id_aparc_2009_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_aparc_2009_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_aparc_2009_full_dict = {}\n",
    "for index, timeseries in zip(index_list, timeseries_list):\n",
    "    timeseries_aparc_2009_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_aparc_2009_full_dict.npy', timeseries_aparc_2009_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract lists from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_aparc_2009_dict_list = [timeseries_aparc_2009_dict[key] for key in timeseries_aparc_2009_dict]\n",
    "timeseries_aparc_2009_dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31015_aparc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths_2 = [\"/Resting_State/rsfMRI_bulk_timeseries/31015_aparc/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31015_aparc/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31015_aparc/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31015_aparc/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31015_aparc/43419/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list_2 = []\n",
    "index_list_2 = []\n",
    "skipped_folders_2 = []\n",
    "\n",
    "for folder_path in folder_paths_2:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.aparc.csv.gz\"\n",
    "\n",
    "        timeseries_file_2 = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file_2):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders_2.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries_2 = pd.read_csv(timeseries_file_2, compression='gzip')\n",
    "            timeseries_list_2.append(timeseries_2)\n",
    "            index_list_2.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file_2}\")\n",
    "            continue\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders_2, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_2_df = pd.DataFrame(index_list_2)\n",
    "index_list_2_df.columns = ['ID']\n",
    "\n",
    "timeseries_2_instance_2 = []\n",
    "index_2_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list_2, timeseries_list_2): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_2_instance_2.append(timeseries)\n",
    "        index_2_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_2_instance_2_df = pd.DataFrame(index_2_instance_2)\n",
    "index_2_instance_2_df.columns = ['ID']\n",
    "\n",
    "# Get instance 2\n",
    "index_2_instance_2_df = index_2_instance_2_df['ID'].str.replace('_31015_2_0', '')\n",
    "index_2_instance_2_df = pd.DataFrame(index_2_instance_2_df, columns=['ID'])\n",
    "index_2_instance_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_aparc = index_2_instance_2_df.copy()\n",
    "id_aparc.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_aparc.csv', index = False)\n",
    "id_aparc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full ID list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_aparc_full = index_list_2_df.copy()\n",
    "id_aparc_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_aparc_full.csv', index = False)\n",
    "id_aparc_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary instance 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_aparc = timeseries_2_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_aparc_dict = {}\n",
    "for index, timeseries in zip(id_aparc['ID'], timeseries_aparc):\n",
    "    timeseries_aparc_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_aparc_dict.npy', timeseries_aparc_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_aparc_full_dict = {}\n",
    "for index, timeseries in zip(index_list_2, timeseries_list_2):\n",
    "    timeseries_aparc_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_aparc_full_dict.npy', timeseries_aparc_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31016_Glasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths_3 = [\"/Resting_State/rsfMRI_bulk_timeseries/31016_Glasser/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31016_Glasser/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31016_Glasser/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31016_Glasser/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31016_Glasser/43419/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list_3 = []\n",
    "index_list_3 = []\n",
    "skipped_folders_3 = []\n",
    "\n",
    "for folder_path in folder_paths_3:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.Glasser.csv.gz\"\n",
    "\n",
    "        timeseries_file_3 = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file_3):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders_3.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries_3 = pd.read_csv(timeseries_file_3, compression='gzip')\n",
    "            timeseries_list_3.append(timeseries_3)\n",
    "            index_list_3.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file_3}\")\n",
    "            continue\n",
    "        \n",
    "        #timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "\n",
    "        #timeseries_list.append(timeseries)\n",
    "        #index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders_3, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_3_df = pd.DataFrame(index_list_3)\n",
    "index_list_3_df.columns = ['ID']\n",
    "\n",
    "timeseries_3_instance_2 = []\n",
    "index_3_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list_3, timeseries_list_3): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_3_instance_2.append(timeseries)\n",
    "        index_3_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_3_instance_2_df = pd.DataFrame(index_3_instance_2)\n",
    "index_3_instance_2_df.columns = ['ID']\n",
    "\n",
    "# Get instance 2\n",
    "index_3_instance_2_df = index_3_instance_2_df['ID'].str.replace('_31016_2_0', '')\n",
    "index_3_instance_2_df = pd.DataFrame(index_3_instance_2_df, columns=['ID'])\n",
    "index_3_instance_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_glasser = index_3_instance_2_df.copy()\n",
    "id_glasser.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_glasser.csv', index = False)\n",
    "id_glasser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_glasser_full = index_list_3_df.copy()\n",
    "id_glasser_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_glasser_full.csv', index = False)\n",
    "id_glasser_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary instance 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_glasser = timeseries_3_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_glasser_dict = {}\n",
    "for index, timeseries in zip(id_glasser['ID'], timeseries_glasser):\n",
    "    timeseries_glasser_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_glasser_dict.npy', timeseries_glasser_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_glasser_full_dict = {}\n",
    "for index, timeseries in zip(index_list_3, timeseries_list_3):\n",
    "    timeseries_glasser_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_glasser_full_dict.npy', timeseries_glasser_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31019_Tian_Subcortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths_4 = [\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/43350/unzipped/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list_4 = []\n",
    "index_list_4 = []\n",
    "skipped_folders_4 = []\n",
    "\n",
    "for folder_path in folder_paths_4:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.Tian_Subcortex_S1_3T.csv.gz\"\n",
    "\n",
    "        timeseries_file_4 = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file_4):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders_4.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries_4 = pd.read_csv(timeseries_file_4, compression='gzip')\n",
    "            timeseries_list_4.append(timeseries_4)\n",
    "            index_list_4.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file_4}\")\n",
    "            continue\n",
    "        \n",
    "        #timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "\n",
    "        #timeseries_list.append(timeseries)\n",
    "        #index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders_4, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_4_df = pd.DataFrame(index_list_4)\n",
    "index_list_4_df.columns = ['ID']\n",
    "#index_list_df.sort_values(by='ID')\n",
    "\n",
    "timeseries_4_instance_2 = []\n",
    "index_4_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list_4, timeseries_list_4): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_4_instance_2.append(timeseries)\n",
    "        index_4_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_4_instance_2_df = pd.DataFrame(index_4_instance_2)\n",
    "index_4_instance_2_df.columns = ['ID']\n",
    "#index_instance_2_df.sort_values(by='ID')\n",
    "\n",
    "# Get instance 2\n",
    "index_4_instance_2_df = index_4_instance_2_df['ID'].str.replace('_31019_2_0', '')\n",
    "index_4_instance_2_df = pd.DataFrame(index_4_instance_2_df, columns=['ID'])\n",
    "index_4_instance_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s1 = index_4_instance_2_df.copy()\n",
    "id_tian_s1.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s1 = timeseries_4_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_tian_s1_dict = {}\n",
    "for index, timeseries in zip(id_tian_s1['ID'], timeseries_tian_s1):\n",
    "    timeseries_tian_s1_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s1_dict.npy', timeseries_tian_s1_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE FULL TIMESERIES DICT and FULL INDEX LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s1_full = index_list_4_df.copy()\n",
    "id_tian_s1_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s1_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s1_full_dict = {}\n",
    "for index, timeseries in zip(index_list_4, timeseries_list_4):\n",
    "    timeseries_tian_s1_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s1_full_dict.npy', timeseries_tian_s1_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths_5 = [\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/43350/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list_5 = []\n",
    "index_list_5 = []\n",
    "skipped_folders_5 = []\n",
    "\n",
    "for folder_path in folder_paths_5:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.Tian_Subcortex_S2_3T.csv.gz\"\n",
    "\n",
    "        timeseries_file_5 = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file_5):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders_5.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries_5 = pd.read_csv(timeseries_file_5, compression='gzip')\n",
    "            timeseries_list_5.append(timeseries_5)\n",
    "            index_list_5.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file_5}\")\n",
    "            continue\n",
    "        \n",
    "        #timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "\n",
    "        #timeseries_list.append(timeseries)\n",
    "        #index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders_5, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_5_df = pd.DataFrame(index_list_5)\n",
    "index_list_5_df.columns = ['ID']\n",
    "\n",
    "timeseries_5_instance_2 = []\n",
    "index_5_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list_5, timeseries_list_5): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_5_instance_2.append(timeseries)\n",
    "        index_5_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_5_instance_2_df = pd.DataFrame(index_5_instance_2)\n",
    "index_5_instance_2_df.columns = ['ID']\n",
    "\n",
    "# Get instance 2\n",
    "index_5_instance_2_df = index_5_instance_2_df['ID'].str.replace('_31019_2_0', '')\n",
    "index_5_instance_2_df = pd.DataFrame(index_5_instance_2_df, columns=['ID'])\n",
    "index_5_instance_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save IDs and make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s2 = index_5_instance_2_df.copy()\n",
    "id_tian_s2.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s2 = timeseries_5_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_tian_s2_dict = {}\n",
    "for index, timeseries in zip(id_tian_s2['ID'], timeseries_tian_s2):\n",
    "    timeseries_tian_s2_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s2_dict.npy', timeseries_tian_s2_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE FULL TIMESERIES DICT and FULL INDEX LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s2_full = index_list_5_df.copy()\n",
    "id_tian_s2_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s2_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s2_full_dict = {}\n",
    "for index, timeseries in zip(index_list_5, timeseries_list_5):\n",
    "    timeseries_tian_s2_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s2_full_dict.npy', timeseries_tian_s2_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths_6 = [\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/43350/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list_6 = []\n",
    "index_list_6 = []\n",
    "skipped_folders_6 = []\n",
    "\n",
    "for folder_path in folder_paths_6:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.Tian_Subcortex_S3_3T.csv.gz\"\n",
    "\n",
    "        timeseries_file_6 = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file_6):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders_6.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries_6 = pd.read_csv(timeseries_file_6, compression='gzip')\n",
    "            timeseries_list_6.append(timeseries_6)\n",
    "            index_list_6.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file_6}\")\n",
    "            continue\n",
    "        \n",
    "        #timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "\n",
    "        #timeseries_list.append(timeseries)\n",
    "        #index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders_6, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_6_df = pd.DataFrame(index_list_6)\n",
    "index_list_6_df.columns = ['ID']\n",
    "\n",
    "timeseries_6_instance_2 = []\n",
    "index_6_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list_6, timeseries_list_6): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_6_instance_2.append(timeseries)\n",
    "        index_6_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_6_instance_2_df = pd.DataFrame(index_6_instance_2)\n",
    "index_6_instance_2_df.columns = ['ID']\n",
    "\n",
    "# Get instance 2\n",
    "index_6_instance_2_df = index_6_instance_2_df['ID'].str.replace('_31019_2_0', '')\n",
    "index_6_instance_2_df = pd.DataFrame(index_6_instance_2_df, columns=['ID'])\n",
    "index_6_instance_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save IDs and make a dictionary for INSTANCE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s3 = index_6_instance_2_df.copy()\n",
    "id_tian_s3.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s3 = timeseries_6_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_tian_s3_dict = {}\n",
    "for index, timeseries in zip(id_tian_s3['ID'], timeseries_tian_s3):\n",
    "    timeseries_tian_s3_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s3_dict.npy', timeseries_tian_s3_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE FULL TIMESERIES DICT and FULL INDEX LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_6_df = pd.DataFrame(index_list_6, columns=['ID'])\n",
    "id_tian_s3_full = index_list_6_df.copy()\n",
    "id_tian_s3_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s3_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s3_full_dict = {}\n",
    "for index, timeseries in zip(index_list_6, timeseries_list_6):\n",
    "    timeseries_tian_s3_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s3_full_dict.npy', timeseries_tian_s3_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths_7 = [\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31019_Tian_Subcortex/43350/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "\n",
    "timeseries_list_7 = []\n",
    "index_list_7 = []\n",
    "skipped_folders_7 = []\n",
    "\n",
    "for folder_path in folder_paths_7:\n",
    "\n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.Tian_Subcortex_S4_3T.csv.gz\"\n",
    "\n",
    "        timeseries_file_7 = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file_7):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders_7.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries_7 = pd.read_csv(timeseries_file_7, compression='gzip')\n",
    "            timeseries_list_7.append(timeseries_7)\n",
    "            index_list_7.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file_7}\")\n",
    "            continue\n",
    "        \n",
    "        #timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "\n",
    "        #timeseries_list.append(timeseries)\n",
    "        #index_list.append(subject_folder_name)\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders_7, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sublist in timeseries_list_6:\n",
    "    sub_array = np.array(sublist)\n",
    "    sub_shape = sub_array.shape\n",
    "    print(sub_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_7_df = pd.DataFrame(index_list_7)\n",
    "index_list_7_df.columns = ['ID']\n",
    "timeseries_7_instance_2 = []\n",
    "index_7_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list_7, timeseries_list_7): # Split folder name by underscore and get the first and last parts\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_7_instance_2.append(timeseries)\n",
    "        index_7_instance_2.append(folder_name)\n",
    "\n",
    "# Convert instance 2 indices to the data frame\n",
    "index_7_instance_2_df = pd.DataFrame(index_7_instance_2)\n",
    "index_7_instance_2_df.columns = ['ID']\n",
    "#index_instance_2_df.sort_values(by='ID')\n",
    "\n",
    "# Get instance 2\n",
    "index_7_instance_2_df = index_7_instance_2_df['ID'].str.replace('_31019_2_0', '')\n",
    "index_7_instance_2_df = pd.DataFrame(index_7_instance_2_df, columns=['ID'])\n",
    "index_7_instance_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s4 = index_7_instance_2_df.copy()\n",
    "id_tian_s4.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s4 = timeseries_7_instance_2.copy()\n",
    "# Create a dictionary to store the timeseries data with the index as the key\n",
    "timeseries_tian_s4_dict = {}\n",
    "for index, timeseries in zip(id_tian_s4['ID'], timeseries_tian_s4):\n",
    "    timeseries_tian_s4_dict[index] = timeseries\n",
    "#np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s4_dict.npy', timeseries_tian_s4_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s4_dict.npy', timeseries_tian_s4_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE FULL TIMESERIES DICT and FULL INDEX LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s4_full = index_list_7_df.copy()\n",
    "id_tian_s4_full.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s4_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_tian_s4_full_dict = {}\n",
    "for index, timeseries in zip(index_list_7, timeseries_list_7):\n",
    "    timeseries_tian_s4_full_dict[index] = timeseries\n",
    "np.save('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/timeseries_tian_s4_full_dict.npy', timeseries_tian_s4_full_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Tian with aparc, aparc.a2009s, and Glasser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aparc.a2009s:**\n",
    "- index_instance_2_df\n",
    "- timeseries_instance_2\n",
    "\n",
    "**aparc:**\n",
    "- index_2_instance_2_df \n",
    "- timeseries_2_instance_2\n",
    "\n",
    "**Glasser:**\n",
    "- index_3_instance_2_df \n",
    "- timeseries_3_instance_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tian S1:**\n",
    "- index_4_instance_2_df \n",
    "- timeseries_4_instance_2\n",
    "\n",
    "\n",
    "**Tian S1:**\n",
    "- index_5_instance_2_df \n",
    "- timeseries_5_instance_2\n",
    "\n",
    "**Tian S3:**\n",
    "- index_6_instance_2_df \n",
    "- timeseries_6_instance_2\n",
    "\n",
    "\n",
    "**Tian S4:**\n",
    "- index_7_instance_2_df \n",
    "- timeseries_7_instance_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glasser / aparc + Tian S1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tian_s1 = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s1.csv')\n",
    "id_tian_s4 = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_tian_s4.csv')\n",
    "id_glasser = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_glasser.csv')\n",
    "#id_aparc = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_aparc.csv')\n",
    "#id_aparc_2009 = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/id_aparc_2009.csv')\n",
    "\n",
    "\n",
    "id_tian_s1['ID'] = id_tian_s1['ID'].astype(int)\n",
    "id_tian_s4['ID'] = id_tian_s4['ID'].astype(int)\n",
    "id_glasser['ID'] = id_glasser['ID'].astype(int)\n",
    "#id_aparc['ID'] = id_aparc['ID'].astype(int)\n",
    "#id_aparc_2009['ID'] = id_aparc_2009['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two sets of IDs to select commmon ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glasser + Tian S1\n",
    "glasser_Tian_s1_id = pd.merge(id_glasser, id_tian_s1, on = 'ID')\n",
    "glasser_Tian_s1_id['ID'] = glasser_Tian_s1_id['ID'].astype(int)\n",
    "# Save\n",
    "glasser_Tian_s1_id.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/glasser_Tian_s1_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glasser + Tian S4\n",
    "glasser_Tian_s4_id = pd.merge(id_glasser, id_tian_s4, on = 'ID')\n",
    "glasser_Tian_s4_id['ID'] = glasser_Tian_s4_id['ID'].astype(int)\n",
    "# Save\n",
    "glasser_Tian_s4_id.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/glasser_Tian_s4_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparc + Tian S1\n",
    "aparc_Tian_s1_id = pd.merge(id_aparc, id_tian_s1, on = 'ID')\n",
    "aparc_Tian_s1_id['ID'] = aparc_Tian_s1_id['ID'].astype(int)\n",
    "# Save\n",
    "aparc_Tian_s1_id.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/aparc_Tian_s1_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparc.a2009s + Tian S1\n",
    "aparc_2009_Tian_s1_id = pd.merge(id_aparc_2009, id_tian_s1, on = 'ID')\n",
    "aparc_2009_Tian_s1_id['ID'] = aparc_2009_Tian_s1_id['ID'].astype(int)\n",
    "# Save\n",
    "aparc_2009_Tian_s1_id.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/aparc_2009_Tian_s1_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert IDs to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glasser + Tian S1\n",
    "glasser_Tian_s1_id = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/glasser_Tian_s1_id.csv')\n",
    "glasser_Tian_s1_id_list = glasser_Tian_s1_id['ID'].tolist()\n",
    "glasser_Tian_s1_id_str = [str(x) for x in glasser_Tian_s1_id_list]\n",
    "glasser_Tian_s1_id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glasser + Tian S4\n",
    "glasser_Tian_s4_id = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/glasser_Tian_s4_id.csv')\n",
    "glasser_Tian_s4_id_list = glasser_Tian_s4_id['ID'].tolist()\n",
    "glasser_Tian_s4_id_str = [str(x) for x in glasser_Tian_s4_id_list]\n",
    "glasser_Tian_s4_id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparc + Tian S1\n",
    "aparc_Tian_s1_id = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/aparc_Tian_s1_id.csv')\n",
    "aparc_Tian_s1_id_list = aparc_Tian_s1_id['ID'].tolist()\n",
    "aparc_Tian_s1_id_str = [str(x) for x in aparc_Tian_s1_id_list]\n",
    "aparc_Tian_s1_id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparc.a2009s + Tian S1\n",
    "aparc_2009_Tian_s1_id = pd.read_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/aparc_2009_Tian_s1_id.csv')\n",
    "aparc_2009_Tian_s1_id_list = aparc_2009_Tian_s1_id['ID'].tolist()\n",
    "aparc_2009_Tian_s1_id_str = [str(x) for x in aparc_2009_Tian_s1_id_list]\n",
    "aparc_2009_Tian_s1_id_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge two dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glasser + Tian S1\n",
    "# Merge making a new dictionary\n",
    "glasser_Tian_s1_dict_df = {}\n",
    "for key, value in timeseries_tian_s1_dict.items():\n",
    "    if key in timeseries_glasser_dict:\n",
    "        glasser_Tian_s1_dict_df[key] = pd.concat([value, timeseries_glasser_dict[key]], ignore_index=True)\n",
    "\n",
    "len(glasser_Tian_s1_dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glasser + Tian S4\n",
    "# Merge making a new dictionary\n",
    "glasser_Tian_s4_dict_df = {}\n",
    "for key, value in timeseries_tian_s4_dict.items():\n",
    "    if key in timeseries_glasser_dict:\n",
    "        glasser_Tian_s4_dict_df[key] = pd.concat([value, timeseries_glasser_dict[key]], ignore_index=True)\n",
    "\n",
    "len(glasser_Tian_s4_dict_df)\n",
    "\n",
    "# 2743001 is empty in Tian 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparc + Tian S1\n",
    "# Merge making a new dictionary\n",
    "aparc_Tian_s1_dict_df = {}\n",
    "for key, value in timeseries_tian_s1_dict.items():\n",
    "    if key in timeseries_aparc_dict:\n",
    "        aparc_Tian_s1_dict_df[key] = pd.concat([value, timeseries_aparc_dict[key]], ignore_index=True)\n",
    "\n",
    "len(aparc_Tian_s1_dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aparc.a2009s + Tian S1\n",
    "# Merge making a new dictionary\n",
    "aparc_2009_Tian_s1_dict_df = {}\n",
    "for key, value in timeseries_tian_s1_dict.items():\n",
    "    if key in timeseries_aparc_2009_dict:\n",
    "        aparc_2009_Tian_s1_dict_df[key] = pd.concat([value, timeseries_aparc_2009_dict[key]], ignore_index=True)\n",
    "\n",
    "len(aparc_2009_Tian_s1_dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_glasser = list(timeseries_glasser_dict.keys())\n",
    "keys_tian_s2 = list(timeseries_tian_s2_dict.keys())\n",
    "len_glasser = len(keys_glasser)\n",
    "len_tian_s2 = len(keys_tian_s2)\n",
    "len_glasser_unique = len(set(keys_glasser))\n",
    "len_tian_s2_unique = len(set(keys_tian_s2))\n",
    "\n",
    "print(f\"The length of the keys of timeseries_glasser_dict is {len_glasser}\")\n",
    "print(f\"The length of the unique keys of timeseries_glasser_dict is {len_glasser_unique}\")\n",
    "print(f\"The length of the keys of timeseries_tian_s2_dict is {len_tian_s2}\")\n",
    "print(f\"The length of the unique keys of timeseries_tian_s2_dict is {len_tian_s2_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each data frame separately\n",
    "for i, df in enumerate(glasser_Tian_s1_dict_df):\n",
    "    # Write each DataFrame to a pickle file with a unique name\n",
    "    df.to_pickle(f\"glasser_Tian_s1_dict_df_{i}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full Tian S1 glasser\n",
    "dir = '/ML_DATASETS/Brain/rsMRI/bulk_timeseries/merged_dict'\n",
    "file_name = 'glasser_Tian_s1_dict_df.pkl'\n",
    "fname = os.path.join(dir, file_name)\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(glasser_Tian_s1_dict_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full Tian S4\n",
    "dir = '/ML_DATASETS/Brain/rsMRI/bulk_timeseries/merged_dict'\n",
    "file_name = 'glasser_Tian_s4_dict_df.pkl'\n",
    "fname = os.path.join(dir, file_name)\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(glasser_Tian_s4_dict_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full Tian S1 aparc\n",
    "dir = '/ML_DATASETS/Brain/rsMRI/bulk_timeseries/merged_dict'\n",
    "file_name = 'aparc_Tian_s1_dict_df.pkl'\n",
    "fname = os.path.join(dir, file_name)\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(aparc_Tian_s1_dict_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full Tian S1 aparc.a2009s\n",
    "dir = '/ML_DATASETS/Brain/rsMRI/bulk_timeseries/merged_dict'\n",
    "file_name = 'aparc_2009_Tian_s1_dict_df.pkl'\n",
    "fname = os.path.join(dir, file_name)\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(aparc_2009_Tian_s1_dict_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list out of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 Glasser\n",
    "glasser_Tian_s1_dict_df_onelist = [glasser_Tian_s1_dict_df[key] for key in glasser_Tian_s1_dict_df]\n",
    "glasser_Tian_s1_dict_df_onelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S4 Glasser\n",
    "glasser_Tian_s4_dict_df_onelist = [glasser_Tian_s4_dict_df[key] for key in glasser_Tian_s4_dict_df]\n",
    "glasser_Tian_s4_dict_df_onelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc\n",
    "aparc_Tian_s1_dict_df_onelist = [aparc_Tian_s1_dict_df[key] for key in aparc_Tian_s1_dict_df]\n",
    "aparc_Tian_s1_dict_df_onelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc.a2009s\n",
    "aparc_2009_Tian_s1_dict_df_onelist = [aparc_2009_Tian_s1_dict_df[key] for key in aparc_2009_Tian_s1_dict_df]\n",
    "aparc_2009_Tian_s1_dict_df_onelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns which contain names of the structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tian S1 Glasser\n",
    "glasser_Tian_s1_t = glasser_Tian_s1_dict_df.copy()\n",
    "for key, df in glasser_Tian_s1_t.items():\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    glasser_Tian_s1_t[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tian S4\n",
    "glasser_Tian_s4_t = glasser_Tian_s4_dict_df.copy()\n",
    "for key, df in glasser_Tian_s4_t.items():\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    glasser_Tian_s4_t[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tian S1 aparc\n",
    "aparc_Tian_s1_t = aparc_Tian_s1_dict_df.copy()\n",
    "for key, df in aparc_Tian_s1_t.items():\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    aparc_Tian_s1_t[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tian S1 aparc.a2009s\n",
    "aparc_2009_Tian_s1_t = aparc_2009_Tian_s1_dict_df.copy()\n",
    "for key, df in aparc_2009_Tian_s1_t.items():\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    aparc_2009_Tian_s1_t[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 Glasser\n",
    "for i in range(len(glasser_Tian_s1_t)):\n",
    "    glasser_Tian_s1_t[i] = glasser_Tian_s1_t[i].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S4\n",
    "for i in range(len(glasser_Tian_s4_t)):\n",
    "    glasser_Tian_s4_t[i] = glasser_Tian_s4_t[i].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc\n",
    "for i in range(len(aparc_Tian_s1_t)):\n",
    "    aparc_Tian_s1_t[i] = aparc_Tian_s1_t[i].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc.a2009s\n",
    "for i in range(len(aparc_2009_Tian_s1_t)):\n",
    "    aparc_2009_Tian_s1_t[i] = aparc_2009_Tian_s1_t[i].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 Glasser\n",
    "for i in range(len(glasser_Tian_s1_t)):\n",
    "    glasser_Tian_s1_t[i] = glasser_Tian_s1_t[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S4\n",
    "for i in range(len(glasser_Tian_s4_t)):\n",
    "    glasser_Tian_s4_t[i] = glasser_Tian_s4_t[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc\n",
    "for i in range(len(aparc_Tian_s1_t)):\n",
    "    aparc_Tian_s1_t[i] = aparc_Tian_s1_t[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc.a2009s\n",
    "for i in range(len(aparc_2009_Tian_s1_t)):\n",
    "    aparc_2009_Tian_s1_t[i] = aparc_2009_Tian_s1_t[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final arrays for Glasser - Tian S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 Glasser\n",
    "glasser_Tian_s1_arrays = []\n",
    "for i in range(len(glasser_Tian_s1_t)):\n",
    "    array = np.array(glasser_Tian_s1_t[i])\n",
    "    glasser_Tian_s1_arrays.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S4 Glasser\n",
    "glasser_Tian_s4_arrays = []\n",
    "for i in range(len(glasser_Tian_s4_t)):\n",
    "    array = np.array(glasser_Tian_s4_t[i])\n",
    "    glasser_Tian_s4_arrays.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc\n",
    "aparc_Tian_s1_arrays = []\n",
    "for i in range(len(aparc_Tian_s1_t)):\n",
    "    array = np.array(aparc_Tian_s1_t[i])\n",
    "    aparc_Tian_s1_arrays.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tian S1 aparc.a2009s\n",
    "aparc_2009_Tian_s1_arrays = []\n",
    "for i in range(len(aparc_2009_Tian_s1_t)):\n",
    "    array = np.array(aparc_2009_Tian_s1_t[i])\n",
    "    aparc_2009_Tian_s1_arrays.append(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schaefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\"/Resting_State/rsfMRI_bulk_timeseries/31018_Schaefer7ns/10000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31018_Schaefer7ns/20000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31018_Schaefer7ns/30000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31018_Schaefer7ns/40000/unzipped/\",\n",
    "\"/Resting_State/rsfMRI_bulk_timeseries/31018_Schaefer7ns/43419/unzipped/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_count = 0\n",
    "timeseries_list = []\n",
    "index_list = []\n",
    "skipped_folders = []\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    \n",
    "    for subject_folder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subject_folder)\n",
    "        subject_folder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        gz_file = \"fMRI.Schaefer7n200p.csv.gz\" # can be replaced with any other Schaefer7 atlas\n",
    "\n",
    "        timeseries_file = os.path.join(subfolder_path, gz_file)\n",
    "\n",
    "        if not os.path.exists(timeseries_file):\n",
    "            missing_file_count += 1\n",
    "            skipped_folders.append(subfolder_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timeseries = pd.read_csv(timeseries_file, compression='gzip')\n",
    "            timeseries_list.append(timeseries)\n",
    "            index_list.append(subject_folder_name)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {timeseries_file}\")\n",
    "            continue\n",
    "\n",
    "print('Number of folders without the file:', missing_file_count)\n",
    "print('Skipped folders:', *skipped_folders, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_df = pd.DataFrame(index_list, columns = ['ID'])\n",
    "index_list_df.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/Schaefer/id_Schaefer7n200p_full_id.csv', index=False)\n",
    "\n",
    "timeseries_instance_2 = []\n",
    "index_instance_2 = []\n",
    "for folder_name, timeseries in zip(index_list, timeseries_list):\n",
    "    id_part = folder_name.split(\"_\")[0]\n",
    "    mid_part = folder_name.split(\"_\")[-2]\n",
    "    id_part = int(id_part)\n",
    "    if mid_part == '2':\n",
    "        timeseries_instance_2.append(timeseries)\n",
    "        index_instance_2.append(folder_name)\n",
    "\n",
    "index_instance_2_df = pd.DataFrame(index_instance_2, columns = ['ID'])\n",
    "index_instance_2_df['ID'] = index_instance_2_df['ID'].str.replace('_31018_2_0', '')\n",
    "index_instance_2_df.to_csv('/ML_DATASETS/Brain/rsMRI/bulk_timeseries/Schaefer/id_Schaefer7n200p.csv', index = False)\n",
    "index_instance_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_Schaefer7n200p_dict = {}\n",
    "for index, timeseries in zip(index_instance_2_df['ID'], timeseries_instance_2):\n",
    "    timeseries_Schaefer7n200p_dict[index] = timeseries\n",
    "len(timeseries_Schaefer7n200p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/ML_DATASETS/Brain/rsMRI/bulk_timeseries/Schaefer'\n",
    "fname = os.path.join(dir, 'timeseries_Schaefer7n200p_dict.pkl')\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(timeseries_Schaefer7n200p_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_Schaefer7n200p_full_dict = {}\n",
    "for index, timeseries in zip(index_list, timeseries_list):\n",
    "    timeseries_Schaefer7n200p_full_dict[index] = timeseries    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(dir, 'timeseries_Schaefer7n200p_full_dict.pkl')\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(timeseries_Schaefer7n200p_full_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code was run on the cluster using slurm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Started loading timeseries_schaefer7n200p_dict', flush=True)\n",
    "with open('/Timeseries/timeseries_schaefer7n200p_dict.pkl', 'rb') as f:\n",
    "    timeseries_schaefer7n200p_dict = pickle.load(f)\n",
    "\n",
    "print('Finihed loading timeseries_schaefer7n200p_dict', flush=True)\n",
    "print('------------------------------------------------------------------------', flush=True)\n",
    "\n",
    "print('Started loading timeseries_instance_2_Tian_Subcortex_S1_3T_dict', flush=True)\n",
    "with open('/Timeseries/timeseries_instance_2_Tian_Subcortex_S1_3T_dict.pkl', 'rb') as f:\n",
    "    timeseries_instance_2_Tian_Subcortex_S1_3T_dict = pickle.load(f)\n",
    "print('Finihed loading timeseries_instance_2_Tian_Subcortex_S1_3T_dict', flush=True)\n",
    "print('------------------------------------------------------------------------', flush=True)\n",
    "\n",
    "# schaefer7n200p_Tian_Subcortex_S1\n",
    "print('Started making schaefer7n200p_Tian_Subcortex_S1', flush=True)\n",
    "schaefer7n200p_Tian_Subcortex_S1 = {}\n",
    "for key, value in timeseries_instance_2_Tian_Subcortex_S1_3T_dict.items():\n",
    "    if key in timeseries_schaefer7n200p_dict:\n",
    "        schaefer7n200p_Tian_Subcortex_S1[key] = pd.concat([value, timeseries_schaefer7n200p_dict[key]], ignore_index=True)\n",
    "print('Finihed making schaefer7n200p_Tian_Subcortex_S1', flush=True)\n",
    "print('Saving schaefer7n200p_Tian_Subcortex_S1', flush=True)\n",
    "with open('/Timeseries/schaefer7n200p_Tian_Subcortex_S1.pkl', 'wb') as f:\n",
    "    pickle.dump(schaefer7n200p_Tian_Subcortex_S1, f)\n",
    "print(len(schaefer7n200p_Tian_Subcortex_S1), flush=True)\n",
    "print('------------------------------------------------------------------------', flush=True)\n",
    "\n",
    "# schaefer7n200p_Tian_Subcortex_S4\n",
    "print('Started loading timeseries_instance_2_Tian_Subcortex_S4_3T_dict', flush=True)\n",
    "with open('/Timeseries/timeseries_instance_2_Tian_Subcortex_S4_3T_dict.pkl', 'rb') as f:\n",
    "    timeseries_instance_2_Tian_Subcortex_S4_3T_dict = pickle.load(f)\n",
    "print('Finihed loading timeseries_instance_2_Tian_Subcortex_S4_3T_dict', flush=True)\n",
    "print('------------------------------------------------------------------------', flush=True)\n",
    "\n",
    "print('Started making schaefer7n200p_Tian_Subcortex_S4', flush=True)\n",
    "schaefer7n200p_Tian_Subcortex_S4 = {}\n",
    "for key, value in timeseries_instance_2_Tian_Subcortex_S4_3T_dict.items():\n",
    "    if key in timeseries_schaefer7n200p_dict:\n",
    "        schaefer7n200p_Tian_Subcortex_S4[key] = pd.concat([value, timeseries_schaefer7n200p_dict[key]], ignore_index=True)\n",
    "print('Finihed making schaefer7n200p_Tian_Subcortex_S4', flush=True)\n",
    "print('Saving making schaefer7n200p_Tian_Subcortex_S4', flush=True)\n",
    "with open('/Timeseries/schaefer7n200p_Tian_Subcortex_S4.pkl', 'wb') as f:\n",
    "    pickle.dump(schaefer7n200p_Tian_Subcortex_S4, f)\n",
    "print(len(schaefer7n200p_Tian_Subcortex_S4), flush=True)\n",
    "print('DONE', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukbiobank_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
