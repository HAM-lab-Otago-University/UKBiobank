{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, MaxNLocator\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import matplotlib_venn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import matplotlib.patheffects as path_effects\n",
    "import matplotlib.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for CA\n",
    "def commonality_analysis(features_g_pred, target_g_real):\n",
    "    if len(features_g_pred.shape)<2:\n",
    "        r2 = LinearRegression().fit(features_g_pred.values.reshape(-1, 1), target_g_real).score(features_g_pred.values.reshape(-1, 1), target_g_real).round(4)\n",
    "    else:\n",
    "        r2 = LinearRegression().fit(features_g_pred, target_g_real).score(features_g_pred, target_g_real).round(4)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main analysis: no demo, only brain and mental health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RS Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "folds = ['0','1','2','3','4']\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "all_g_mh_rs_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_rs_idp_rf = pd.read_csv(f'/PLS/brain/stacking/g/RS_IDP_Timeseries_best_metrics_target_pred_2nd_level_rf_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_rs_idp_rf, on='eid').drop(columns=['eid'])\n",
    "    all_g.columns = ['g_real', 'g_pred_mh', 'g_pred_rs_ts']\n",
    "    all_g_mh_rs_concat.append(all_g)\n",
    "    all_g_mh_rs = pd.concat(all_g_mh_rs_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_rs.to_csv('/commonality_analysis/rs/g_real_pred_mh_rs-idp-atlases.csv', index=False)\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh_rs = model.fit(all_g_mh_rs['g_pred_mh'].values.reshape(-1, 1), all_g_mh_rs['g_real']).score(all_g_mh_rs['g_pred_mh'].values.reshape(-1, 1), all_g_mh_rs['g_real'])\n",
    "r2_rs = model.fit(all_g_mh_rs['g_pred_rs_ts'].values.reshape(-1, 1), all_g_mh_rs['g_real']).score(all_g_mh_rs['g_pred_rs_ts'].values.reshape(-1, 1), all_g_mh_rs['g_real'])\n",
    "r2_mh_and_rs = model.fit(pd.concat([all_g_mh_rs['g_pred_rs_ts'], all_g_mh_rs['g_pred_mh']], axis=1), all_g_mh_rs['g_real']).score(pd.concat([all_g_mh_rs['g_pred_rs_ts'], all_g_mh_rs['g_pred_mh']], axis=1), all_g_mh_rs['g_real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_rs = pd.read_csv('/commonality_analysis/rs/g_real_pred_mh_rs-idp-atlases.csv')\n",
    "r2_mh_rs = commonality_analysis(all_g_mh_rs['g_pred_mh'], all_g_mh_rs['g_real'])\n",
    "r2_rs = commonality_analysis(all_g_mh_rs['g_pred_rs_ts'], all_g_mh_rs['g_real'])\n",
    "r2_mh_and_rs = commonality_analysis(pd.concat([all_g_mh_rs['g_pred_rs_ts'], all_g_mh_rs['g_pred_mh']], axis=1), all_g_mh_rs['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "u_mh_rs = r2_mh_and_rs - r2_rs\n",
    "u_rs = r2_mh_and_rs - r2_mh_rs\n",
    "\n",
    "c_mh_and_rs = r2_mh_and_rs - u_mh_rs - u_rs\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh_rs.round(3)) # f\"{r2_mh_rs_m:.5f}\")\n",
    "print(\"R squared for RS\", r2_rs.round(3))\n",
    "print(\"R squared for MH and RS\", r2_mh_and_rs.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh_rs.round(3)) #\"{:.5f}\".format(u_mh_rs)) # \"{:.5f}\".format(u_mh_rs)\n",
    "print(\"Unique variance for RS:\", u_rs.round(3)) #f\"{u_rs:.5f}\")\n",
    "\n",
    "print('_______________')\n",
    "\n",
    "print(\"Common variance for MH and RS:\", c_mh_and_rs.round(3))\n",
    "print('_______________')\n",
    "perc_mh = u_mh_rs / r2_mh_and_rs * 100 #u_mh_rs / R squared for MH and RS\n",
    "perc_rs = u_rs / r2_mh_and_rs * 100 #u_rs / R squared for MH and RS\n",
    "perc_c = c_mh_and_rs / r2_mh_and_rs * 100 #c_mh_and_rs / R squared for MH and RS\n",
    "\n",
    "print(f\"Proportion of unique variance attributed to MH is {perc_mh.round(2)}\")\n",
    "print(f\"Proportion of unique variance attributed to Resting state (atlases) is {perc_rs.round(2)}\")\n",
    "print(f\"Proportion of common variance attributed to MH and Resting state (atlases) is {perc_c.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RS PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Match g: original, from mental health, and from resting state\n",
    "folds = [f'{i}' for i in range(5)]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "pls_g_mh_rs_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_pls_test_rs = pd.read_csv(f'/PLS/brain/stacking/features_test_level1_stacked/rs_idp_ts_best_metrics/features_test_level1_g_matched_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    pls_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_pls_test_rs, on='eid').drop(columns=['eid'])\n",
    "    pls_g_mh_rs_concat.append(pls_g)\n",
    "    pls_g_mh_rs = pd.concat(pls_g_mh_rs_concat, axis=0, ignore_index=True)\n",
    "    pls_g_mh_rs.to_csv('/commonality_analysis/rs/pls/g_real_pred_mh_rs-pls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Extract R2 with commonality_analysis function\n",
    "pls_g_mh_rs = pd.read_csv('/commonality_analysis/rs/pls/g_real_pred_mh_rs-pls.csv')\n",
    "r2_mh_from_rs_pls = LinearRegression().fit(pls_g_mh_rs['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_rs['g']).score(pls_g_mh_rs['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_rs['g'])\n",
    "\n",
    "rs_cols= [\n",
    "    'aparc_s1_full_correlation', 'aparc_2009_s1_full_correlation', \n",
    "    'glasser_s1_full_correlation', 'glasser_s4_full_correlation', \n",
    "    'Schaefer7n200p_s1_full_correlation', 'Schaefer7n500p_s4_full_correlation', \n",
    "    'amplitudes_21', 'amplitudes_55', 'tangent_matrices_21', 'tangent_matrices_55'\n",
    "]\n",
    "r2_rs = {}\n",
    "for col in rs_cols:\n",
    "    r2_score = commonality_analysis(pls_g_mh_rs[col], pls_g_mh_rs['g'])\n",
    "    r2_rs[col] = r2_score\n",
    "    globals()[f'r2_{col}'] = r2_score\n",
    "\n",
    "# Iterate through each column to combine with 'g_pred_mh'\n",
    "r2_mh_and_rs = {}\n",
    "for col in rs_cols:\n",
    "    combined_columns = pd.concat([pls_g_mh_rs[col], pls_g_mh_rs['g_pred_mh']], axis=1)\n",
    "    r2_combined = commonality_analysis(combined_columns, pls_g_mh_rs['g'])\n",
    "    r2_mh_and_rs[f'r2_mh_and_{col}'] = r2_combined\n",
    "    # Create a variable for each combined R^2 score\n",
    "    globals()[f'r2_mh_and_{col}'] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save R2 dictionaries\n",
    "with open('/commonality_analysis/rs/pls/r2_rs.pkl', 'wb') as f:\n",
    "    pickle.dump(r2_rs, f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/r2_mh_and_rs.pkl', 'wb') as f:\n",
    "    pickle.dump(r2_mh_and_rs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Calculate  unique and common variances\n",
    "unique_mh = {}\n",
    "unique_rs = {}\n",
    "common_mh_rs = {}\n",
    "\n",
    "for key in r2_mh_and_rs:\n",
    "    rs_key = key.replace('r2_mh_and_', '')\n",
    "    u_mh = r2_mh_and_rs[key] - r2_rs[rs_key]\n",
    "    u_rs = r2_mh_and_rs[key] - r2_mh_from_rs_pls #[key]\n",
    "    c_mh_and_rs = r2_mh_and_rs[key] - u_mh - u_rs\n",
    "    \n",
    "    unique_mh[f'u_mh_from_{rs_key}'] = u_mh\n",
    "    unique_rs[f'u_rs_{rs_key}'] = u_rs\n",
    "    common_mh_rs[f'c_mh_and_rs_{rs_key}'] = c_mh_and_rs\n",
    "\n",
    "# Print results\n",
    "for key in r2_mh_and_rs:\n",
    "    rs_key = key.replace('r2_mh_and_', '')\n",
    "    print(f\"R squared for {rs_key}: {r2_rs[rs_key]:.3f}\")\n",
    "    print(f\"R squared for MH and {rs_key}: {r2_mh_and_rs[key]:.3f}\")\n",
    "    print(f\"Unique variance for MH ({rs_key}): {unique_mh[f'u_mh_from_{rs_key}']:.3f}\")\n",
    "    print(f\"Unique variance for RS ({rs_key}): {unique_rs[f'u_rs_{rs_key}']:.3f}\")\n",
    "    print(f\"Common variance for MH and RS ({rs_key}): {common_mh_rs[f'c_mh_and_rs_{rs_key}']:.3f}\")\n",
    "    print('_______________')\n",
    "\n",
    "# Calculate proportions\n",
    "for key in r2_mh_and_rs:\n",
    "    rs_key = key.replace('r2_mh_and_', '')\n",
    "    perc_mh = unique_mh[f'u_mh_from_{rs_key}'] / r2_mh_and_rs[key] * 100\n",
    "    perc_rs = unique_rs[f'u_rs_{rs_key}'] / r2_mh_and_rs[key] * 100\n",
    "    perc_c = common_mh_rs[f'c_mh_and_rs_{rs_key}'] / r2_mh_and_rs[key] * 100\n",
    "    \n",
    "    print(f\"Proportion of unique variance attributed to MH ({rs_key}): {perc_mh:.2f}%\")\n",
    "    print(f\"Proportion of unique variance attributed to RS ({rs_key}): {perc_rs:.2f}%\")\n",
    "    print(f\"Proportion of common variance attributed to MH and RS ({rs_key}): {perc_c:.2f}%\")\n",
    "    print('_______________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unique and common variances\n",
    "with open('/commonality_analysis/rs/pls/unique_mh.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_mh, f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/unique_rs.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_rs, f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/common_mh_rs.pkl', 'wb') as f:\n",
    "    pickle.dump(common_mh_rs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload CA variables for RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "pls_g_mh_rs = pd.read_csv('/commonality_analysis/rs/pls/g_real_pred_mh_rs-pls.csv')\n",
    "r2_mh_from_rs_pls = LinearRegression().fit(pls_g_mh_rs['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_rs['g']).score(pls_g_mh_rs['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_rs['g'])\n",
    "\n",
    "# Load R2 dictionaries from pickle files\n",
    "with open('/commonality_analysis/rs/pls/r2_rs.pkl', 'rb') as f:\n",
    "    r2_rs = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/r2_mh_and_rs.pkl', 'rb') as f:\n",
    "    r2_mh_and_rs = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "for col in r2_rs:\n",
    "    globals()[col] = round(r2_rs[col] * 100, 3)\n",
    "    r2_rs[col] = round(r2_rs[col] * 100, 3)  # Update the dictionary with rounded values\n",
    "\n",
    "for col in r2_mh_and_rs:\n",
    "    globals()[f'r2_mh_and_{col}'] = round(r2_mh_and_rs[col] * 100, 3)\n",
    "    r2_mh_and_rs[col] = round(r2_mh_and_rs[col] * 100, 3)\n",
    "\n",
    "print(r2_rs)\n",
    "print(r2_mh_and_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U/C\n",
    "with open('/commonality_analysis/rs/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/unique_rs.pkl', 'rb') as f:\n",
    "    unique_rs = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/common_mh_rs.pkl', 'rb') as f:\n",
    "    common_mh_rs = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "for key in unique_mh:\n",
    "    globals()[key] = round(unique_mh[key] * 100, 3)\n",
    "    unique_mh[key] = round(unique_mh[key] * 100, 3)  # Update the dictionary with rounded values\n",
    "\n",
    "for key in unique_rs:\n",
    "    globals()[key] = round(unique_rs[key] * 100, 3)\n",
    "    unique_rs[key] = round(unique_rs[key] * 100, 3) \n",
    "\n",
    "for key in common_mh_rs:\n",
    "    globals()[key] = round(common_mh_rs[key] * 100, 3)\n",
    "    common_mh_rs[key] = round(common_mh_rs[key] * 100, 3)\n",
    "\n",
    "print(unique_mh)\n",
    "print(unique_rs)\n",
    "print(common_mh_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTI Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "all_g_mh_dti_all_concat = []\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_dti_all_rf = pd.read_csv(f'/PLS/brain/stacking/g/DTI_All_target_pred_2nd_level_rf_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_dti_all_rf, on='eid').drop(columns=['eid'])\n",
    "    all_g.columns = ['g_real', 'g_pred_mh', 'g_pred_dti_all']\n",
    "    all_g_mh_dti_all_concat.append(all_g)\n",
    "    all_g_mh_dti_all = pd.concat(all_g_mh_dti_all_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_dti_all.to_csv('/commonality_analysis/dti/g_real_pred_mh_dti-all.csv', index=False)\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh_dti = model.fit(all_g_mh_dti_all['g_pred_mh'].values.reshape(-1, 1), all_g_mh_dti_all['g_real']).score(all_g_mh_dti_all['g_pred_mh'].values.reshape(-1, 1), all_g_mh_dti_all['g_real'])\n",
    "r2_dti = model.fit(all_g_mh_dti_all['g_pred_dti_all'].values.reshape(-1, 1), all_g_mh_dti_all['g_real']).score(all_g_mh_dti_all['g_pred_dti_all'].values.reshape(-1, 1), all_g_mh_dti_all['g_real'])\n",
    "r2_mh_and_dti = model.fit(pd.concat([all_g_mh_dti_all['g_pred_dti_all'], all_g_mh_dti_all['g_pred_mh']], axis=1), all_g_mh_dti_all['g_real']).score(pd.concat([all_g_mh_dti_all['g_pred_dti_all'], all_g_mh_dti_all['g_pred_mh']], axis=1), all_g_mh_dti_all['g_real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_dti_all = pd.read_csv('/commonality_analysis/dti/g_real_pred_mh_dti-all.csv')\n",
    "r2_mh_dti = commonality_analysis(all_g_mh_dti_all['g_pred_mh'], all_g_mh_dti_all['g_real'])\n",
    "r2_dti = commonality_analysis(all_g_mh_dti_all['g_pred_dti_all'], all_g_mh_dti_all['g_real'])\n",
    "r2_mh_and_dti = commonality_analysis(pd.concat([all_g_mh_dti_all['g_pred_dti_all'], all_g_mh_dti_all['g_pred_mh']], axis=1), all_g_mh_dti_all['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "u_mh_dti = r2_mh_and_dti - r2_dti\n",
    "u_dti = r2_mh_and_dti - r2_mh_dti\n",
    "\n",
    "c_mh_and_dti = r2_mh_and_dti - u_mh_dti - u_dti\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh_dti.round(3)) # f\"{r2_mh_m:.5f}\")\n",
    "print(\"R squared for DTI All\", r2_dti.round(3))\n",
    "print(\"R squared for MH and DTI All\", r2_mh_and_dti.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh_dti.round(3)) #\"{:.5f}\".format(u_mh_dti)) # \"{:.5f}\".format(u_mh_for_dti)\n",
    "print(\"Unique variance for DTI All:\", u_dti.round(3)) #f\"{u_rs:.5f}\")\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and DTI All:\", c_mh_and_dti.round(3))\n",
    "print('_______________')\n",
    "perc_mh = u_mh_dti / r2_mh_and_dti * 100 #u_mh_dti / R squared for MH and DTI All\n",
    "perc_dti = u_dti / r2_mh_and_dti * 100 #u_rs / R squared for MH and DTI All\n",
    "perc_c = c_mh_and_dti / r2_mh_and_dti * 100 #c_mh_rs / R squared for MH and DTI All\n",
    "\n",
    "print(f\"Proportion of unique variance attributed to MH is {perc_mh.round(2)}\")\n",
    "print(f\"Proportion of unique variance attributed to DTI All is {perc_dti.round(2)}\")\n",
    "print(f\"Proportion of common variance attributed to MH and DTI All is {perc_c.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTI PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Match g: original, from mental health, and from DTI\n",
    "folds = [f'{i}' for i in range(5)]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "pls_g_mh_dti_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_pls_test_dti = pd.read_csv(f'/PLS/brain/stacking/features_test_level1_stacked/dti_all/features_test_level1_g_matched_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    pls_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_pls_test_dti, on='eid').drop(columns=['eid'])\n",
    "    #pls_g.columns = ['g_real', 'g_pred_mh', 'g_pred_dti_ts']\n",
    "    pls_g_mh_dti_concat.append(pls_g)\n",
    "    pls_g_mh_dti = pd.concat(pls_g_mh_dti_concat, axis=0, ignore_index=True)\n",
    "    pls_g_mh_dti.to_csv('/commonality_analysis/dti/pls/g_real_pred_mh_dti-pls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Extract R2 with commonality_analysis function\n",
    "pls_g_mh_dti = pd.read_csv('/commonality_analysis/dti/pls/g_real_pred_mh_dti-pls.csv')\n",
    "r2_mh_from_dti_pls = LinearRegression().fit(pls_g_mh_dti['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_dti['g']).score(pls_g_mh_dti['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_dti['g'])\n",
    "\n",
    "dti_cols= pls_g_mh_dti.columns[2:].to_list()\n",
    "\n",
    "r2_dti = {}\n",
    "for col in dti_cols:\n",
    "    r2_score = commonality_analysis(pls_g_mh_dti[col], pls_g_mh_dti['g'])\n",
    "    r2_dti[col] = r2_score\n",
    "    globals()[f'r2_{col}'] = r2_score\n",
    "\n",
    "# Iterate through each column to combine with 'g_pred_mh'\n",
    "r2_mh_and_dti = {}\n",
    "for col in dti_cols:\n",
    "    combined_columns = pd.concat([pls_g_mh_dti[col], pls_g_mh_dti['g_pred_mh']], axis=1)\n",
    "    r2_combined = commonality_analysis(combined_columns, pls_g_mh_dti['g'])\n",
    "    r2_mh_and_dti[f'r2_mh_and_{col}'] = r2_combined\n",
    "    # Create a variable for each combined R^2 score\n",
    "    globals()[f'r2_mh_and_{col}'] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save R2 dictionaries\n",
    "with open('/commonality_analysis/dti/pls/r2_dti.pkl', 'wb') as f:\n",
    "    pickle.dump(r2_dti, f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/r2_mh_and_dti.pkl', 'wb') as f:\n",
    "    pickle.dump(r2_mh_and_dti, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Calculate  unique and common variances\n",
    "unique_mh = {}\n",
    "unique_dti = {}\n",
    "common_mh_dti = {}\n",
    "\n",
    "for key in r2_mh_and_dti:\n",
    "    dti_key = key.replace('r2_mh_and_', '')\n",
    "    u_mh = r2_mh_and_dti[key] - r2_dti[dti_key]\n",
    "    u_dti = r2_mh_and_dti[key] - r2_mh_from_dti_pls #[key]\n",
    "    c_mh_and_dti = r2_mh_and_dti[key] - u_mh - u_dti\n",
    "    \n",
    "    unique_mh[f'u_mh_from_{dti_key}'] = u_mh\n",
    "    unique_dti[f'u_dti_{dti_key}'] = u_dti\n",
    "    common_mh_dti[f'c_mh_and_dti_{dti_key}'] = c_mh_and_dti\n",
    "\n",
    "# Print results\n",
    "for key in r2_mh_and_dti:\n",
    "    dti_key = key.replace('r2_mh_and_', '')\n",
    "    print(f\"R squared for {dti_key}: {r2_dti[dti_key]:.3f}\")\n",
    "    print(f\"R squared for MH and {dti_key}: {r2_mh_and_dti[key]:.3f}\")\n",
    "    print(f\"Unique variance for MH ({dti_key}): {unique_mh[f'u_mh_from_{dti_key}']:.3f}\")\n",
    "    print(f\"Unique variance for dti ({dti_key}): {unique_dti[f'u_dti_{dti_key}']:.3f}\")\n",
    "    print(f\"Common variance for MH and dti ({dti_key}): {common_mh_dti[f'c_mh_and_dti_{dti_key}']:.3f}\")\n",
    "    print('_______________')\n",
    "\n",
    "# Calculate proportions\n",
    "for key in r2_mh_and_dti:\n",
    "    dti_key = key.replace('r2_mh_and_', '')\n",
    "    perc_mh = unique_mh[f'u_mh_from_{dti_key}'] / r2_mh_and_dti[key] * 100\n",
    "    perc_dti = unique_dti[f'u_dti_{dti_key}'] / r2_mh_and_dti[key] * 100\n",
    "    perc_c = common_mh_dti[f'c_mh_and_dti_{dti_key}'] / r2_mh_and_dti[key] * 100\n",
    "    \n",
    "    print(f\"Proportion of unique variance attributed to MH ({dti_key}): {perc_mh:.2f}%\")\n",
    "    print(f\"Proportion of unique variance attributed to dti ({dti_key}): {perc_dti:.2f}%\")\n",
    "    print(f\"Proportion of common variance attributed to MH and dti ({dti_key}): {perc_c:.2f}%\")\n",
    "    print('_______________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unique and common variances\n",
    "with open('/commonality_analysis/dti/pls/unique_mh.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_mh, f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/unique_dti.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_dti, f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/common_mh_dti.pkl', 'wb') as f:\n",
    "    pickle.dump(common_mh_dti, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload CA variables for DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "pls_g_mh_dti = pd.read_csv('/commonality_analysis/dti/pls/g_real_pred_mh_dti-pls.csv')\n",
    "r2_mh_from_dti_pls = LinearRegression().fit(pls_g_mh_dti['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_dti['g']).score(pls_g_mh_dti['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_dti['g'])\n",
    "\n",
    "# Load R2 dictionaries from pickle files\n",
    "with open('/commonality_analysis/dti/pls/r2_dti.pkl', 'rb') as f:\n",
    "    r2_dti = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/r2_mh_and_dti.pkl', 'rb') as f:\n",
    "    r2_mh_and_dti = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "for col in r2_dti:\n",
    "    globals()[col] = round(r2_dti[col] * 100, 3)\n",
    "    r2_dti[col] = round(r2_dti[col] * 100, 3)  # Update the dictionary with rounded values\n",
    "\n",
    "for col in r2_mh_and_dti:\n",
    "    globals()[f'r2_mh_and_{col}'] = round(r2_mh_and_dti[col] * 100, 3)\n",
    "    r2_mh_and_dti[col] = round(r2_mh_and_dti[col] * 100, 3)\n",
    "\n",
    "print(r2_dti)\n",
    "print(r2_mh_and_dti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U/C\n",
    "with open('/commonality_analysis/dti/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/unique_dti.pkl', 'rb') as f:\n",
    "    unique_dti = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/common_mh_dti.pkl', 'rb') as f:\n",
    "    common_mh_dti = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "for key in unique_mh:\n",
    "    globals()[key] = round(unique_mh[key] * 100, 3)\n",
    "    unique_mh[key] = round(unique_mh[key] * 100, 3)\n",
    "\n",
    "for key in unique_dti:\n",
    "    globals()[key] = round(unique_dti[key] * 100, 3)\n",
    "    unique_dti[key] = round(unique_dti[key] * 100, 3) \n",
    "\n",
    "for key in common_mh_dti:\n",
    "    globals()[key] = round(common_mh_dti[key] * 100, 3)\n",
    "    common_mh_dti[key] = round(common_mh_dti[key] * 100, 3)\n",
    "\n",
    "print(unique_mh)\n",
    "print(unique_dti)\n",
    "print(common_mh_dti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1/T2 Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "all_g_mh_t1_concat = []\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_t1_svr = pd.read_csv(f'/PLS/brain/stacking/g/T1_T2_whole_brain_target_pred_2nd_level_svr_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_t1_svr, on='eid').drop(columns=['eid'])\n",
    "    all_g.columns = ['g_real', 'g_pred_mh', 'g_pred_t1']\n",
    "    all_g_mh_t1_concat.append(all_g)\n",
    "    all_g_mh_t1 = pd.concat(all_g_mh_t1_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_t1.to_csv('/commonality_analysis/struct/g_real_pred_mh_t1.csv', index=False)\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh_t1 = model.fit(all_g_mh_t1['g_pred_mh'].values.reshape(-1, 1), all_g_mh_t1['g_real'].values.reshape(-1, 1)).score(all_g_mh_t1['g_pred_mh'].values.reshape(-1, 1), all_g_mh_t1['g_real'].values.reshape(-1, 1))\n",
    "r2_t1 = model.fit(all_g_mh_t1['g_pred_t1'].values.reshape(-1, 1), all_g_mh_t1['g_real'].values.reshape(-1, 1)).score(all_g_mh_t1['g_pred_t1'].values.reshape(-1, 1), all_g_mh_t1['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_and_t1 = model.fit(pd.concat([all_g_mh_t1['g_pred_t1'], all_g_mh_t1['g_pred_mh']], axis=1), all_g_mh_t1['g_real']).score(pd.concat([all_g_mh_t1['g_pred_t1'], all_g_mh_t1['g_pred_mh']], axis=1), all_g_mh_t1['g_real'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_t1 = pd.read_csv('/commonality_analysis/struct/g_real_pred_mh_t1.csv')\n",
    "r2_mh_t1 = commonality_analysis(all_g_mh_t1['g_pred_mh'], all_g_mh_t1['g_real'])\n",
    "r2_t1 = commonality_analysis(all_g_mh_t1['g_pred_t1'], all_g_mh_t1['g_real'])\n",
    "r2_mh_and_t1 = commonality_analysis(pd.concat([all_g_mh_t1['g_pred_t1'], all_g_mh_t1['g_pred_mh']], axis=1), all_g_mh_t1['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "u_mh_t1 = r2_mh_and_t1 - r2_t1\n",
    "u_t1 = r2_mh_and_t1 - r2_mh_t1\n",
    "\n",
    "c_mh_and_t1 = r2_mh_and_t1 - u_mh_t1 - u_t1\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh_t1.round(3)) # f\"{r2_mh_m:.5f}\")\n",
    "print(\"R squared for T1\", r2_t1.round(3))\n",
    "print(\"R squared for MH and T1\", r2_mh_and_t1.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh_t1.round(3))\n",
    "print(\"Unique variance for T1:\", u_t1.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and T1:\", c_mh_and_t1.round(3))\n",
    "print('_______________')\n",
    "perc_mh = u_mh_t1 / r2_mh_and_t1 * 100\n",
    "perc_t1 = u_t1 / r2_mh_and_t1 * 100\n",
    "perc_c = c_mh_and_t1 / r2_mh_and_t1 * 100\n",
    "\n",
    "print(f\"Proportion of unique variance attributed to MH is {perc_mh.round(2)}\")\n",
    "print(f\"Proportion of unique variance attributed to T1 is {perc_t1.round(2)}\")\n",
    "print(f\"Proportion of common variance attributed to MH and T1 is {perc_c.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1/T2 PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Match g: original, from mental health, and from sMRI\n",
    "folds = [f'{i}' for i in range(5)]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "pls_g_mh_t1_t2_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_pls_test_t1_t2 = pd.read_csv(f'/PLS/brain/stacking/features_test_level1_stacked/t1_t2_struct/features_test_level1_g_matched_fold_{fold}.csv')\n",
    "    g_pred_mh = g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    pls_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_pls_test_t1_t2, on='eid').drop(columns=['eid'])\n",
    "    pls_g_mh_t1_t2_concat.append(pls_g)\n",
    "    pls_g_mh_t1_t2 = pd.concat(pls_g_mh_t1_t2_concat, axis=0, ignore_index=True)\n",
    "    pls_g_mh_t1_t2.to_csv('/commonality_analysis/struct/pls/g_real_pred_mh_t1_t2-pls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Extract R2 with commonality_analysis function\n",
    "pls_g_mh_t1_t2 = pd.read_csv('/commonality_analysis/struct/pls/g_real_pred_mh_t1_t2-pls.csv')\n",
    "r2_mh_from_t1_t2_pls = LinearRegression().fit(pls_g_mh_t1_t2['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_t1_t2['g']).score(pls_g_mh_t1_t2['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_t1_t2['g'])\n",
    "\n",
    "t1_t2_cols= pls_g_mh_t1_t2.columns[2:].to_list()\n",
    "\n",
    "r2_t1_t2 = {}\n",
    "for col in t1_t2_cols:\n",
    "    r2_score = commonality_analysis(pls_g_mh_t1_t2[col], pls_g_mh_t1_t2['g'])\n",
    "    r2_t1_t2[col] = r2_score\n",
    "    globals()[f'r2_{col}'] = r2_score\n",
    "\n",
    "# Iterate through each column to combine with 'g_pred_mh'\n",
    "r2_mh_and_t1_t2 = {}\n",
    "for col in t1_t2_cols:\n",
    "    combined_columns = pd.concat([pls_g_mh_t1_t2[col], pls_g_mh_t1_t2['g_pred_mh']], axis=1)\n",
    "    r2_combined = commonality_analysis(combined_columns, pls_g_mh_t1_t2['g'])\n",
    "    r2_mh_and_t1_t2[f'r2_mh_and_{col}'] = r2_combined\n",
    "    # Create a variable for each combined R^2 score\n",
    "    globals()[f'r2_mh_and_{col}'] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save R2 dictionaries\n",
    "with open('/commonality_analysis/struct/pls/r2_t1_t2.pkl', 'wb') as f:\n",
    "    pickle.dump(r2_t1_t2, f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/r2_mh_and_t1_t2.pkl', 'wb') as f:\n",
    "    pickle.dump(r2_mh_and_t1_t2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Calculate  unique and common variances\n",
    "unique_mh = {}\n",
    "unique_t1_t2 = {}\n",
    "common_mh_t1_t2 = {}\n",
    "\n",
    "for key in r2_mh_and_t1_t2:\n",
    "    t1_t2_key = key.replace('r2_mh_and_', '')\n",
    "    u_mh = r2_mh_and_t1_t2[key] - r2_t1_t2[t1_t2_key]\n",
    "    u_t1_t2 = r2_mh_and_t1_t2[key] - r2_mh_from_t1_t2_pls #[key]\n",
    "    c_mh_and_t1_t2 = r2_mh_and_t1_t2[key] - u_mh - u_t1_t2\n",
    "    \n",
    "    unique_mh[f'u_mh_from_{t1_t2_key}'] = u_mh\n",
    "    unique_t1_t2[f'u_t1_t2_{t1_t2_key}'] = u_t1_t2\n",
    "    common_mh_t1_t2[f'c_mh_and_t1_t2_{t1_t2_key}'] = c_mh_and_t1_t2\n",
    "\n",
    "# Print results\n",
    "for key in r2_mh_and_t1_t2:\n",
    "    t1_t2_key = key.replace('r2_mh_and_', '')\n",
    "    print(f\"R squared for {t1_t2_key}: {r2_t1_t2[t1_t2_key]:.3f}\")\n",
    "    print(f\"R squared for MH and {t1_t2_key}: {r2_mh_and_t1_t2[key]:.3f}\")\n",
    "    print(f\"Unique variance for MH ({t1_t2_key}): {unique_mh[f'u_mh_from_{t1_t2_key}']:.3f}\")\n",
    "    print(f\"Unique variance for t1_t2 ({t1_t2_key}): {unique_t1_t2[f'u_t1_t2_{t1_t2_key}']:.3f}\")\n",
    "    print(f\"Common variance for MH and t1_t2 ({t1_t2_key}): {common_mh_t1_t2[f'c_mh_and_t1_t2_{t1_t2_key}']:.3f}\")\n",
    "    print('_______________')\n",
    "\n",
    "# Calculate proportions\n",
    "for key in r2_mh_and_t1_t2:\n",
    "    t1_t2_key = key.replace('r2_mh_and_', '')\n",
    "    perc_mh = unique_mh[f'u_mh_from_{t1_t2_key}'] / r2_mh_and_t1_t2[key] * 100\n",
    "    perc_t1_t2 = unique_t1_t2[f'u_t1_t2_{t1_t2_key}'] / r2_mh_and_t1_t2[key] * 100\n",
    "    perc_c = common_mh_t1_t2[f'c_mh_and_t1_t2_{t1_t2_key}'] / r2_mh_and_t1_t2[key] * 100\n",
    "    \n",
    "    print(f\"Proportion of unique variance attributed to MH ({t1_t2_key}): {perc_mh:.2f}%\")\n",
    "    print(f\"Proportion of unique variance attributed to t1_t2 ({t1_t2_key}): {perc_t1_t2:.2f}%\")\n",
    "    print(f\"Proportion of common variance attributed to MH and t1_t2 ({t1_t2_key}): {perc_c:.2f}%\")\n",
    "    print('_______________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unique and common variances\n",
    "with open('/commonality_analysis/struct/pls/unique_mh.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_mh, f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/unique_t1_t2.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_t1_t2, f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/common_mh_t1_t2.pkl', 'wb') as f:\n",
    "    pickle.dump(common_mh_t1_t2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload CA variables for T1/T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "pls_g_mh_t1_t2 = pd.read_csv('/commonality_analysis/struct/pls/g_real_pred_mh_t1_t2-pls.csv')\n",
    "r2_mh_from_t1_t2_pls = LinearRegression().fit(pls_g_mh_t1_t2['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_t1_t2['g']).score(pls_g_mh_t1_t2['g_pred_mh'].values.reshape(-1, 1), pls_g_mh_t1_t2['g'])\n",
    "\n",
    "# Load R2 dictionaries from pickle files\n",
    "with open('/commonality_analysis/struct/pls/r2_t1_t2.pkl', 'rb') as f:\n",
    "    r2_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/r2_mh_and_t1_t2.pkl', 'rb') as f:\n",
    "    r2_mh_and_t1_t2 = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "for col in r2_t1_t2:\n",
    "    globals()[col] = round(r2_t1_t2[col] * 100, 3)\n",
    "    r2_t1_t2[col] = round(r2_t1_t2[col] * 100, 3)  # Update the dictionary with rounded values\n",
    "\n",
    "for col in r2_mh_and_t1_t2:\n",
    "    globals()[f'r2_mh_and_{col}'] = round(r2_mh_and_t1_t2[col] * 100, 3)\n",
    "    r2_mh_and_t1_t2[col] = round(r2_mh_and_t1_t2[col] * 100, 3)\n",
    "\n",
    "print(r2_t1_t2)\n",
    "print(r2_mh_and_t1_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U/C\n",
    "with open('/commonality_analysis/struct/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/unique_t1_t2.pkl', 'rb') as f:\n",
    "    unique_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/common_mh_t1_t2.pkl', 'rb') as f:\n",
    "    common_mh_t1_t2 = pickle.load(f)\n",
    "\n",
    "# Extract variables\n",
    "for key in unique_mh:\n",
    "    globals()[key] = round(unique_mh[key] * 100, 3)\n",
    "    unique_mh[key] = round(unique_mh[key] * 100, 3)  # Update the dictionary with rounded values\n",
    "\n",
    "for key in unique_t1_t2:\n",
    "    globals()[key] = round(unique_t1_t2[key] * 100, 3)\n",
    "    unique_t1_t2[key] = round(unique_t1_t2[key] * 100, 3) \n",
    "\n",
    "for key in common_mh_t1_t2:\n",
    "    globals()[key] = round(common_mh_t1_t2[key] * 100, 3)\n",
    "    common_mh_t1_t2[key] = round(common_mh_t1_t2[key] * 100, 3)\n",
    "\n",
    "print(unique_mh)\n",
    "print(unique_t1_t2)\n",
    "print(common_mh_t1_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Modalities Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "folds = ['0','1','2','3','4']\n",
    "\n",
    "all_g_mh_mri_all_concat = []\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_mri_all = pd.read_csv(f'/PLS/brain/stacking/g/All_modalities_target_pred_2nd_level_xgb_test_fold_{fold}.csv')\n",
    "    g_pred_mh = g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_mri_all, on='eid').drop(columns=['eid'])\n",
    "    all_g.columns = ['g_real', 'g_pred_mh', 'g_pred_mri_all']\n",
    "    all_g_mh_mri_all_concat.append(all_g)\n",
    "    all_g_mh_mri_all = pd.concat(all_g_mh_mri_all_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_mri_all.to_csv('/commonality_analysis/all/g_real_pred_mh_mri-all.csv', index=False)\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh_mri = model.fit(all_g_mh_mri_all['g_pred_mh'].values.reshape(-1, 1), all_g_mh_mri_all['g_real'].values.reshape(-1, 1)).score(all_g_mh_mri_all['g_pred_mh'].values.reshape(-1, 1), all_g_mh_mri_all['g_real'].values.reshape(-1, 1))\n",
    "r2_mri = model.fit(all_g_mh_mri_all['g_pred_mri_all'].values.reshape(-1, 1), all_g_mh_mri_all['g_real'].values.reshape(-1, 1)).score(all_g_mh_mri_all['g_pred_mri_all'].values.reshape(-1, 1), all_g_mh_mri_all['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_and_mri = model.fit(pd.concat([all_g_mh_mri_all['g_pred_mri_all'], all_g_mh_mri_all['g_pred_mh']], axis=1), all_g_mh_mri_all['g_real']).score(pd.concat([all_g_mh_mri_all['g_pred_mri_all'], all_g_mh_mri_all['g_pred_mh']], axis=1), all_g_mh_mri_all['g_real'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_mri_all = pd.read_csv('/commonality_analysis/all/g_real_pred_mh_mri-all.csv')\n",
    "r2_mh_mri = commonality_analysis(all_g_mh_mri_all['g_pred_mh'], all_g_mh_mri_all['g_real'])\n",
    "r2_mri = commonality_analysis(all_g_mh_mri_all['g_pred_mri_all'], all_g_mh_mri_all['g_real'])\n",
    "r2_mh_and_mri = commonality_analysis(pd.concat([all_g_mh_mri_all['g_pred_mri_all'], all_g_mh_mri_all['g_pred_mh']], axis=1), all_g_mh_mri_all['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "u_mh_mri = r2_mh_and_mri - r2_mri\n",
    "u_mri = r2_mh_and_mri - r2_mh_mri\n",
    "\n",
    "c_mh_and_mri = r2_mh_and_mri - u_mh_mri - u_mri\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh_mri.round(3)) # f\"{r2_mh_mri_m:.5f}\")\n",
    "print(\"R squared for MRI\", r2_mri.round(3))\n",
    "print(\"R squared for MH and MRI\", r2_mh_and_mri.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh_mri.round(3)) #\"{:.5f}\".format(u_mh_mri)) # \"{:.5f}\".format(u_mh_mri)\n",
    "print(\"Unique variance for MRI:\", u_mri.round(3)) #f\"{u_rs:.5f}\")\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and MRI:\", c_mh_and_mri.round(3))\n",
    "print('_______________')\n",
    "perc_mh = u_mh_mri / r2_mh_and_mri * 100 #u_mh_mri / R squared for MH and T1 plus T2\n",
    "perc_mri = u_mri / r2_mh_and_mri * 100 #u_rs / R squared for MH and T1 plus T2\n",
    "perc_c = c_mh_and_mri / r2_mh_and_mri * 100 #c_mh_rs / R squared for MH and T1 plus T2\n",
    "\n",
    "print(f\"Proportion of unique variance attributed to MH is {perc_mh.round(2)}\")\n",
    "print(f\"Proportion of unique variance attributed to MRI is {perc_mri.round(2)}\")\n",
    "print(f\"Proportion of common variance attributed to MH and MRI is {perc_c.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all PLS CA results in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload variables\n",
    "# DTI\n",
    "with open('/commonality_analysis/dti/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_dti = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/unique_dti.pkl', 'rb') as f:\n",
    "    unique_dti = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/common_mh_dti.pkl', 'rb') as f:\n",
    "    common_mh_dti = pickle.load(f)\n",
    "\n",
    "# RS\n",
    "with open('/commonality_analysis/rs/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_rs = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/unique_rs.pkl', 'rb') as f:\n",
    "    unique_rs = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/common_mh_rs.pkl', 'rb') as f:\n",
    "    common_mh_rs = pickle.load(f)\n",
    "\n",
    "# T1/T2\n",
    "with open('/commonality_analysis/struct/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/unique_t1_t2.pkl', 'rb') as f:\n",
    "    unique_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/common_mh_t1_t2.pkl', 'rb') as f:\n",
    "    common_mh_t1_t2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all into one data frame: long format\n",
    "commonality_pls = {}\n",
    "\n",
    "# DTI\n",
    "for key in unique_mh_dti:\n",
    "    commonality_pls[f'{key}'] = round(unique_mh_dti[key] * 100, 3)\n",
    "for key in unique_dti:\n",
    "    commonality_pls[f'{key}'] = round(unique_dti[key] * 100, 3)\n",
    "for key in common_mh_dti:\n",
    "    commonality_pls[f'{key}'] = round(common_mh_dti[key] * 100, 3)\n",
    "\n",
    "# RS\n",
    "for key in unique_mh_rs:\n",
    "    commonality_pls[f'{key}'] = round(unique_mh_rs[key] * 100, 3)\n",
    "for key in unique_rs:\n",
    "    commonality_pls[f'{key}'] = round(unique_rs[key] * 100, 3)\n",
    "for key in common_mh_rs:\n",
    "    commonality_pls[f'{key}'] = round(common_mh_rs[key] * 100, 3)\n",
    "\n",
    "# T1/T2\n",
    "for key in unique_mh_t1_t2:\n",
    "    commonality_pls[f'{key}'] = round(unique_mh_t1_t2[key] * 100, 3)\n",
    "for key in unique_t1_t2:\n",
    "    commonality_pls[f'{key}'] = round(unique_t1_t2[key] * 100, 3)\n",
    "for key in common_mh_t1_t2:\n",
    "    commonality_pls[f'{key}'] = round(common_mh_t1_t2[key] * 100, 3)\n",
    "\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "commonality_pls_df = pd.DataFrame.from_dict(commonality_pls, orient='index', columns=['Value'])\n",
    "commonality_pls_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all into one data frame: wide format\n",
    "commonality_pls_dict = {}\n",
    "\n",
    "# DTI\n",
    "commonality_metrics_dti = {}\n",
    "for key in unique_mh_dti:\n",
    "    modality = key.replace('u_mh_from_', '')\n",
    "    commonality_pls_dict[modality] = {\n",
    "        'Unique: Mental Health': round(unique_mh_dti[key] * 100, 3),\n",
    "        'Unique: MRI': round(unique_dti[key.replace('u_mh_from_', 'u_dti_')] * 100, 3),\n",
    "        'Common': round(common_mh_dti[key.replace('u_mh_from_', 'c_mh_and_dti_')] * 100, 3)\n",
    "    }\n",
    "    commonality_metrics_dti[modality] = {\n",
    "        'Unique: Mental Health': round(unique_mh_dti[key] * 100, 3),\n",
    "        'Unique: MRI': round(unique_dti[key.replace('u_mh_from_', 'u_dti_')] * 100, 3),\n",
    "        'Common': round(common_mh_dti[key.replace('u_mh_from_', 'c_mh_and_dti_')] * 100, 3)\n",
    "    }\n",
    "\n",
    "# T1/T2\n",
    "commonality_metrics_struct = {}\n",
    "for key in unique_mh_t1_t2:\n",
    "    modality = key.replace('u_mh_from_', '')\n",
    "    commonality_pls_dict[modality] = {\n",
    "        'Unique: Mental Health': round(unique_mh_t1_t2[key] * 100, 3),\n",
    "        'Unique: MRI': round(unique_t1_t2[key.replace('u_mh_from_', 'u_t1_t2_')] * 100, 3),\n",
    "        'Common': round(common_mh_t1_t2[key.replace('u_mh_from_', 'c_mh_and_t1_t2_')] * 100, 3)\n",
    "    }\n",
    "    commonality_metrics_struct[modality] = {\n",
    "        'Unique: Mental Health': round(unique_mh_t1_t2[key] * 100, 3),\n",
    "        'Unique: MRI': round(unique_t1_t2[key.replace('u_mh_from_', 'u_t1_t2_')] * 100, 3),\n",
    "        'Common': round(common_mh_t1_t2[key.replace('u_mh_from_', 'c_mh_and_t1_t2_')] * 100, 3)\n",
    "    }\n",
    "\n",
    "# RS\n",
    "commonality_metrics_rs = {}\n",
    "for key in unique_mh_rs:\n",
    "    modality = key.replace('u_mh_from_', '')\n",
    "    commonality_pls_dict[modality] = {\n",
    "        'Unique: Mental Health': round(unique_mh_rs[key] * 100, 3),\n",
    "        'Unique: MRI': round(unique_rs[key.replace('u_mh_from_', 'u_rs_')] * 100, 3),\n",
    "        'Common': round(common_mh_rs[key.replace('u_mh_from_', 'c_mh_and_rs_')] * 100, 3)\n",
    "    }\n",
    "    commonality_metrics_rs[modality] = {\n",
    "        'Unique: Mental Health': round(unique_mh_rs[key] * 100, 3),\n",
    "        'Unique: MRI': round(unique_rs[key.replace('u_mh_from_', 'u_rs_')] * 100, 3),\n",
    "        'Common': round(common_mh_rs[key.replace('u_mh_from_', 'c_mh_and_rs_')] * 100, 3)\n",
    "    }\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "commonality_metrics_df = pd.DataFrame.from_dict(commonality_pls_dict, orient='index')\n",
    "commonality_metrics_dti_df = pd.DataFrame.from_dict(commonality_metrics_dti, orient='index')\n",
    "commonality_metrics_dti_df.to_csv('/commonality_analysis/commonality_metrics_dti_df.csv')\n",
    "\n",
    "commonality_metrics_struct_df = pd.DataFrame.from_dict(commonality_metrics_struct, orient='index')\n",
    "commonality_metrics_struct_df.to_csv('/commonality_analysis/commonality_metrics_struct_df.csv')\n",
    "\n",
    "commonality_metrics_rs_df = pd.DataFrame.from_dict(commonality_metrics_rs, orient='index')\n",
    "commonality_metrics_rs_df.to_csv('/commonality_analysis/commonality_metrics_rs_df.csv')\n",
    "\n",
    "\n",
    "with open('/commonality_analysis/commonality_metrics_dti.pkl', 'wb') as f: #to open, change wb to rb, and dump to load\n",
    "    pickle.dump(commonality_metrics_dti, f)\n",
    "\n",
    "with open('/commonality_analysis/commonality_metrics_struct_df.pkl', 'wb') as f: #to open, change wb to rb, and dump to load\n",
    "    pickle.dump(commonality_metrics_struct_df, f)\n",
    "\n",
    "with open('/commonality_analysis/commonality_metrics_rs_df.pkl', 'wb') as f: #to open, change wb to rb, and dump to load\n",
    "    pickle.dump(commonality_metrics_rs_df, f)\n",
    "\n",
    "    \n",
    "commonality_metrics_df.to_csv('/commonality_analysis/commonality_metrics_df.csv')\n",
    "commonality_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all Stack CA results in one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract commonality metrics and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_mh_rs_na = u_mh_rs.astype(float).round(4)*100\n",
    "u_rs_na = u_rs.astype(float).round(4)*100\n",
    "c_mh_and_rs_na = c_mh_and_rs.astype(float).round(4)*100\n",
    "u_mh_dti_na = u_mh_dti.astype(float).round(4)*100\n",
    "u_dti_na = u_dti.astype(float).round(4)*100\n",
    "c_mh_and_dti_na = c_mh_and_dti.astype(float).round(4)*100\n",
    "u_mh_t1_na = u_mh_t1.astype(float).round(4)*100\n",
    "u_t1_na = u_t1.astype(float).round(4)*100\n",
    "c_mh_and_t1_na = (c_mh_and_t1.astype(float).round(4)*100).round(1)\n",
    "u_mh_mri_na = u_mh_mri.astype(float).round(4)*100\n",
    "u_mri_na = u_mri.astype(float).round(4)*100\n",
    "c_mh_and_mri_na = c_mh_and_mri.astype(float).round(4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonality_metrics_no_demo_dict = {\n",
    "'u_mh_rs_na': u_mh_rs_na,\n",
    "'u_rs_na': u_rs_na,\n",
    "'c_mh_and_rs_na': c_mh_and_rs_na,\n",
    "'u_mh_dti_na': u_mh_dti_na,\n",
    "'u_dti_na': u_dti_na,\n",
    "'c_mh_and_dti_na': c_mh_and_dti_na,\n",
    "'u_mh_t1_na': u_mh_t1_na,\n",
    "'u_t1_na': u_t1_na,\n",
    "'c_mh_and_t1_na': c_mh_and_t1_na,\n",
    "'u_mh_mri_na': u_mh_mri_na,\n",
    "'u_mri_na': u_mri_na,\n",
    "'c_mh_and_mri_na': c_mh_and_mri_na\n",
    "}\n",
    "\n",
    "commonality_metrics_no_demo = pd.DataFrame(commonality_metrics_no_demo_dict, index=[0])\n",
    "commonality_metrics_no_demo.to_csv('/commonality_analysis/commonality_metrics_no_demo.csv', index=False)\n",
    "commonality_metrics_no_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values from the DataFrame\n",
    "u_mh_rs_na = commonality_metrics_no_demo['u_mh_rs_na'].iloc[0]\n",
    "u_rs_na = commonality_metrics_no_demo['u_rs_na'].iloc[0]\n",
    "c_mh_and_rs_na = commonality_metrics_no_demo['c_mh_and_rs_na'].iloc[0]\n",
    "u_mh_dti_na = commonality_metrics_no_demo['u_mh_dti_na'].iloc[0]\n",
    "u_dti_na = commonality_metrics_no_demo['u_dti_na'].iloc[0]\n",
    "c_mh_and_dti_na = commonality_metrics_no_demo['c_mh_and_dti_na'].iloc[0]\n",
    "u_mh_t1_na = commonality_metrics_no_demo['u_mh_t1_na'].iloc[0]\n",
    "u_t1_na = commonality_metrics_no_demo['u_t1_na'].iloc[0]\n",
    "c_mh_and_t1_na = commonality_metrics_no_demo['c_mh_and_t1_na'].iloc[0]\n",
    "u_mh_mri_na = commonality_metrics_no_demo['u_mh_mri_na'].iloc[0]\n",
    "u_mri_na = commonality_metrics_no_demo['u_mri_na'].iloc[0]\n",
    "c_mh_and_mri_na = commonality_metrics_no_demo['c_mh_and_mri_na'].iloc[0]\n",
    "\n",
    "# Print the extracted values\n",
    "print(f\"u_mh_rs_na: {u_mh_rs_na.round(3)}\")\n",
    "print(f\"u_rs_na: {u_rs_na.round(3)}\")\n",
    "print(f\"c_mh_and_rs_na: {c_mh_and_rs_na.round(3)}\")\n",
    "print(f\"u_mh_dti_na: {u_mh_dti_na.round(3)}\")\n",
    "print(f\"u_dti_na: {u_dti_na.round(3)}\")\n",
    "print(f\"c_mh_and_dti_na: {c_mh_and_dti_na.round(3)}\")\n",
    "print(f\"u_mh_t1_na: {u_mh_t1_na.round(3)}\")\n",
    "print(f\"u_t1_na: {u_t1_na.astype(float).round(3)}\")\n",
    "print(f\"c_mh_and_t1_na: {c_mh_and_t1_na.round(3)}\")\n",
    "print(f\"u_mh_mri_na: {u_mh_mri_na.round(3)}\")\n",
    "print(f\"u_mri_na: {u_mri_na.round(3)}\")\n",
    "print(f\"c_mh_and_mri_na: {c_mh_and_mri_na.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'dwMRI': {\n",
    "        'Unique: Mental Health': (commonality_metrics_no_demo['u_mh_dti_na'].iloc[0]),\n",
    "        'Unique: MRI': (commonality_metrics_no_demo['u_dti_na'].iloc[0]),\n",
    "        'Common': (commonality_metrics_no_demo['c_mh_and_dti_na'].iloc[0])\n",
    "    },\n",
    "    'rsMRI': {\n",
    "        'Unique: Mental Health': (commonality_metrics_no_demo['u_mh_rs_na'].iloc[0]),\n",
    "        'Unique: MRI': (commonality_metrics_no_demo['u_rs_na'].iloc[0]),\n",
    "        'Common': (commonality_metrics_no_demo['c_mh_and_rs_na'].iloc[0])\n",
    "    },\n",
    "    'T1w/T2w MRI': {\n",
    "        'Unique: Mental Health': (commonality_metrics_no_demo['u_mh_t1_na'].iloc[0]),\n",
    "        'Unique: MRI': (commonality_metrics_no_demo['u_t1_na'].iloc[0]),\n",
    "        'Common': (commonality_metrics_no_demo['c_mh_and_t1_na'].iloc[0])\n",
    "    },\n",
    "    'Stacked MRI': {\n",
    "        'Unique: Mental Health': (commonality_metrics_no_demo['u_mh_mri_na'].iloc[0]),\n",
    "        'Unique: MRI': (commonality_metrics_no_demo['u_mri_na'].iloc[0]),\n",
    "        'Common': (commonality_metrics_no_demo['c_mh_and_mri_na'].iloc[0])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data).T\n",
    "df.to_csv('/commonality_analysis/commonality_metrics_no_demo_supplem_table.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculate % of variance explained by MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonality_no_demo_df = pd.read_csv('/commonality_analysis/commonality_metrics_no_demo_supplem_table.csv').rename(columns = {'Unnamed: 0': 'Modality'})\n",
    "commonality_no_demo_df['Var Exp MRI'] = commonality_no_demo_df['Common'].round(2) / (commonality_no_demo_df['Common'].round(2) + commonality_no_demo_df['Unique: Mental Health'].round(2))\n",
    "commonality_no_demo_df['Var Exp MRI'] = (commonality_no_demo_df['Var Exp MRI'] * 100).round(2)\n",
    "commonality_no_demo_df['Total Var'] = commonality_no_demo_df.iloc[:, 1:-1].sum(axis=1)\n",
    "commonality_no_demo_df = commonality_no_demo_df.sort_values(by='Total Var', ascending = False)\n",
    "commonality_no_demo_df.to_csv('/commonality_analysis/var_explained_dti_full.csv')\n",
    "commonality_no_demo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked barplots for PLSR CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename phenotypes\n",
    "base_rename_dict_dti = {'fa_tbss':'FA TBSS',\n",
    "'fa_prob':'FA Prob.',\n",
    "'md_tbss':'MD TBSS',\n",
    "'md_prob':'MD Prob.',\n",
    "'l1_tbss':'L1 TBSS',\n",
    "'l1_prob':'L1 Prob.',\n",
    "'l2_tbss':'L2 TBSS',\n",
    "'l2_prob':'L2 Prob.',\n",
    "'l3_tbss':'L3 TBSS',\n",
    "'l3_prob':'L3 Prob.',\n",
    "'mo_tbss':'MO TBSS',\n",
    "'mo_prob':'MO Prob.',\n",
    "'od_tbss':'OD TBSS',\n",
    "'od_prob':'OD Prob.',\n",
    "'icvf_tbss':'ICVF TBSS',\n",
    "'icvf_prob':'ICVF Prob.',\n",
    "'isovf_tbss':'ISOVF TBSS',\n",
    "'isovf_prob':'ISOVF Prob.',\n",
    "\n",
    "'31020_connectome_fa':'aparc.a2009s-I FA',\n",
    "'31020_connectome_mean_length':'aparc.a2009s-I Mean Length',\n",
    "'31020_connectome_sift2':'aparc.a2009s-I SIFT2',\n",
    "'31020_connectome_streamline_count':'aparc.a2009s-I Streamline Count',\n",
    "\n",
    "'31021_connectome_fa':'aparc-I FA',\n",
    "'31021_connectome_mean_length':'aparc-I Mean Length',\n",
    "'31021_connectome_sift2':'aparc-I SIFT2',\n",
    "'31021_connectome_streamline_count':'aparc-I Streamline Count',\n",
    "\n",
    "'31022_connectome_fa':'Glasser-I FA',\n",
    "'31022_connectome_mean_length':'Glasser-I Mean Length',\n",
    "'31022_connectome_sift2':'Glasser-I SIFT2',\n",
    "'31022_connectome_streamline_count':'Glasser-I Streamline Count',\n",
    "\n",
    "'31023_connectome_fa':'Glasser-IV FA',\n",
    "'31023_connectome_mean_length':'Glasser-IV Mean Length',\n",
    "'31023_connectome_sift2':'Glasser-IV SIFT2',\n",
    "'31023_connectome_streamline_count':'Glasser-IV Streamline Count',\n",
    "\n",
    "'31024_connectome_fa':'Schaefer200-I FA',\n",
    "'31024_connectome_mean_length':'Schaefer200-I Mean Length',\n",
    "'31024_connectome_sift2':'Schaefer200-I SIFT2',\n",
    "'31024_connectome_streamline_count':'Schaefer200-I Streamline Count',\n",
    "\n",
    "'31025_connectome_fa':'Schaefer500-IV FA',\n",
    "'31025_connectome_mean_length':'Schaefer500-IV Mean Length',\n",
    "'31025_connectome_sift2':'Schaefer500-IV SIFT2',\n",
    "'31025_connectome_streamline_count_10M':'Schaefer500-IV Streamline Count'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables\n",
    "with open('/commonality_analysis/dti/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_dti = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/unique_dti.pkl', 'rb') as f:\n",
    "    unique_dti = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/dti/pls/common_mh_dti.pkl', 'rb') as f:\n",
    "    common_mh_dti = pickle.load(f)\n",
    "# Set variables\n",
    "for key in unique_mh_dti: # unique for MH\n",
    "    globals()[key] = round(unique_mh_dti[key] * 100, 3)\n",
    "for key in unique_dti: # unique for rs modalities\n",
    "    globals()[key] = round(unique_dti[key] * 100, 3)\n",
    "for key in common_mh_dti: # common for MH and rs modalities\n",
    "    globals()[key] = round(common_mh_dti[key] * 100, 3)\n",
    "\n",
    "# Define x\n",
    "modalities = list(unique_mh_dti.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in common_mh_dti.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in unique_mh_dti.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_dti.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1] \n",
    "\n",
    "x_sorted = [base_rename_dict_dti.get(item, item) for item in np.array(x)[sorted_indices]] \n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.barh(x_sorted, y1_sorted, linewidth=1, color= '#f56f5c', alpha=0.6) \n",
    "plt.barh(x_sorted, y2_sorted, left=y1_sorted, color='#B24745FF', linewidth=1, alpha=0.3)\n",
    "plt.barh(x_sorted, y3_sorted, left=y1_sorted+y2_sorted, color= '#00A1D599', linewidth=1, alpha=0.3)\n",
    "\n",
    "\n",
    "for i, (value_c, value_u_mh, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    plt.text(value_c / 2.2, i, f'{value_c:.1f}%', ha='left', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh / 2, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh + value_u_mri / 2, i, f'{value_u_mri:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "\n",
    "plt.xlabel(\"% Variance Explained\", fontsize=25, labelpad=20)\n",
    "#plt.legend([\"Common Variance: Mental Health + dwMRI\", \"Unique variance: Mental Health\", \"Unique variance: dwMRI\"], fontsize = 10, ncol=3, loc='lower center', bbox_to_anchor=(0.5, -0.2))\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.tick_params(axis='y', length=0)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1.5))\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"/CA-dwMRI.png\",\n",
    "            bbox_inches=\"tight\", \n",
    "            pad_inches=1, \n",
    "            transparent=False, \n",
    "            facecolor=\"w\", \n",
    "            edgecolor='w', \n",
    "            orientation='landscape',\n",
    "            format='png')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename phenotypes\n",
    "base_rename_dict_rs = {\"aparc_s1_full_correlation\":'aparc-I Func. Connectivity',\n",
    "\"aparc_2009_s1_full_correlation\":'aparc.a2009s-I Func. Connectivity',\n",
    "\"glasser_s1_full_correlation\":'Glasser-I Func. Connectivity',\n",
    "\"glasser_s4_full_correlation\":'Glasser-IV Func. Connectivity',\n",
    "\"Schaefer7n200p_s1_full_correlation\":'Schaefer200-I Func. Connectivity',\n",
    "\"Schaefer7n500p_s4_full_correlation\":'Schaefer500-IV Func. Connectivity',\n",
    "\"amplitudes_21\":'21 IC Amplitudes',\n",
    "\"amplitudes_55\":'55 IC Amplitudes',\n",
    "\"tangent_matrices_21\":'21 IC Func. Connectivity',\n",
    "\"tangent_matrices_55\":'55 IC Func. Connectivity'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot\n",
    "# Load variables\n",
    "with open('/commonality_analysis/rs/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_rs = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/unique_rs.pkl', 'rb') as f:\n",
    "    unique_rs = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/rs/pls/common_mh_rs.pkl', 'rb') as f:\n",
    "    common_mh_rs = pickle.load(f)\n",
    "# Set variables\n",
    "for key in unique_mh_rs: # unique for MH\n",
    "    globals()[key] = round(unique_mh_rs[key] * 100, 3)\n",
    "for key in unique_rs: # unique for rs modalities\n",
    "    globals()[key] = round(unique_rs[key] * 100, 3)\n",
    "for key in common_mh_rs: # common for MH and rs modalities\n",
    "    globals()[key] = round(common_mh_rs[key] * 100, 3)\n",
    "\n",
    "\n",
    "# Define x\n",
    "modalities = list(unique_mh_rs.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in common_mh_rs.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in unique_mh_rs.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_rs.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1]\n",
    "\n",
    "x_sorted = [base_rename_dict_rs.get(item, item) for item in np.array(x)[sorted_indices]] #checks if item exists in base_rename_dict, if item exists, it returns the corresponding value from the dictionary, if doesnt, it returns item itself\n",
    "\n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.barh(x_sorted, y1_sorted, linewidth=1, color= '#f56f5c', alpha=0.6) #fill=False, edgecolor='darkgreen',\n",
    "plt.barh(x_sorted, y2_sorted, left=y1_sorted, color='#B24745FF', linewidth=1, alpha=0.3)#fill=False,edgecolor='navy',\n",
    "plt.barh(x_sorted, y3_sorted, left=y1_sorted+y2_sorted, color= '#79AF9799', linewidth=1, alpha=0.3)#fill=False,edgecolor='crimson',\n",
    "\n",
    "\n",
    "for i, (value_c, value_u_mh, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    plt.text(value_c / 2, i, f'{value_c:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh / 2, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh + value_u_mri / 2, i, f'{value_u_mri:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "\n",
    "plt.xlabel(\"% Variance Explained\", fontsize=25, labelpad=20)\n",
    "#plt.legend([\"Common Variance: Mental Health + rsMRI\", \"Unique variance: Mental Health\", \"Unique variance: rsMRI\"], fontsize = 10, ncol=3, loc='lower center', bbox_to_anchor=(0.5, -0.32)) #, , bbox_to_anchor=(1.3, 0.98) loc='upper right'\n",
    "#plt.title(\"Variance Attributed to Mental Health and MRI Modalities\", fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.tick_params(axis='y', length=0)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1.5))\n",
    "\n",
    "plt.savefig(\"/CA-rsMRI.png\",\n",
    "            bbox_inches=\"tight\", \n",
    "            pad_inches=1, \n",
    "            transparent=False, \n",
    "            facecolor=\"w\", \n",
    "            edgecolor='w', \n",
    "            orientation='landscape',\n",
    "            format='png')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1/T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename phenotypes\n",
    "base_rename_dict_struct = {\"struct_fast\":'FSL FAST',\n",
    "\"struct_sub_first\":'FSL FIRST',\n",
    "\"struct_aseg_mean_intensity\":'ASEG Mean Thickness',\n",
    "\"struct_aseg_volume\":'ASEG Volume',\n",
    "\"struct_ba_exvivo_area\":'BA ex-vivo Area',\n",
    "\"struct_ba_exvivo_mean_thickness\":'BA ex-vivo Mean Thickness',\n",
    "\"struct_ba_exvivo_volume\":'BA ex-vivo Volume',\n",
    "\"struct_a2009s_area\":'aparc.a2009s Area',\n",
    "\"struct_a2009s_mean_thickness\":'aparc.a2009s Mean Thickness',\n",
    "\"struct_a2009s_volume\":'aparc.a2009s Volume',\n",
    "\"struct_dkt_area\":'DKT Area',\n",
    "\"struct_dkt_mean_thickness\":'DKT Mean Thickness',\n",
    "\"struct_dkt_volume\":'DKT Volume',\n",
    "\"struct_desikan_gw\":'Desikan GM/WM Intensity',\n",
    "\"struct_desikan_pial\":'Desikan Pial',\n",
    "\"struct_desikan_white_area\":'Desikan WM Area',\n",
    "\"struct_desikan_white_mean_thickness\":'Desikan WM Mean Thickness',\n",
    "\"struct_desikan_white_volume\":'Desikan WM Volume',\n",
    "\"struct_subsegmentation\":'Subcortical Volumetric Subseg.',\n",
    "'T1_T2_whole_brain':'Whole-brain T1/T2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables\n",
    "with open('/commonality_analysis/struct/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/unique_t1_t2.pkl', 'rb') as f:\n",
    "    unique_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/common_mh_t1_t2.pkl', 'rb') as f:\n",
    "    common_mh_t1_t2 = pickle.load(f)\n",
    "# Set variables\n",
    "for key in unique_mh_t1_t2: # unique for MH\n",
    "    globals()[key] = round(unique_mh_t1_t2[key] * 100, 3)\n",
    "for key in unique_t1_t2: # unique for rs modalities\n",
    "    globals()[key] = round(unique_t1_t2[key] * 100, 3)\n",
    "for key in common_mh_t1_t2: # common for MH and rs modalities\n",
    "    globals()[key] = round(common_mh_t1_t2[key] * 100, 3)\n",
    "\n",
    "# Define x\n",
    "modalities = list(unique_mh_t1_t2.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in common_mh_t1_t2.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in unique_mh_t1_t2.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_t1_t2.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1] \n",
    "\n",
    "x_sorted = [base_rename_dict_struct.get(item, item) for item in np.array(x)[sorted_indices]] \n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.barh(x_sorted, y1_sorted, linewidth=1, color= '#f56f5c', alpha=0.6) \n",
    "plt.barh(x_sorted, y2_sorted, left=y1_sorted, color='#B24745FF', linewidth=1, alpha=0.3)\n",
    "plt.barh(x_sorted, y3_sorted, left=y1_sorted+y2_sorted, color= '#DF8F4499', linewidth=1, alpha=0.3)\n",
    "\n",
    "\n",
    "for i, (value_c, value_u_mh, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    plt.text(value_c / 2.2, i, f'{value_c:.1f}%', ha='left', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh / 2, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh + value_u_mri / 2, i, f'{value_u_mri:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "\n",
    "plt.xlabel(\"% Variance Explained\", fontsize=25, labelpad=20)\n",
    "#plt.legend([\"Common Variance: Mental Health + sMRI\", \"Unique variance: Mental Health\", \"Unique variance: sMRI\"], fontsize = 10, ncol=3, loc='lower center', bbox_to_anchor=(0.5, -0.2))\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.tick_params(axis='y', length=0)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1.5))\n",
    "\n",
    "plt.savefig(\"/CA-sMRI.png\",\n",
    "            bbox_inches=\"tight\", \n",
    "            pad_inches=1, \n",
    "            transparent=False, \n",
    "            facecolor=\"w\", \n",
    "            edgecolor='w', \n",
    "            orientation='landscape',\n",
    "            format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables\n",
    "with open('/commonality_analysis/struct/pls/unique_mh.pkl', 'rb') as f:\n",
    "    unique_mh_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/unique_t1_t2.pkl', 'rb') as f:\n",
    "    unique_t1_t2 = pickle.load(f)\n",
    "\n",
    "with open('/commonality_analysis/struct/pls/common_mh_t1_t2.pkl', 'rb') as f:\n",
    "    common_mh_t1_t2 = pickle.load(f)\n",
    "# Set variables\n",
    "for key in unique_mh_t1_t2: # unique for MH\n",
    "    globals()[key] = round(unique_mh_t1_t2[key] * 100, 3)\n",
    "for key in unique_t1_t2: # unique for rs modalities\n",
    "    globals()[key] = round(unique_t1_t2[key] * 100, 3)\n",
    "for key in common_mh_t1_t2: # common for MH and rs modalities\n",
    "    globals()[key] = round(common_mh_t1_t2[key] * 100, 3)\n",
    "\n",
    "# Define x\n",
    "modalities = list(unique_mh_t1_t2.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in common_mh_t1_t2.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in unique_mh_t1_t2.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_t1_t2.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1] \n",
    "\n",
    "x_sorted = [base_rename_dict_struct.get(item, item) for item in np.array(x)[sorted_indices]] \n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.barh(x_sorted, y1_sorted, linewidth=1, color= '#f56f5c', alpha=0.6) \n",
    "plt.barh(x_sorted, y2_sorted, left=y1_sorted, color='#B24745FF', linewidth=1, alpha=0.3)\n",
    "plt.barh(x_sorted, y3_sorted, left=y1_sorted+y2_sorted, color= '#DF8F4499', linewidth=1, alpha=0.3)\n",
    "\n",
    "\n",
    "for i, (value_c, value_u_mh, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    plt.text(value_c / 2.2, i, f'{value_c:.1f}%', ha='left', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh / 2, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "    plt.text(value_c + value_u_mh + value_u_mri / 2, i, f'{value_u_mri:.1f}%', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "\n",
    "plt.xlabel(\"% Variance Explained\", fontsize=25, labelpad=20)\n",
    "plt.legend([\"Common Variance: Mental Health + sMRI\", \"Unique variance: Mental Health\", \"Unique variance: sMRI\"], fontsize = 10, ncol=3, loc='lower center', bbox_to_anchor=(0.5, -0.2))\n",
    "\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.tick_params(axis='y', length=0)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1.5))\n",
    "\n",
    "plt.savefig(\"/CA-sMRI.png\",\n",
    "            bbox_inches=\"tight\", \n",
    "            pad_inches=1, \n",
    "            transparent=False, \n",
    "            facecolor=\"w\", \n",
    "            edgecolor='w', \n",
    "            orientation='landscape',\n",
    "            format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculate % of variance explained by MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTI\n",
    "commonality_metrics_dti_df = pd.DataFrame.from_dict(commonality_metrics_dti, orient='index')\n",
    "commonality_metrics_dti_df['Var Exp MRI'] = commonality_metrics_dti_df['Common'].round(2) / (commonality_metrics_dti_df['Common'].round(2) + commonality_metrics_dti_df['Unique: Mental Health'].round(2))\n",
    "commonality_metrics_dti_df['Var Exp MRI'] = (commonality_metrics_dti_df['Var Exp MRI'] * 100).round(2)\n",
    "commonality_metrics_dti_df['Total Var'] = commonality_metrics_dti_df.iloc[:, :-1].sum(axis=1)\n",
    "commonality_metrics_dti_df = commonality_metrics_dti_df.sort_values(by='Total Var', ascending = False)\n",
    "commonality_metrics_dti_df.to_csv('/commonality_analysis/var_explained_dti_full.csv')\n",
    "commonality_metrics_dti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename according to plots\n",
    "commonality_metrics_dti_df_renamed = commonality_metrics_dti_df.rename(index=base_rename_dict_dti)\n",
    "commonality_metrics_dti_df_renamed.to_csv('/commonality_analysis/var_explained_dti_full_renamed.csv')\n",
    "commonality_metrics_dti_df_renamed.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RS\n",
    "commonality_metrics_rs_df = pd.DataFrame.from_dict(commonality_metrics_rs, orient='index')\n",
    "commonality_metrics_rs_df['Var Exp MRI'] = commonality_metrics_rs_df['Common'].round(2) / (commonality_metrics_rs_df['Common'].round(2) + commonality_metrics_rs_df['Unique: Mental Health'].round(2))\n",
    "commonality_metrics_rs_df['Var Exp MRI'] = (commonality_metrics_rs_df['Var Exp MRI'] * 100).round(2)\n",
    "commonality_metrics_rs_df['Total Var'] = commonality_metrics_rs_df.iloc[:, :-1].sum(axis=1)\n",
    "commonality_metrics_rs_df = commonality_metrics_rs_df.sort_values(by='Total Var', ascending = False)\n",
    "commonality_metrics_rs_df.to_csv('/commonality_analysis/var_explained_rs_full.csv')\n",
    "commonality_metrics_rs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename according to plots\n",
    "commonality_metrics_rs_df_renamed = commonality_metrics_rs_df.rename(index=base_rename_dict_rs)\n",
    "commonality_metrics_rs_df_renamed.to_csv('/commonality_analysis/var_explained_rs_full_renamed.csv')\n",
    "commonality_metrics_rs_df_renamed.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T1/T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1/T2\n",
    "commonality_metrics_struct_df = pd.DataFrame.from_dict(commonality_metrics_struct, orient='index')\n",
    "commonality_metrics_struct_df['Var Exp MRI'] = commonality_metrics_struct_df['Common'].round(2) / (commonality_metrics_struct_df['Common'].round(2) + commonality_metrics_struct_df['Unique: Mental Health'].round(2))\n",
    "commonality_metrics_struct_df['Var Exp MRI'] = (commonality_metrics_struct_df['Var Exp MRI'] * 100).round(2)\n",
    "commonality_metrics_struct_df['Total Var'] = commonality_metrics_struct_df.iloc[:, :-1].sum(axis=1)\n",
    "commonality_metrics_struct_df = commonality_metrics_struct_df.sort_values(by='Total Var', ascending = False)\n",
    "commonality_metrics_struct_df.to_csv('/var_explained_struct_full.csv')\n",
    "commonality_metrics_struct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename according to plots\n",
    "commonality_metrics_struct_df_renamed = commonality_metrics_struct_df.rename(index=base_rename_dict_struct)\n",
    "commonality_metrics_struct_df_renamed.to_csv('/commonality_analysis/var_explained_struct_full_renamed.csv')\n",
    "commonality_metrics_struct_df_renamed.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all three CA PLSR stacked bar plots in one figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Birght\n",
    "fig, axs = plt.subplots(1, 3, figsize=(57, 40))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "######################## DTI\n",
    "modalities = list(unique_mh_dti.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in unique_mh_dti.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in common_mh_dti.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_dti.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1] \n",
    "\n",
    "x_sorted = [base_rename_dict_dti.get(item, item) for item in np.array(x)[sorted_indices]] \n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "bar_height = 0.7\n",
    "axs[0].barh(x_sorted, y1_sorted, height=bar_height, linewidth=2, color= '#B24745FF', alpha=0.6) \n",
    "axs[0].barh(x_sorted, y2_sorted, height=bar_height, left=y1_sorted, color='#66CCEEFF', linewidth=1, alpha=0.9) #cbdae8\n",
    "axs[0].barh(x_sorted, y3_sorted, height=bar_height, left=y1_sorted+y2_sorted, color= '#5C88DAFF', linewidth=1, alpha=0.9)\n",
    "\n",
    "for i, (value_u_mh, value_c, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    axs[0].text(value_u_mh / 2, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=33)\n",
    "    axs[0].text(value_u_mh + value_c / 2, i, f'{value_c:.1f}%', ha='center', va='center', color='black', fontsize=33)\n",
    "    if i == 0 or i == 1:\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2 + 3.0, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif 2 <= i <= 6: #i in range(2, 9)\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2 + 2.4, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif 7 <= i <= 12:\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2 + 2.2, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif 13 <= i <= 15:\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2 + 1.9, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif 16 <= i <= 28:\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2 + 1.5, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif 29 <= i <= 36:\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2 + 1.3, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    else:\n",
    "        axs[0].text(value_u_mh + value_c + value_u_mri / 2, i, f'{value_u_mri.round(1)}%', ha='center', va='center', color='black', fontsize=33)\n",
    "\n",
    "axs[0].set_yticklabels([])\n",
    "axs[0].set_yticklabels(x_sorted, fontsize=45)\n",
    "axs[0].set_xticklabels(axs[0].get_xticks(), fontsize=50)\n",
    "axs[0].tick_params(axis='y', length=0)\n",
    "\n",
    "axs[0].spines['top'].set_visible(False)\n",
    "axs[0].spines['right'].set_visible(False)\n",
    "axs[0].spines['left'].set_visible(False)\n",
    "\n",
    "######################## RS\n",
    "\n",
    "modalities = list(unique_mh_rs.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in unique_mh_rs.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in common_mh_rs.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_rs.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1]\n",
    "\n",
    "x_sorted = [base_rename_dict_rs.get(item, item) for item in np.array(x)[sorted_indices]] \n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "# Plot\n",
    "bar_height = 0.8\n",
    "axs[1].barh(x_sorted, y1_sorted, height=bar_height, linewidth=1, color= '#B24745FF', alpha=0.6)\n",
    "axs[1].barh(x_sorted, y2_sorted, height=bar_height, left=y1_sorted, color='#79AF9799', linewidth=1, alpha=0.6)  #edded7 418D87FF\n",
    "axs[1].barh(x_sorted, y3_sorted, height=bar_height, left=y1_sorted+y2_sorted, color= '#2C715FFF', linewidth=1, alpha=0.6)\n",
    "\n",
    "for i, (value_u_mh, value_c, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    axs[1].text(value_u_mh / 2, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=33)\n",
    "    axs[1].text(value_u_mh + value_c / 2, i, f'{value_c:.1f}%', ha='center', va='center', color='black', fontsize=33)\n",
    "\n",
    "    if i == 0 or i == 1:\n",
    "        axs[1].text(value_u_mh + value_c + value_u_mri / 2 + 3.5, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i == 2:\n",
    "        axs[1].text(value_u_mh + value_c + value_u_mri / 2 + 2.2, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i in range (3,5): \n",
    "        axs[1].text(value_u_mh + value_c + value_u_mri / 2 + 2, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i in range (4,7):\n",
    "        axs[1].text(value_u_mh + value_c + value_u_mri / 2 + 1.6, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    else:\n",
    "        axs[1].text(value_u_mh + value_c + value_u_mri / 2, i, f'{value_u_mri.round(1)}%', ha='center', va='center', color='black', fontsize=33)\n",
    "\n",
    "axs[1].set_yticklabels([])\n",
    "axs[1].set_yticklabels(x_sorted, fontsize=52)\n",
    "axs[1].set_xticklabels(axs[1].get_xticks(), fontsize=50)\n",
    "axs[1].tick_params(axis='y', length=0)\n",
    "\n",
    "axs[1].spines['top'].set_visible(False)\n",
    "axs[1].spines['right'].set_visible(False)\n",
    "axs[1].spines['left'].set_visible(False)\n",
    "\n",
    "######################## T1/T2\n",
    "\n",
    "modalities = list(unique_mh_t1_t2.keys())\n",
    "x = [modality.replace('u_mh_from_', '') for modality in modalities]\n",
    "y1 = np.array([globals()[f'{key}'] for key in unique_mh_t1_t2.keys()])\n",
    "y2 = np.array([globals()[f'{key}'] for key in common_mh_t1_t2.keys()])\n",
    "y3 = np.array([globals()[f'{key}'] for key in unique_t1_t2.keys()])\n",
    "\n",
    "# Sort plots based on total variance\n",
    "total_variance = y1 + y2 + y3\n",
    "sorted_indices = np.argsort(total_variance)[::1] \n",
    "\n",
    "x_sorted = [base_rename_dict_struct.get(item, item) for item in np.array(x)[sorted_indices]] \n",
    "y1_sorted = y1[sorted_indices]\n",
    "y2_sorted = y2[sorted_indices]\n",
    "y3_sorted = y3[sorted_indices]\n",
    "\n",
    "bar_height = 0.8\n",
    "axs[2].barh(x_sorted, y1_sorted, height=bar_height, linewidth=1, color= '#B24745FF', alpha=0.6) \n",
    "axs[2].barh(x_sorted, y2_sorted, height=bar_height, left=y1_sorted, color='#EDD03EFF', linewidth=1, alpha=0.5) #E37D41FF\n",
    "axs[2].barh(x_sorted, y3_sorted, height=bar_height, left=y1_sorted+y2_sorted, color= '#DF8F44FF', linewidth=1, alpha=0.7)\n",
    "\n",
    "# Adjust % positions\n",
    "for i, (value_u_mh, value_c, value_u_mri) in enumerate(zip(y1_sorted, y2_sorted, y3_sorted)):\n",
    "    axs[2].text(value_u_mh / 2 + 0.1, i, f'{value_u_mh:.1f}%', ha='center', va='center', color='black', fontsize=33)\n",
    "    axs[2].text(value_u_mh + value_c / 2, i, f'{value_c:.1f}%', ha='center', va='center', color='black', fontsize=33)\n",
    "\n",
    "    if i == 0:\n",
    "        axs[2].text(value_u_mh + value_c + value_u_mri / 2 + 3.1, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i in range(1,4):\n",
    "        axs[2].text(value_u_mh + value_c + value_u_mri / 2 + 2.8, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i in range(4,13):\n",
    "        axs[2].text(value_u_mh + value_c + value_u_mri / 2 + 2.5, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i == 13:\n",
    "        axs[2].text(value_u_mh + value_c + value_u_mri / 2 + 2.4, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    elif i in range(14,19):\n",
    "        axs[2].text(value_u_mh + value_c + value_u_mri / 2 + 1.8, i, f'{value_u_mri.round(1)}%', ha='right', va='center', color='black', fontsize=33)\n",
    "    else:\n",
    "        axs[2].text(value_u_mh + value_c + value_u_mri / 2, i, f'{value_u_mri.round(1)}%', ha='center', va='center', color='black', fontsize=33)\n",
    "\n",
    "axs[2].set_yticklabels([])\n",
    "axs[2].set_yticklabels(x_sorted, fontsize=52)\n",
    "axs[2].set_xticklabels(axs[2].get_xticks(), fontsize=50)\n",
    "axs[2].tick_params(axis='y', length=0)\n",
    "\n",
    "\n",
    "axs[2].spines['top'].set_visible(False)\n",
    "axs[2].spines['right'].set_visible(False)\n",
    "axs[2].spines['left'].set_visible(False)\n",
    "\n",
    "####\n",
    "\n",
    "axs[0].tick_params(axis='y', pad=10)\n",
    "axs[0].xaxis.set_major_formatter(ticker.FuncFormatter(lambda val, pos: f'{int(val)}')) \n",
    "axs[1].tick_params(axis='y', pad=10)\n",
    "axs[1].xaxis.set_major_formatter(ticker.FuncFormatter(lambda val, pos: f'{int(val)}')) \n",
    "axs[2].tick_params(axis='y', pad=10)\n",
    "axs[2].xaxis.set_major_formatter(ticker.FuncFormatter(lambda val, pos: f'{int(val)}'))\n",
    "\n",
    "####\n",
    "\n",
    "axs[0].set_title('dwMRI', fontsize=100, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#66CCEEFF', boxstyle='round', pad=0.1, linewidth=4), y=0.97, x=0.42)\n",
    "axs[1].set_title('rsMRI', fontsize=100, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#79AF9799', boxstyle='round', pad=0.1, linewidth=4), y=0.97, x=0.42)\n",
    "axs[2].set_title('sMRI', fontsize=100, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#EDD03EFF', boxstyle='round', pad=0.1, linewidth=4), y=0.97, x=0.43)\n",
    "\n",
    "\n",
    "# Add common title and x-axis label\n",
    "fig.text(0.5, -0.03, '% Variance Explained', ha='center', fontsize=80)\n",
    "\n",
    "# Add common legend\n",
    "colors = ['#66CCEEFF', '#79AF9799', '#EDD03EFF',\n",
    "          '#B24745FF', '#5C88DAFF', '#2C715FFF', '#DF8F44FF']\n",
    "alphas = [0.9, 0.9, 0.5, 0.6, 0.9, 0.6, 0.9]\n",
    "labels = ['Common Variance: Mental Health + dwMRI', 'Common Variance: Mental Health + rsMRI', 'Common Variance: Mental Health + sMRI',\n",
    "          'Unique variance: Mental Health', 'Unique variance: dwMRI', 'Unique variance: rsMRI', 'Unique variance: sMRI']\n",
    "handles = [plt.Rectangle((0,0),1,1, color=color, alpha=alpha) for color, alpha in zip(colors, alphas)]\n",
    "\n",
    "fig.legend(handles, labels, loc='lower center', ncol=3, fontsize=65, frameon=False, bbox_to_anchor=(0.55, -0.2))\n",
    "fig.tight_layout() #rect=[0, 0, 1, 0.96]\n",
    "\n",
    "plt.subplots_adjust(top=1.2)\n",
    "\n",
    "plt.savefig(\"/media/hcs-sci-psy-narun/IBu/Articles-Conferences/Articles/eLife/figures/finset/Fig4b.pdf\",\n",
    "            bbox_inches =\"tight\", \n",
    "            #pad_inches = 0.5, \n",
    "            transparent = False, \n",
    "            facecolor =\"w\", \n",
    "            edgecolor ='w', \n",
    "            orientation ='landscape',\n",
    "            format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporate demographics: sex, age*sex, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U (i) = R2_ijk  R2_jk\n",
    "- U (j) = R2_ijk  R2_ik\n",
    "- U (k) = R2_ijk  R2_ij\n",
    "\n",
    "- C (ij) = R2_ik + R2_jk  R2_k  R2_ijk\n",
    "- C (ik) = R2_ij + R2_jk  R2_j  R2_ijk\n",
    "- C (jk) = R2_ij + R2_ik  R2_i  R2_ijk\n",
    "- C (ijk) = R2_i + R2_j + R2_k  R2_ij  R2_ik  R2_jk + R2_ijk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U (mh) = R2_mh_rs_age  R2_rs_age\n",
    "- U (rs) = R2_mh_rs_age  R2_mh_age\n",
    "- U (age) = R2_mh_rs_age  R2_mh_rs\n",
    "\n",
    "- C (mh_rs) = R2_mh_age + R2_rs_age  R2_age  R2_mh_rs_age\n",
    "- C (mh_age) = R2_mh_rs + R2_rs_age  R2_rs  R2_mh_rs_age\n",
    "- C (rs_age) = R2_mh_rs + R2_mh_age  R2_mh  R2_mh_rs_age\n",
    "- C (mh_rs_age) = R2_mh + R2_rs + R2_age  R2_mh_rs  R2_mh_age  R2_rs_age + R2_mh_rs_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_confounds_min = pd.read_csv('/ML_DATASETS/basic_confounds_min.csv')[['Age when attended assessment centre', 'eid', 'Sex', 'Age2', 'Age*Sex', 'Age2*Sex']]\n",
    "demo = pd.DataFrame(basic_confounds_min)\n",
    "demo = demo.rename(columns={'Age when attended assessment centre': 'Age'})\n",
    "demo.to_csv('/PLS/brain/stacking/all/demo.csv', index=False)\n",
    "demo = pd.read_csv('/PLS/brain/stacking/all/demo.csv')\n",
    "demo_var = ['Age', 'Sex', 'Age2', 'Age*Sex', 'Age2*Sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "all_g_mh_rs_demo_concat = []\n",
    "demo_var = ['Age', 'Sex', 'Age2', 'Age*Sex', 'Age2*Sex']\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_rs_idp_rf = pd.read_csv(f'/PLS/brain/stacking/g/RS_IDP_Timeseries_best_metrics_target_pred_2nd_level_rf_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_rs_idp_rf, on='eid').merge(demo, on='eid').drop(columns=['eid'])\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_rs_idp_ts_best_stack_test': 'g_pred_rs_idp'})\n",
    "    all_g_mh_rs_demo_concat.append(all_g)\n",
    "    all_g_mh_rs_demo = pd.concat(all_g_mh_rs_demo_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_rs_demo.to_csv('/commonality_analysis/g_real_pred_mh_rs_demo.csv', index=False)\n",
    "    \n",
    "model = LinearRegression()\n",
    "r2_mh = model.fit(all_g_mh_rs_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_rs_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_rs = model.fit(all_g_mh_rs_demo['g_pred_rs_idp'].values.reshape(-1, 1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_rs_demo['g_pred_rs_idp'].values.reshape(-1, 1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = model.fit(all_g_mh_rs_demo[demo_var].values, all_g_mh_rs_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_rs_demo[demo_var].values, all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_rs_demo = model.fit(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real']).score(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "\n",
    "r2_mh_demo = model.fit(pd.concat([all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real']).score(pd.concat([all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_rs_demo = model.fit(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real']).score(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_rs = model.fit(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo['g_pred_mh']], axis=1), all_g_mh_rs_demo['g_real']).score(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo['g_pred_mh']], axis=1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_rs_demo = pd.read_csv('/commonality_analysis/g_real_pred_mh_rs_demo.csv')\n",
    "\n",
    "r2_mh = commonality_analysis(all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo['g_real'])\n",
    "r2_rs = commonality_analysis(all_g_mh_rs_demo['g_pred_rs_idp'].values.reshape(-1, 1), all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = commonality_analysis(all_g_mh_rs_demo[demo_var], all_g_mh_rs_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_rs_demo = commonality_analysis(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real'])\n",
    "\n",
    "r2_mh_demo = commonality_analysis(pd.concat([all_g_mh_rs_demo['g_pred_mh'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real'])\n",
    "r2_rs_demo = commonality_analysis(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo[demo_var]], axis=1), all_g_mh_rs_demo['g_real'])\n",
    "r2_mh_rs = commonality_analysis(pd.concat([all_g_mh_rs_demo['g_pred_rs_idp'], all_g_mh_rs_demo['g_pred_mh']], axis=1), all_g_mh_rs_demo['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# i = mh, j = rs, k = demo\n",
    "\n",
    "u_mh = r2_mh_rs_demo - r2_rs_demo\n",
    "u_rs = r2_mh_rs_demo - r2_mh_demo\n",
    "u_demo = r2_mh_rs_demo - r2_mh_rs\n",
    "\n",
    "c_mh_rs = r2_mh_demo + r2_rs_demo - r2_demo - r2_mh_rs_demo\n",
    "c_mh_demo = r2_mh_rs + r2_rs_demo - r2_rs - r2_mh_rs_demo\n",
    "c_rs_demo = r2_mh_rs + r2_mh_demo - r2_mh - r2_mh_rs_demo\n",
    "c_mh_rs_demo = r2_mh + r2_rs + r2_demo - r2_mh_rs - r2_mh_demo - r2_rs_demo + r2_mh_rs_demo\n",
    "\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh.round(3)) # f\"{r2_mh_m:.5f}\")\n",
    "print(\"R squared for RS\", r2_rs.round(3))\n",
    "print(\"R squared for demo\", r2_demo.round(3))\n",
    "print(\"R squared for MH, RS, and demo\", r2_mh_rs_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh.round(3)) #\"{:.5f}\".format(u_mh)) # \"{:.5f}\".format(u_mh)\n",
    "print(\"Unique variance for RS:\", u_rs.round(3))\n",
    "print(\"Unique variance for demo:\", u_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and RS:\", c_mh_rs.round(3))\n",
    "print(\"Common variance for MH and demo:\", c_mh_demo.round(3))\n",
    "print(\"Common variance for RS and demo:\", c_rs_demo.round(3))\n",
    "print(\"Common variance for MH, RS, and demo:\", c_mh_rs_demo.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up a data frame with commonality metrics\n",
    "mh_rs_demo_commonality = pd.DataFrame([{'u_mh-rs_plus_demo': u_mh, 'u_rs-rs_plus_demo': u_rs, 'u_demo-rs_plus_demo': u_demo,\n",
    "                                       'c_mh_and_rs-rs_plus_demo': c_mh_rs, 'c_mh_and_demo-rs_plus_demo':c_mh_demo, 'c_rs_and_demo-rs_plus_demo': c_rs_demo, 'c_mh_and_rs_and_demo-rs_plus_demo': c_mh_rs_demo}]).round(4)\n",
    "mh_rs_demo_commonality.to_csv('/commonality_analysis/mh_rs_demo_commonality.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "all_g_mh_dti_demo_concat = []\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_dti_rf = pd.read_csv(f'/PLS/brain/stacking/g/DTI_All_target_pred_2nd_level_rf_test_fold_{fold}.csv')\n",
    "    g_pred_mh = g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_dti_rf, on='eid').merge(demo, on='eid').drop(columns=['eid'])\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_dti_all_stack_test': 'g_pred_dti'})\n",
    "    all_g_mh_dti_demo_concat.append(all_g)\n",
    "    all_g_mh_dti_demo = pd.concat(all_g_mh_dti_demo_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_dti_demo.to_csv('/commonality_analysis/g_real_pred_mh_dti-all_demo.csv', index=False)\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh = model.fit(all_g_mh_dti_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_dti_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_dti = model.fit(all_g_mh_dti_demo['g_pred_dti'].values.reshape(-1, 1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_dti_demo['g_pred_dti'].values.reshape(-1, 1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = model.fit(all_g_mh_dti_demo[demo_var].values, all_g_mh_dti_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_dti_demo[demo_var].values, all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_dti_demo = model.fit(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real']).score(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "\n",
    "r2_mh_demo = model.fit(pd.concat([all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real']).score(pd.concat([all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_dti_demo = model.fit(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real']).score(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_dti = model.fit(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo['g_pred_mh']], axis=1), all_g_mh_dti_demo['g_real']).score(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo['g_pred_mh']], axis=1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_dti_demo = pd.read_csv('/commonality_analysis/g_real_pred_mh_dti-all_demo.csv')\n",
    "r2_mh = commonality_analysis(all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo['g_real'])\n",
    "r2_dti = commonality_analysis(all_g_mh_dti_demo['g_pred_dti'].values.reshape(-1, 1), all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = commonality_analysis(all_g_mh_dti_demo[demo_var], all_g_mh_dti_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_dti_demo = commonality_analysis(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real'])\n",
    "\n",
    "r2_mh_demo = commonality_analysis(pd.concat([all_g_mh_dti_demo['g_pred_mh'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real'])\n",
    "r2_dti_demo = commonality_analysis(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo[demo_var]], axis=1), all_g_mh_dti_demo['g_real'])\n",
    "r2_mh_dti = commonality_analysis(pd.concat([all_g_mh_dti_demo['g_pred_dti'], all_g_mh_dti_demo['g_pred_mh']], axis=1), all_g_mh_dti_demo['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# i = mh, j = rs, k = demo\n",
    "\n",
    "u_mh = r2_mh_dti_demo - r2_dti_demo\n",
    "u_dti = r2_mh_dti_demo - r2_mh_demo\n",
    "u_demo = r2_mh_dti_demo - r2_mh_dti\n",
    "\n",
    "c_mh_dti = r2_mh_demo + r2_dti_demo - r2_demo - r2_mh_dti_demo\n",
    "c_mh_demo = r2_mh_dti + r2_dti_demo - r2_dti - r2_mh_dti_demo\n",
    "c_dti_demo = r2_mh_dti + r2_mh_demo - r2_mh - r2_mh_dti_demo\n",
    "c_mh_dti_demo = r2_mh + r2_dti + r2_demo - r2_mh_dti - r2_mh_demo - r2_dti_demo + r2_mh_dti_demo\n",
    "\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh.round(3))\n",
    "print(\"R squared for DTI All\", r2_dti.round(3))\n",
    "print(\"R squared for demo\", r2_demo.round(3))\n",
    "print(\"R squared for MH, DTI All, and demo\", r2_mh_dti_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh.round(3)) \n",
    "print(\"Unique variance for DTI All:\", u_dti.round(3)) #f\"{u_dti:.5f}\".round(3))\n",
    "print(\"Unique variance for demo:\", u_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and DTI All:\", c_mh_dti.round(3))\n",
    "print(\"Common variance for MH and demo:\", c_mh_demo.round(3))\n",
    "print(\"Common variance for dti and demo:\", c_dti_demo.round(3))\n",
    "print(\"Common variance for MH, DTI All, and demo:\", c_mh_dti_demo.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up a data frame with commonality metrics\n",
    "mh_dti_demo_commonality = pd.DataFrame([{'u_mh-dti_plus_demo': u_mh, 'u_dti-dti_plus_demo': u_dti, 'u_demo-dti_plus_demo': u_demo,\n",
    "                                       'c_mh_and_dti-dti_plus_demo': c_mh_dti, 'c_mh_and_demo-dti_plus_demo':c_mh_demo,\n",
    "                                       'c_dti_and_demo-dti_plus_demo': c_dti_demo, 'c_mh_and_dti_and_demo-dti_plus_demo': c_mh_dti_demo}]).round(4)\n",
    "mh_dti_demo_commonality.to_csv('/commonality_analysis/mh_dti_demo_commonality.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1/T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "all_g_mh_t1_demo_concat = []\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_t1_svr = pd.read_csv(f'/PLS/brain/stacking/g/T1_T2_whole_brain_target_pred_2nd_level_svr_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_t1_svr, on='eid').merge(demo, on='eid').drop(columns=['eid'])\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_mribest_stack_test': 'g_pred_t1'}) #the column was incorrectly named g_pred_mribest_stack_test instead of g_pred_t1_stack_test\n",
    "    all_g_mh_t1_demo_concat.append(all_g)\n",
    "    all_g_mh_t1_demo = pd.concat(all_g_mh_t1_demo_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_t1_demo.to_csv('/commonality_analysis/g_real_pred_mh_t1_demo.csv', index=False)\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh = model.fit(all_g_mh_t1_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_t1_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_t1 = model.fit(all_g_mh_t1_demo['g_pred_t1'].values.reshape(-1, 1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_t1_demo['g_pred_t1'].values.reshape(-1, 1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = model.fit(all_g_mh_t1_demo[demo_var].values, all_g_mh_t1_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_t1_demo[demo_var].values, all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_t1_demo = model.fit(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real']).score(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "\n",
    "r2_mh_demo = model.fit(pd.concat([all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real']).score(pd.concat([all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_t1_demo = model.fit(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real']).score(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_t1 = model.fit(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo['g_pred_mh']], axis=1), all_g_mh_t1_demo['g_real']).score(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo['g_pred_mh']], axis=1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_t1_demo = pd.read_csv('/commonality_analysis/g_real_pred_mh_t1_demo.csv')\n",
    "r2_mh = commonality_analysis(all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo['g_real'])\n",
    "r2_t1 = commonality_analysis(all_g_mh_t1_demo['g_pred_t1'].values.reshape(-1, 1), all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = commonality_analysis(all_g_mh_t1_demo[demo_var], all_g_mh_t1_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_t1_demo = commonality_analysis(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real'])\n",
    "\n",
    "r2_mh_demo = commonality_analysis(pd.concat([all_g_mh_t1_demo['g_pred_mh'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real'])\n",
    "r2_t1_demo = commonality_analysis(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo[demo_var]], axis=1), all_g_mh_t1_demo['g_real'])\n",
    "r2_mh_t1 = commonality_analysis(pd.concat([all_g_mh_t1_demo['g_pred_t1'], all_g_mh_t1_demo['g_pred_mh']], axis=1), all_g_mh_t1_demo['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# i = mh, j = rs, k = demo\n",
    "\n",
    "u_mh = r2_mh_t1_demo - r2_t1_demo\n",
    "u_t1 = r2_mh_t1_demo - r2_mh_demo\n",
    "u_demo = r2_mh_t1_demo - r2_mh_t1\n",
    "\n",
    "c_mh_t1 = r2_mh_demo + r2_t1_demo - r2_demo - r2_mh_t1_demo\n",
    "c_mh_demo = r2_mh_t1 + r2_t1_demo - r2_t1 - r2_mh_t1_demo\n",
    "c_t1_demo = r2_mh_t1 + r2_mh_demo - r2_mh - r2_mh_t1_demo\n",
    "c_mh_t1_demo = r2_mh + r2_t1 + r2_demo - r2_mh_t1 - r2_mh_demo - r2_t1_demo + r2_mh_t1_demo\n",
    "\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh.round(3))\n",
    "print(\"R squared for T1\", r2_t1.round(3))\n",
    "print(\"R squared for demo\", r2_demo.round(3))\n",
    "print(\"R squared for MH, T1, and demo\", r2_mh_t1_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh.round(3)) \n",
    "print(\"Unique variance for T1:\", u_t1.round(3)) #f\"{u_t1:.5f}\".round(3))\n",
    "print(\"Unique variance for demo:\", u_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and T1:\", c_mh_t1.round(3))\n",
    "print(\"Common variance for MH and demo:\", c_mh_demo.round(3))\n",
    "print(\"Common variance for T1 and demo:\", c_t1_demo.round(3))\n",
    "print(\"Common variance for MH, T1, and demo:\", c_mh_t1_demo.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up a data frame with commonality metrics\n",
    "mh_t1_demo_commonality = pd.DataFrame([{'u_mh-t1_plus_demo': u_mh, 'u_t1-t1_plus_demo': u_t1, 'u_demo-t1_plus_demo': u_demo,\n",
    "                                       'c_mh_and_t1-t1_plus_demo': c_mh_t1, 'c_mh_and_demo-t1_plus_demo':c_mh_demo, 'c_t1_and_demo-t1_plus_demo': c_t1_demo, 'c_mh_and_t1_and_demo-t1_plus_demo': c_mh_t1_demo}]).round(4)\n",
    "mh_t1_demo_commonality.to_csv('/commonality_analysis/mh_t1_demo_commonality.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 for each model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "all_g_mh_mri_demo_concat = []\n",
    "\n",
    "for fold in folds:\n",
    "    g_pred_mri_all = pd.read_csv(f'/PLS/brain/stacking/g/All_modalities_target_pred_2nd_level_xgb_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_mri_all, on='eid').merge(demo, on='eid').drop(columns=['eid'])\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_mribest_stack_test': 'g_pred_mri'})\n",
    "    all_g_mh_mri_demo_concat.append(all_g)\n",
    "    all_g_mh_mri_demo = pd.concat(all_g_mh_mri_demo_concat, axis=0, ignore_index=True)\n",
    "    all_g_mh_mri_demo.to_csv('/commonality_analysis/g_real_pred_mh_all-mri_demo.csv', index=False)\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "r2_mh = model.fit(all_g_mh_mri_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_mri_demo['g_pred_mh'].values.reshape(-1, 1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mri = model.fit(all_g_mh_mri_demo['g_pred_mri'].values.reshape(-1, 1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_mri_demo['g_pred_mri'].values.reshape(-1, 1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = model.fit(all_g_mh_mri_demo[demo_var], all_g_mh_mri_demo['g_real'].values.reshape(-1, 1)).score(all_g_mh_mri_demo[demo_var], all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_mri_demo = model.fit(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real']).score(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "\n",
    "r2_mh_demo = model.fit(pd.concat([all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real']).score(pd.concat([all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mri_demo = model.fit(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real']).score(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_mri = model.fit(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo['g_pred_mh']], axis=1), all_g_mh_mri_demo['g_real']).score(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo['g_pred_mh']], axis=1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_mri_demo = pd.read_csv('/commonality_analysis/g_real_pred_mh_all-mri_demo.csv')\n",
    "\n",
    "r2_mh = commonality_analysis(all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo['g_real'])\n",
    "r2_mri = commonality_analysis(all_g_mh_mri_demo['g_pred_mri'].values.reshape(-1, 1), all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_demo = commonality_analysis(all_g_mh_mri_demo[demo_var], all_g_mh_mri_demo['g_real'].values.reshape(-1, 1))\n",
    "r2_mh_mri_demo = commonality_analysis(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real'])\n",
    "\n",
    "r2_mh_demo = commonality_analysis(pd.concat([all_g_mh_mri_demo['g_pred_mh'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real'])\n",
    "r2_mri_demo = commonality_analysis(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo[demo_var]], axis=1), all_g_mh_mri_demo['g_real'])\n",
    "r2_mh_mri = commonality_analysis(pd.concat([all_g_mh_mri_demo['g_pred_mri'], all_g_mh_mri_demo['g_pred_mh']], axis=1), all_g_mh_mri_demo['g_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique/common variance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# i = mh, j = rs, k = demo\n",
    "\n",
    "u_mh = r2_mh_mri_demo - r2_mri_demo\n",
    "u_mri = r2_mh_mri_demo - r2_mh_demo\n",
    "u_demo = r2_mh_mri_demo - r2_mh_mri\n",
    "\n",
    "c_mh_mri = r2_mh_demo + r2_mri_demo - r2_demo - r2_mh_mri_demo\n",
    "c_mh_demo = r2_mh_mri + r2_mri_demo - r2_mri - r2_mh_mri_demo\n",
    "c_mri_demo = r2_mh_mri + r2_mh_demo - r2_mh - r2_mh_mri_demo\n",
    "c_mh_mri_demo = r2_mh + r2_mri + r2_demo - r2_mh_mri - r2_mh_demo - r2_mri_demo + r2_mh_mri_demo\n",
    "\n",
    "print(\"R squared for MH:\", r2_mh.round(3)) \n",
    "print(\"R squared for MRI All (no tang)\", r2_mri.round(3))\n",
    "print(\"R squared for demo\", r2_demo.round(3))\n",
    "print(\"R squared for MH, MRI All (no tang), and demo\", r2_mh_mri_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Unique variance for MH:\", u_mh.round(3)) #\"{:.5f}\".format(u_mh)) # \"{:.5f}\".format(u_mh)\n",
    "print(\"Unique variance for MRI All (no tang):\", u_mri.round(3)) #f\"{u_mri:.5f}\")\n",
    "print(\"Unique variance for demo:\", u_demo.round(3))\n",
    "\n",
    "print('_______________')\n",
    "print(\"Common variance for MH and MRI All (no tang):\", c_mh_mri.round(3))\n",
    "print(\"Common variance for MH and demo:\", c_mh_demo.round(3))\n",
    "print(\"Common variance for MRI All (no tang) and demo:\", c_mri_demo.round(3))\n",
    "print(\"Common variance for MH, MRI All, and demo:\", c_mh_mri_demo.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up a data frame with commonality metrics\n",
    "mh_mri_demo_commonality = pd.DataFrame([{'u_mh-mri_plus_demo': u_mh, 'u_mri-mri_plus_demo': u_mri, 'u_demo-mri_plus_demo': u_demo,\n",
    "                                       'c_mh_and_mri-mri_plus_demo': c_mh_mri, 'c_mh_and_demo-mri_plus_demo': c_mh_demo, 'c_mri_and_demo-mri_plus_demo': c_mri_demo, 'c_mh_and_mri_and_demo-mri_plus_demo': c_mh_mri_demo}]).round(4)\n",
    "mh_mri_demo_commonality.to_csv('/commonality_analysis/mh_mri_demo_commonality.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract commonality metrics for demo sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a table with commonality metrics\n",
    "mh_mri_demo_commonality_path = '/commonality_analysis/mh_mri_demo_commonality.csv'\n",
    "mh_rs_demo_commonality_path = '/commonality_analysis/mh_rs_demo_commonality.csv'\n",
    "mh_t1_demo_commonality_path = '/commonality_analysis/mh_t1_demo_commonality.csv'\n",
    "mh_dti_demo_commonality_path = '/commonality_analysis/mh_dti_demo_commonality.csv'\n",
    "\n",
    "# Load the data from CSV files\n",
    "mh_mri_demo_commonality = pd.read_csv(mh_mri_demo_commonality_path)\n",
    "mh_rs_demo_commonality = pd.read_csv(mh_rs_demo_commonality_path)\n",
    "mh_t1_demo_commonality = pd.read_csv(mh_t1_demo_commonality_path)\n",
    "mh_dti_demo_commonality = pd.read_csv(mh_dti_demo_commonality_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC, Common to MH, MRI, and Demo\n",
    "c_mh_and_mri_and_demo = (mh_mri_demo_commonality['c_mh_and_mri_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_rs_and_demo  = (mh_rs_demo_commonality['c_mh_and_rs_and_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_t1_and_demo  = (mh_t1_demo_commonality['c_mh_and_t1_and_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_dti_and_demo  = (mh_dti_demo_commonality['c_mh_and_dti_and_demo-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# AB, Common to MH and MRI\n",
    "c_mh_and_mri = (mh_mri_demo_commonality['c_mh_and_mri-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_rs  = (mh_rs_demo_commonality['c_mh_and_rs-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_t1  = (mh_t1_demo_commonality['c_mh_and_t1-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_dti  = (mh_dti_demo_commonality['c_mh_and_dti-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# AC, Common to MH and Demo\n",
    "c_mh_and_demo_mri = (mh_mri_demo_commonality['c_mh_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_demo_rs  = (mh_rs_demo_commonality['c_mh_and_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_demo_t1  = (mh_t1_demo_commonality['c_mh_and_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_demo_dti  = (mh_dti_demo_commonality['c_mh_and_demo-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# BC, Common to MRI and Demo\n",
    "c_mri_and_demo = (mh_mri_demo_commonality['c_mri_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_rs_and_demo  = (mh_rs_demo_commonality['c_rs_and_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_t1_and_demo  = (mh_t1_demo_commonality['c_t1_and_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_dti_and_demo  = (mh_dti_demo_commonality['c_dti_and_demo-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# A, Unique to MH\n",
    "u_mh_mri = (mh_mri_demo_commonality['u_mh-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "u_mh_rs = (mh_rs_demo_commonality['u_mh-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "u_mh_t1 = (mh_t1_demo_commonality['u_mh-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "u_mh_dti = (mh_dti_demo_commonality['u_mh-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# B, Unique to MRI\n",
    "u_mri = (mh_mri_demo_commonality['u_mri-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "u_rs = (mh_rs_demo_commonality['u_rs-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "u_t1 = (mh_t1_demo_commonality['u_t1-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "u_dti = (mh_dti_demo_commonality['u_dti-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# C, Unique to Demo\n",
    "u_demo_mri = (mh_mri_demo_commonality['u_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "u_demo_rs = (mh_rs_demo_commonality['u_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "u_demo_t1 = (mh_t1_demo_commonality['u_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "u_demo_dti = (mh_dti_demo_commonality['u_demo-dti_plus_demo'].iloc[0]*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the required values and round them\n",
    "data = {\n",
    "    'Index': ['dti', 'rs', 't1', 'mri'],\n",
    "    'Unique Mental Health': [\n",
    "        (mh_dti_demo_commonality['u_mh-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['u_mh-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['u_mh-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['u_mh-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ],\n",
    "    'Unique MRI': [\n",
    "        (mh_dti_demo_commonality['u_dti-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['u_rs-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['u_t1-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['u_mri-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ],\n",
    "    'Unique Demo': [\n",
    "        (mh_dti_demo_commonality['u_demo-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['u_demo-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['u_demo-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['u_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ],\n",
    "    'Common to MH and MRI': [\n",
    "        (mh_dti_demo_commonality['c_mh_and_dti-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['c_mh_and_rs-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['c_mh_and_t1-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['c_mh_and_mri-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ],\n",
    "    'Common to MH and Demo': [\n",
    "        (mh_dti_demo_commonality['c_mh_and_demo-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['c_mh_and_demo-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['c_mh_and_demo-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['c_mh_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ],\n",
    "    'Common to MH, MRI, and Demo': [\n",
    "        (mh_dti_demo_commonality['c_mh_and_dti_and_demo-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['c_mh_and_rs_and_demo-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['c_mh_and_t1_and_demo-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['c_mh_and_mri_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ],\n",
    "    'Common to MRI and Demo': [\n",
    "        (mh_dti_demo_commonality['c_dti_and_demo-dti_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_rs_demo_commonality['c_rs_and_demo-rs_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_t1_demo_commonality['c_t1_and_demo-t1_plus_demo'].iloc[0]*100).round(2),\n",
    "        (mh_mri_demo_commonality['c_mri_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('/commonality_analysis/commonality_metrics_plus_demo_supplem_table.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print default font settings\n",
    "print(\"Default font family:\", plt.rcParams['font.family'])\n",
    "print(\"Default font size:\", plt.rcParams['font.size'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create scatter plot for each modality\n",
    "def create_scatter_plot(ax, y_true, y_pred, title, cmap_name, edge_color, r_mean_stack, r_std_stack, r2_mean_stack, r2_std_stack, folds_data, modality):\n",
    "    dist_i = np.sqrt((y_true - y_pred.mean())**2 + (y_pred - y_pred.mean())**2)\n",
    "    sns.scatterplot(x=y_pred, y=y_true, c=dist_i, cmap=cmap_name, s=17, alpha=0.6, edgecolor=edge_color,  ax=ax, linewidth=0.1) #\n",
    "    \n",
    "    fontsize = 35\n",
    "    \n",
    "    # Add regression lines for each fold\n",
    "    for i, fold in enumerate(folds):\n",
    "        sns.regplot(x=folds_data[modality]['y_pred'][i], y=folds_data[modality]['y_true'][i], scatter=False, ax=ax, label=f'Fold {fold}', line_kws={\"color\": \"red\", \"linewidth\": 0.8})\n",
    "    \n",
    "    sns.despine(top=True, right=True, ax=ax)\n",
    "    ax.set_xlabel('Predicted $g$-factor ($z$)', fontsize=fontsize)\n",
    "    ax.set_ylabel('')\n",
    "    ax.tick_params(axis='x', labelsize=30)\n",
    "    ax.tick_params(axis='y', labelsize=30)\n",
    "    ax.set_title(title, fontsize=30, y=1.3)\n",
    "    \n",
    "    ax.text(0.05, 1.05, f'$r_{{mean}}$ = {r_mean_stack:.2f} (SD={r_std_stack:.2f})', transform=ax.transAxes, fontsize=fontsize)\n",
    "    if modality == 'rs':\n",
    "        ax.text(0.05, 0.96, f'$R^2_{{mean}}$ = {r2_mean_stack:.3f} (SD={r2_std_stack:.2f})', transform=ax.transAxes, fontsize=fontsize)\n",
    "    else:\n",
    "        ax.text(0.05, 0.96, f'$R^2_{{mean}}$ = {r2_mean_stack:.2f} (SD={r2_std_stack:.2f})', transform=ax.transAxes, fontsize=fontsize)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a table with commonality metrics without demo\n",
    "commonality_metrics_no_demo = pd.read_csv('/commonality_analysis/commonality_metrics_no_demo.csv')\n",
    "# Extract the values from the DataFrame\n",
    "u_mh_rs_na = commonality_metrics_no_demo['u_mh_rs_na'].iloc[0]\n",
    "u_rs_na = commonality_metrics_no_demo['u_rs_na'].iloc[0]\n",
    "c_mh_and_rs_na = commonality_metrics_no_demo['c_mh_and_rs_na'].iloc[0]\n",
    "u_mh_dti_na = commonality_metrics_no_demo['u_mh_dti_na'].iloc[0]\n",
    "u_dti_na = commonality_metrics_no_demo['u_dti_na'].iloc[0]\n",
    "c_mh_and_dti_na = commonality_metrics_no_demo['c_mh_and_dti_na'].iloc[0]\n",
    "u_mh_t1_na = commonality_metrics_no_demo['u_mh_t1_na'].iloc[0]\n",
    "u_t1_na = commonality_metrics_no_demo['u_t1_na'].iloc[0].astype(float).round(3)\n",
    "c_mh_and_t1_na = commonality_metrics_no_demo['c_mh_and_t1_na'].iloc[0]\n",
    "u_mh_mri_na = commonality_metrics_no_demo['u_mh_mri_na'].iloc[0]\n",
    "u_mri_na = commonality_metrics_no_demo['u_mri_na'].iloc[0]\n",
    "c_mh_and_mri_na = commonality_metrics_no_demo['c_mh_and_mri_na'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a table with commonality metrics for demo\n",
    "mh_mri_demo_commonality_path = '/commonality_analysis/mh_mri_demo_commonality.csv'\n",
    "mh_rs_demo_commonality_path = '/commonality_analysis/mh_rs_demo_commonality.csv'\n",
    "mh_t1_demo_commonality_path = '/commonality_analysis/mh_t1_demo_commonality.csv'\n",
    "mh_dti_demo_commonality_path = '/commonality_analysis/mh_dti_demo_commonality.csv'\n",
    "\n",
    "# Load the data from CSV files\n",
    "mh_mri_demo_commonality = pd.read_csv(mh_mri_demo_commonality_path)\n",
    "mh_rs_demo_commonality = pd.read_csv(mh_rs_demo_commonality_path)\n",
    "mh_t1_demo_commonality = pd.read_csv(mh_t1_demo_commonality_path)\n",
    "mh_dti_demo_commonality = pd.read_csv(mh_dti_demo_commonality_path)\n",
    "\n",
    "# Extract variances\n",
    "# ABC, Common to MH, MRI, and Demo\n",
    "c_mh_and_mri_and_demo = (mh_mri_demo_commonality['c_mh_and_mri_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_rs_and_demo  = (mh_rs_demo_commonality['c_mh_and_rs_and_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_t1_and_demo  = (mh_t1_demo_commonality['c_mh_and_t1_and_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_dti_and_demo  = (mh_dti_demo_commonality['c_mh_and_dti_and_demo-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# AB, Common to MH and MRI\n",
    "c_mh_and_mri = (mh_mri_demo_commonality['c_mh_and_mri-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_rs  = (mh_rs_demo_commonality['c_mh_and_rs-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_t1  = (mh_t1_demo_commonality['c_mh_and_t1-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_dti  = (mh_dti_demo_commonality['c_mh_and_dti-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# AC, Common to MH and Demo\n",
    "c_mh_and_demo_mri = (mh_mri_demo_commonality['c_mh_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_demo_rs  = (mh_rs_demo_commonality['c_mh_and_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_demo_t1  = (mh_t1_demo_commonality['c_mh_and_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_mh_and_demo_dti  = (mh_dti_demo_commonality['c_mh_and_demo-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# BC, Common to MRI and Demo\n",
    "c_mri_and_demo = (mh_mri_demo_commonality['c_mri_and_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "c_rs_and_demo  = (mh_rs_demo_commonality['c_rs_and_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "c_t1_and_demo  = (mh_t1_demo_commonality['c_t1_and_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "c_dti_and_demo  = (mh_dti_demo_commonality['c_dti_and_demo-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# A, Unique to MH\n",
    "u_mh_mri = (mh_mri_demo_commonality['u_mh-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "u_mh_rs = (mh_rs_demo_commonality['u_mh-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "u_mh_t1 = (mh_t1_demo_commonality['u_mh-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "u_mh_dti = (mh_dti_demo_commonality['u_mh-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# B, Unique to MRI\n",
    "u_mri = (mh_mri_demo_commonality['u_mri-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "u_rs = (mh_rs_demo_commonality['u_rs-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "u_t1 = (mh_t1_demo_commonality['u_t1-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "u_dti = (mh_dti_demo_commonality['u_dti-dti_plus_demo'].iloc[0]*100).round(2)\n",
    "\n",
    "# C, Unique to Demo\n",
    "u_demo_mri = (mh_mri_demo_commonality['u_demo-mri_plus_demo'].iloc[0]*100).round(2)\n",
    "u_demo_rs = (mh_rs_demo_commonality['u_demo-rs_plus_demo'].iloc[0]*100).round(2)\n",
    "u_demo_t1 = (mh_t1_demo_commonality['u_demo-t1_plus_demo'].iloc[0]*100).round(2)\n",
    "u_demo_dti = (mh_dti_demo_commonality['u_demo-dti_plus_demo'].iloc[0]*100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y_true and y_pred for all modalities\n",
    "from sklearn.metrics import r2_score\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "y_pred_pooled = []\n",
    "y_true_pooled = []\n",
    "\n",
    "for i,fold in enumerate(folds):\n",
    "    y_pred = pd.read_csv(f'/PLS/brain/stacking/g/DTI_All_target_pred_2nd_level_rf_test_fold_{fold}.csv')['g_pred_dti_all_stack_test']\n",
    "    y_pred_pooled.append(y_pred)\n",
    "    y_pred_pooled_df = pd.DataFrame(pd.concat(y_pred_pooled, ignore_index=True)).rename(columns={'g_pred_dti_all_stack_test': 'y_pred'})\n",
    "    y_true = pd.read_csv(f'/PLS/brain/stacking/g/DTI_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    y_true_pooled.append(y_true)\n",
    "    y_true_pooled_df = pd.DataFrame(pd.concat(y_true_pooled, ignore_index=True)).rename(columns={'g_real_scaled': 'y_true'})\n",
    "\n",
    "y_true_dti = y_true_pooled_df.copy()['y_true']\n",
    "y_pred_dti = y_pred_pooled_df.copy()['y_pred']\n",
    "\n",
    "r2_scores = []\n",
    "pearson_corrs = []\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.read_csv(f'/PLS/brain/stacking/g/DTI_All_target_pred_2nd_level_rf_test_fold_{fold}.csv')['g_pred_dti_all_stack_test']\n",
    "    y_true_fold = pd.read_csv(f'/PLS/brain/stacking/g/DTI_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "    r2_scores.append(r2)\n",
    "    corr, _ = pearsonr(y_true_fold.squeeze(), y_pred_fold.squeeze())\n",
    "    pearson_corrs.append(corr)\n",
    "    \n",
    "# Annotate the plot with Pearson correlation and R score\n",
    "#corr, p = pearsonr(y_true, y_pred)\n",
    "r_mean_dti = np.mean(pearson_corrs)\n",
    "r_std_dti = np.std(pearson_corrs)\n",
    "r2_mean_dti = np.mean(r2_scores)\n",
    "r2_std_dti = np.std(r2_scores)\n",
    "\n",
    "\n",
    "############################################# RS\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "y_pred_pooled = []\n",
    "y_true_pooled = []\n",
    "\n",
    "for i,fold in enumerate(folds):\n",
    "    y_pred = pd.read_csv(f'/PLS/brain/stacking/g/RS_IDP_Timeseries_best_metrics_target_pred_2nd_level_rf_test_fold_{fold}.csv')['g_pred_rs_idp_ts_best_stack_test']\n",
    "    y_pred_pooled.append(y_pred)\n",
    "    y_pred_pooled_df = pd.DataFrame(pd.concat(y_pred_pooled, ignore_index=True)).rename(columns={'g_pred_rs_idp_ts_best_stack_test': 'y_pred'})\n",
    "    y_true = pd.read_csv(f'/PLS/brain/stacking/g/RS_Aug_ID_g/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    y_true_pooled.append(y_true)\n",
    "    y_true_pooled_df = pd.DataFrame(pd.concat(y_true_pooled, ignore_index=True)).rename(columns={'g_real_scaled': 'y_true'})\n",
    "\n",
    "y_true_rs = y_true_pooled_df.copy()['y_true']\n",
    "y_pred_rs = y_pred_pooled_df.copy()['y_pred']\n",
    "\n",
    "r2_scores = []\n",
    "pearson_corrs = []\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.read_csv(f'/PLS/brain/stacking/g/RS_IDP_Timeseries_best_metrics_target_pred_2nd_level_rf_test_fold_{fold}.csv')['g_pred_rs_idp_ts_best_stack_test']\n",
    "    y_true_fold = pd.read_csv(f'/PLS/brain/stacking/g/RS_Aug_ID_g/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "    r2_scores.append(r2)\n",
    "    corr, _ = pearsonr(y_true_fold.squeeze(), y_pred_fold.squeeze())\n",
    "    pearson_corrs.append(corr)\n",
    "\n",
    "# Annotate the plot with Pearson correlation and R score\n",
    "#corr, p = pearsonr(y_true, y_pred)\n",
    "r_mean_rs = np.mean(pearson_corrs)\n",
    "r_std_rs = np.std(pearson_corrs)\n",
    "r2_mean_rs = np.mean(r2_scores)\n",
    "r2_std_rs = np.std(r2_scores)\n",
    "\n",
    "################################### T1/T2\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "y_pred_pooled = []\n",
    "y_true_pooled = []\n",
    "\n",
    "for i,fold in enumerate(folds):\n",
    "    y_pred = pd.read_csv(f'/PLS/brain/stacking/g/T1_T2_whole_brain_target_pred_2nd_level_svr_test_fold_{fold}.csv')['g_pred_mribest_stack_test']\n",
    "    y_pred_pooled.append(y_pred)\n",
    "    y_pred_pooled_df = pd.DataFrame(pd.concat(y_pred_pooled, ignore_index=True)).rename(columns={'g_pred_mribest_stack_test': 'y_pred'})\n",
    "    y_true = pd.read_csv(f'/PLS/brain/stacking/g/T1_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    y_true_pooled.append(y_true)\n",
    "    y_true_pooled_df = pd.DataFrame(pd.concat(y_true_pooled, ignore_index=True)).rename(columns={'g_real_scaled': 'y_true'})\n",
    "\n",
    "y_true_t1 = y_true_pooled_df.copy()['y_true']\n",
    "y_pred_t1 = y_pred_pooled_df.copy()['y_pred']\n",
    "\n",
    "\n",
    "pearson_corrs = []\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.read_csv(f'/PLS/brain/stacking/g/T1_T2_whole_brain_target_pred_2nd_level_svr_test_fold_{fold}.csv')['g_pred_mribest_stack_test']\n",
    "    y_true_fold = pd.read_csv(f'/PLS/brain/stacking/g/T1_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "    r2_scores.append(r2)\n",
    "    corr, _ = pearsonr(y_true_fold.squeeze(), y_pred_fold.squeeze())\n",
    "    pearson_corrs.append(corr)\n",
    "    \n",
    "# Annotate the plot with Pearson correlation and R score\n",
    "#corr, p = pearsonr(y_true, y_pred)\n",
    "r_mean_t1 = np.mean(pearson_corrs)\n",
    "r_std_t1 = np.std(pearson_corrs)\n",
    "r2_mean_t1 = np.mean(r2_scores)\n",
    "r2_std_t1 = np.std(r2_scores)\n",
    "\n",
    "######################################## All\n",
    "folds = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "y_pred_pooled = []\n",
    "y_true_pooled = []\n",
    "\n",
    "for i,fold in enumerate(folds):\n",
    "    y_pred = pd.read_csv(f'/PLS/brain/stacking/g/All_modalities_target_pred_2nd_level_xgb_test_fold_{fold}.csv')['g_pred_mribest_stack_test']\n",
    "    y_pred_pooled.append(y_pred)\n",
    "    y_pred_pooled_df = pd.DataFrame(pd.concat(y_pred_pooled, ignore_index=True)).rename(columns={'g_pred_mribest_stack_test': 'y_pred'})\n",
    "    y_true = pd.read_csv(f'/PLS/brain/stacking/g/Stacked_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    y_true_pooled.append(y_true)\n",
    "    y_true_pooled_df = pd.DataFrame(pd.concat(y_true_pooled, ignore_index=True)).rename(columns={'g_real_scaled': 'y_true'})\n",
    "\n",
    "y_true_stack = y_true_pooled_df.copy()['y_true']\n",
    "y_pred_stack  = y_pred_pooled_df.copy()['y_pred']\n",
    "\n",
    "r2_scores = []\n",
    "pearson_corrs = []\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.read_csv(f'/PLS/brain/stacking/g/All_modalities_target_pred_2nd_level_xgb_test_fold_{fold}.csv')['g_pred_mribest_stack_test']\n",
    "    y_true_fold = pd.read_csv(f'/PLS/brain/stacking/g/Stacked_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled']\n",
    "    r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "    r2_scores.append(r2)\n",
    "    corr, _ = pearsonr(y_true_fold.squeeze(), y_pred_fold.squeeze())\n",
    "    pearson_corrs.append(corr)\n",
    "    \n",
    "# Annotate the plot with Pearson correlation and R score\n",
    "#corr, p = pearsonr(y_true, y_pred)\n",
    "r_mean_stack = np.mean(pearson_corrs)\n",
    "r_std_stack = np.std(pearson_corrs)\n",
    "r2_mean_stack = np.mean(r2_scores)\n",
    "r2_std_stack = np.std(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data to plot all scatterplots in one figure\n",
    "folds = ['0', '1', '2', '3', '4']\n",
    "folds_data = {\n",
    "    'dti': {'y_pred': [], 'y_true': []},\n",
    "    'rs': {'y_pred': [], 'y_true': []},\n",
    "    't1t2': {'y_pred': [], 'y_true': []},\n",
    "    'all': {'y_pred': [], 'y_true': []}\n",
    "}\n",
    "\n",
    "# DTI\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/DTI_All_target_pred_2nd_level_rf_test_fold_{fold}.csv')['g_pred_dti_all_stack_test'])\n",
    "    y_pred_fold.columns = ['y_pred']\n",
    "    y_true_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/DTI_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled'])\n",
    "    y_true_fold.columns = ['y_true']\n",
    "    folds_data['dti']['y_pred'].append(y_pred_fold)\n",
    "    folds_data['dti']['y_true'].append(y_true_fold)\n",
    "\n",
    "# RS\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/RS_IDP_Timeseries_best_metrics_target_pred_2nd_level_rf_test_fold_{fold}.csv')['g_pred_rs_idp_ts_best_stack_test'])\n",
    "    y_pred_fold.columns = ['y_pred']\n",
    "    y_true_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/RS_Aug_ID_g/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled'])\n",
    "    y_true_fold.columns = ['y_true']\n",
    "    folds_data['rs']['y_pred'].append(y_pred_fold)\n",
    "    folds_data['rs']['y_true'].append(y_true_fold)\n",
    "\n",
    "# T1/T2\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/T1_T2_whole_brain_target_pred_2nd_level_svr_test_fold_{fold}.csv')['g_pred_mribest_stack_test'])\n",
    "    y_pred_fold.columns = ['y_pred']\n",
    "    y_true_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/T1_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled'])\n",
    "    y_true_fold.columns = ['y_true']\n",
    "    folds_data['t1t2']['y_pred'].append(y_pred_fold)\n",
    "    folds_data['t1t2']['y_true'].append(y_true_fold)\n",
    "\n",
    "# All\n",
    "for fold in folds:\n",
    "    y_pred_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/All_modalities_target_pred_2nd_level_xgb_test_fold_{fold}.csv')['g_pred_mribest_stack_test'])\n",
    "    y_pred_fold.columns = ['y_pred']\n",
    "    y_true_fold = pd.DataFrame(pd.read_csv(f'/PLS/brain/stacking/g/Stacked_Aug_ID/g/target_real_test_feature_matched_scaled_fold_{fold}.csv')['g_real_scaled'])\n",
    "    y_true_fold.columns = ['y_true']\n",
    "    folds_data['all']['y_pred'].append(y_pred_fold)\n",
    "    folds_data['all']['y_true'].append(y_true_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatterplots and commonality analysis together\n",
    "fig = plt.figure(figsize=(22, 24))\n",
    "\n",
    "data_venn2 = [\n",
    "    (u_mh_dti_na.round(3), u_dti_na.round(3), c_mh_and_dti_na.round(3), '#B24745FF', '#00A1D599', 'dwMRI', \"$g$ ~\"),\n",
    "    (u_mh_rs_na.round(3), u_rs_na.round(3), c_mh_and_rs_na.round(3), '#B24745FF', '#79AF9799', 'rsMRI', \"$g$ ~\"),\n",
    "    (u_mh_t1_na.round(3), u_t1_na.round(3), c_mh_and_t1_na.round(3), '#B24745FF', '#DF8F4499', 'sMRI', \"$g$ ~\"),\n",
    "    (u_mh_mri_na.round(3), u_mri_na.round(3), c_mh_and_mri_na.round(3), '#B24745FF', '#6A659999', 'all MRI Stacked', \"$g$ ~\")\n",
    "]\n",
    "\n",
    "\n",
    "# Rearranged data for venn3\n",
    "data_venn3 = [\n",
    "    ((u_mh_dti.round(3), u_dti.round(3), c_mh_and_dti.round(3), u_demo_dti.round(3), c_mh_and_demo_dti.round(3), c_dti_and_demo.round(3), c_mh_and_dti_and_demo.round(3)),\n",
    "     ('#B24745FF', '#00A1D599', '#374E5599'), ('Mental Health', 'dwMRI', 'Age and Sex')),\n",
    "    ((u_mh_rs.round(3), u_rs.round(3), c_mh_and_rs.round(3), u_demo_rs.round(3), c_mh_and_demo_rs.round(3), c_rs_and_demo.round(3), c_mh_and_rs_and_demo.round(3)),\n",
    "     ('#B24745FF', '#79AF9799', '#374E5599'), ('Mental Health', 'rsMRI', 'Age and Sex')),\n",
    "    ((u_mh_t1.round(3), u_t1.round(3), c_mh_and_t1.round(3), u_demo_t1.round(3), c_mh_and_demo_t1.round(3), c_t1_and_demo.round(3), c_mh_and_t1_and_demo.round(3)),\n",
    "     ('#B24745FF', '#DF8F4499', '#374E5599'), ('Mental Health', 'T1w/T2w MRI', 'Age and Sex')),\n",
    "    ((u_mh_mri.round(3), u_mri.round(3), c_mh_and_mri.round(3), u_demo_mri.round(3), c_mh_and_demo_mri.round(3), c_mri_and_demo.round(3), c_mh_and_mri_and_demo.round(3)),\n",
    "     ('#B24745FF', '#6A659999', '#374E5599'), ('Mental Health', 'all MRI Stacked', 'Age&Sex'))\n",
    "]\n",
    " \n",
    "# Define the positions of each subplot\n",
    "positions = [\n",
    "    [0.05, 0.76, 0.28, 0.15], [0.45, 0.73, 0.28, 0.2], [0.73, 0.73, 0.25, 0.2],\n",
    "    [0.05, 0.505 , 0.28, 0.15], [0.45, 0.49 , 0.28, 0.2], [0.73, 0.49, 0.25, 0.2],\n",
    "    [0.05, 0.25, 0.28, 0.15], [0.45, 0.24, 0.28, 0.2], [0.73, 0.22, 0.25, 0.2],\n",
    "    [0.05, -0.004, 0.28, 0.15], [0.45, -0.013, 0.28, 0.2], [0.73, -0.013, 0.25, 0.2]\n",
    "]\n",
    "\n",
    "# Create subplots at the specified positions\n",
    "axes = []\n",
    "for pos in positions:\n",
    "    axes.append(fig.add_axes(pos))\n",
    "    \n",
    "# Plot scatter plots on the left side\n",
    "dticmap = sns.light_palette(\"#8cd0e5\", as_cmap=True)\n",
    "rscmap = sns.light_palette(\"#39ab7e\", as_cmap=True)\n",
    "t1cmap = sns.light_palette(\"#f8b976\", as_cmap=True)\n",
    "stackcmap = sns.light_palette(\"#826b88\", as_cmap=True)\n",
    "\n",
    "create_scatter_plot(axes[0], y_true_dti, y_pred_dti, '', dticmap, '#8cd0e5',\n",
    "                    r_mean_dti, r_std_dti, r2_mean_dti, r2_std_dti,\n",
    "                    folds_data, 'dti')\n",
    "axes[0].set_xlim(-2, 2)\n",
    "create_scatter_plot(axes[3], y_true_rs, y_pred_rs, '', rscmap, \"#39ab7e\",\n",
    "                    r_mean_rs, r_std_rs, r2_mean_rs, r2_std_rs,\n",
    "                    folds_data, 'rs')\n",
    "axes[3].set_xlim(-2, 2)\n",
    "create_scatter_plot(axes[6], y_true_t1, y_pred_t1, '', t1cmap, \"#f8b976\",\n",
    "                    r_mean_t1, r_std_t1, r2_mean_t1, r2_std_t1,\n",
    "                    folds_data, 't1t2')\n",
    "axes[6].set_xlim(-2, 2)\n",
    "create_scatter_plot(axes[9], y_true_stack, y_pred_stack, '', stackcmap, \"#826b88\",\n",
    "                    r_mean_stack, r_std_stack, r2_mean_stack, r2_std_stack,\n",
    "                    folds_data, 'all')\n",
    "axes[9].set_xlim(-2, 2)\n",
    "\n",
    "# Plot Venn diagrams on the right side\n",
    "for i in range(4):\n",
    "\n",
    "    # Venn diagrams for venn2\n",
    "    u_mh_dti_na, u_dti_na, c_mh_and_dti_na, color1, color2, label, title = data_venn2[i]\n",
    "    subset_sizes = (u_mh_dti_na, u_dti_na, c_mh_and_dti_na)\n",
    "    colors = (color1, color2)\n",
    "    venn = venn2(subsets=subset_sizes, set_labels=('', ''), set_colors=colors, ax=axes[i*3+1])\n",
    "\n",
    "    # Apply transparency and set line width\n",
    "    alphas = (0.20, 0.20)\n",
    "    for patch, alpha in zip(venn.patches, alphas):\n",
    "        if patch:\n",
    "            patch.set_alpha(alpha)\n",
    "            patch.set_edgecolor('white')\n",
    "            patch.set_linewidth(2)\n",
    "            patch.set_path_effects([path_effects.withStroke(linewidth=3, foreground='grey')])\n",
    "            \n",
    "    # Set font size for labels\n",
    "    for text in venn.set_labels:\n",
    "        text.set_fontsize(24)\n",
    "    for text in venn.subset_labels:\n",
    "        if text:\n",
    "            text.set_fontsize(24)\n",
    "            text.set_text(f'{text.get_text()}%')\n",
    "\n",
    "    # Move annotations within the circles\n",
    "    lab = venn.get_label_by_id('10') #left\n",
    "    if lab:\n",
    "        x, y = lab.get_position()\n",
    "        lab.set_position((x + 0.03, y))\n",
    "\n",
    "    lab = venn.get_label_by_id('01') #middle\n",
    "    if lab:\n",
    "        x, y = lab.get_position()\n",
    "        lab.set_position((x - 0.01, y))\n",
    "\n",
    "    # Adjust position of 'g~' in legend\n",
    "    if i == 0:\n",
    "        axes[i*3+1].set_title(title, fontsize=40, pad=3, y=1.0387, x=-0.35) # 1st\n",
    "    elif i == 1:\n",
    "        axes[i*3+1].set_title(title, fontsize=40, pad=3, y=1.105, x=-0.35) # 2nd\n",
    "    elif i == 3:\n",
    "        axes[i*3+1].set_title(title, fontsize=40, pad=3, y=0.98, x=-0.35) # the last legend\n",
    "    else:\n",
    "        # Keep the rest of the legends as they are\n",
    "        axes[i*3+1].set_title(title, fontsize=40, pad=3, y=1.095, x=-0.35)\n",
    "\n",
    "    # Create custom legend for each subplot\n",
    "    handles = [\n",
    "        patches.Patch(color=color1, alpha=0.3, label='Mental Health'),\n",
    "        patches.Patch(color=color2, alpha=0.3, label=label),\n",
    "        patches.Patch(color='#374E5599', alpha=0.3, label='Age&Sex')\n",
    "    ]\n",
    "    # Add legend elements\n",
    "    if i == 0:\n",
    "        axes[i*3+1].legend(handles=handles, loc='upper left', bbox_to_anchor=(-0.3, 1.23), ncol=3, fontsize=35, title='', frameon=False, columnspacing=0.5) # the first legend\n",
    "    elif i == 1:\n",
    "        axes[i*3+1].legend(handles=handles, loc='upper left', bbox_to_anchor=(-0.3, 1.31), ncol=3, fontsize=35, title='', frameon=False, columnspacing=0.5) # second row legend\n",
    "    elif i == 2:\n",
    "        axes[i*3+1].legend(handles=handles, loc='upper left', bbox_to_anchor=(-0.3, 1.30), ncol=3, fontsize=35, title='', frameon=False, columnspacing=0.5) # third row legend\n",
    "    elif i == 3:\n",
    "        axes[i*3+1].legend(handles=handles, loc='upper left', bbox_to_anchor=(-0.3, 1.16), ncol=3, fontsize=35, title='', frameon=False, columnspacing=0.5) # the last legend\n",
    "\n",
    "    #axes[i*3+1].set_aspect(0.7)\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    # Venn diagrams for venn3\n",
    "    subset_sizes, colors, labels = data_venn3[i]\n",
    "    venn = venn3(subsets=subset_sizes, set_labels=['', '', ''], set_colors=colors, ax=axes[i*3+2])\n",
    "\n",
    "    # Apply transparency and set line width\n",
    "    alphas = (0.20, 0.20, 0.20)\n",
    "    for patch, alpha in zip(venn.patches, alphas):\n",
    "        if patch:\n",
    "            patch.set_alpha(alpha)\n",
    "            patch.set_edgecolor('white')\n",
    "            patch.set_linewidth(2)\n",
    "            patch.set_path_effects([path_effects.withStroke(linewidth=3, foreground='grey')])\n",
    "\n",
    "            #t = transforms.Affine2D().rotate_deg(45) + axes[i*3+2].transData\n",
    "            #patch.set_transform(t)\n",
    "\n",
    "    # Set font size for labels\n",
    "    for text in venn.set_labels:\n",
    "        text.set_fontsize(24)\n",
    "    for text in venn.subset_labels:\n",
    "        if text:\n",
    "            text.set_fontsize(24)\n",
    "            text.set_text(f'{text.get_text()}%')\n",
    "\n",
    "    # Move annotations within the circles\n",
    "    lab = venn.get_label_by_id('100') #left\n",
    "    if lab:\n",
    "        x, y = lab.get_position()\n",
    "        lab.set_position((x + 0.05, y))\n",
    "\n",
    "        lab = venn.get_label_by_id('010') #left\n",
    "    if lab:\n",
    "        x, y = lab.get_position()\n",
    "        lab.set_position((x - 0.01, y))\n",
    "\n",
    "# Add letters to subplots\n",
    "labels = ['a', 'e', 'i', 'b', 'f', 'j', 'c', 'g', 'k', 'd', 'h', 'l']\n",
    "positions = [\n",
    "    (-0.4, 1.09), (-0.0, 0.98), (-0.0, 0.97),\n",
    "    (-0.4, 1.13), (-0.0, 1.0), (-0.0, 0.92),\n",
    "    (-0.4, 1.13), (-0.0, 0.985), (-0.0, 1.05),\n",
    "    (-0.4, 1.17), (-0.0, 0.94), (-0.0, 0.938)\n",
    "]\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    axes[i].text(positions[i][0], positions[i][1], labels[i], transform=axes[i].transAxes,\n",
    "                 fontsize=50,  fontweight='bold', va='top', ha='right')\n",
    "#plt.subplots_adjust(wspace=0.05, hspace=1.8)\n",
    "plt.tight_layout()  # Adjust the rect parameter to fit the legends more tightly rect=[0, 0, 1, 1]\n",
    "\n",
    "# Common y axis for scatterplots\n",
    "fig.text(-0.04, 0.5, '$g$-factor derived from ESEM ($z$)', va='center', rotation='vertical', fontsize=45)\n",
    "#fig.text(0.35, 0.8, 'dwMRI Stacked', ha='center', fontsize=20, bbox=dict(facecolor='none', edgecolor='#826b88', boxstyle='round', pad=0.5)) #fontweight='bold',\n",
    "\n",
    "axes[0].set_title('dwMRI Stacked', fontsize=33, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#8cd0e5', boxstyle='round', pad=0.2), y=1.242, x=0.42)\n",
    "axes[3].set_title('rsMRI Stacked', fontsize=33, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#39ab7e', boxstyle='round', pad=0.2), y=1.25, x=0.41)\n",
    "axes[6].set_title('sMRI Stacked', fontsize=33, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#f8b976', boxstyle='round', pad=0.2), y=1.258, x=0.40)\n",
    "axes[9].set_title('all MRI Stacked', fontsize=33, fontweight='bold', bbox=dict(facecolor='none', edgecolor='#826b88', boxstyle='round', pad=0.2), y=1.25, x=0.45)\n",
    "\n",
    "plt.savefig(\"/media/hcs-sci-psy-narun/IBu/Plots_and_Tables/final/svg/Fig5.svg\",\n",
    "            bbox_inches=\"tight\", \n",
    "            pad_inches=1, \n",
    "            transparent=False, \n",
    "            facecolor=\"w\", \n",
    "            edgecolor='w', \n",
    "            orientation='landscape',\n",
    "            format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute correlation between PLSR performance and % of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonality metrics PLS\n",
    "commonality_metrics_struct_pls = pd.read_csv('/commonality_analysis/var_explained_struct_full_renamed.csv').rename(columns={'Unnamed: 0':'Modality'})[['Modality', 'Var Exp MRI']]\n",
    "commonality_metrics_rs_pls = pd.read_csv('/commonality_analysis/var_explained_rs_full_renamed.csv').rename(columns={'Unnamed: 0':'Modality'})[['Modality', 'Var Exp MRI']]\n",
    "commonality_metrics_dti_pls = pd.read_csv('/commonality_analysis/var_explained_dti_full_renamed.csv').rename(columns={'Unnamed: 0':'Modality'})[['Modality', 'Var Exp MRI']]\n",
    "\n",
    "# Commonality metrics stack\n",
    "commonality_metrics_stack = pd.read_csv('/PLS/brain/commonality_analysis/commonality_metrics_stack.csv')[['Modality', 'Var Exp MRI']]\n",
    "\n",
    "## Model performance\n",
    "# Model performance PLS sMRI\n",
    "struct_perf_pls = pd.read_csv('/PLS/brain/performance/struct_perf_pls_mean.csv')[['Modality', 'R2', 'Pearson r']]\n",
    "# Model performance PLS RS\n",
    "rs_perf_pls = pd.read_csv('/PLS/brain/performance/rs_perf_pls_mean.csv')[['Modality', 'R2', 'Pearson r']]\n",
    "# Model performance PLS DTI\n",
    "dti_perf_pls = pd.read_csv('/PLS/brain/performance/dti_perf_pls_mean.csv')[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "\n",
    "# Model performance stack\n",
    "rs_perf_stack = pd.read_csv('/PLS/brain/stacking/RS_IDP_Timeseries_best_metrics_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "rs_perf_stack_rf = rs_perf_stack[rs_perf_stack['Algorithm']=='rf']\n",
    "rs_perf_stack_rf['Modality'] = 'rsMRI'\n",
    "rs_perf_stack_rf = rs_perf_stack_rf[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "dti_perf_stack = pd.read_csv('/PLS/brain/stacking/DTI_All_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "dti_perf_stack_rf = dti_perf_stack[dti_perf_stack['Algorithm']=='rf']\n",
    "dti_perf_stack_rf['Modality'] = 'dwMRI'\n",
    "dti_perf_stack_rf = dti_perf_stack_rf[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "struct_perf_stack = pd.read_csv('/PLS/brain/stacking/T1_T2_whole_brain_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "struct_perf_stack_svr = struct_perf_stack[struct_perf_stack['Algorithm']=='svr']\n",
    "struct_perf_stack_svr['Modality'] = 'T1w/T2w MRI'\n",
    "struct_perf_stack_svr = struct_perf_stack_svr[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "all_mod_perf_stack = pd.read_csv('/PLS/brain/stacking/All_modalities_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "all_mod_perf_stack = all_mod_perf_stack[all_mod_perf_stack['Algorithm']=='xgb']\n",
    "all_mod_perf_stack['Modality'] = 'Stacked MRI'\n",
    "all_mod_perf_stack = all_mod_perf_stack[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "## Merge results of predicion and commonality analysis\n",
    "# Stacked\n",
    "stack_perf = pd.concat([rs_perf_stack_rf, dti_perf_stack_rf, struct_perf_stack_svr, all_mod_perf_stack], axis=0)\n",
    "stack_res = pd.merge(stack_perf, commonality_metrics_stack, on = 'Modality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonality metrics PLS\n",
    "commonality_metrics_struct_pls = pd.read_csv('/commonality_analysis/var_explained_struct_full_renamed.csv').rename(columns={'Unnamed: 0':'Modality'})[['Modality', 'Var Exp MRI']]\n",
    "commonality_metrics_rs_pls = pd.read_csv('/commonality_analysis/var_explained_rs_full_renamed.csv').rename(columns={'Unnamed: 0':'Modality'})[['Modality', 'Var Exp MRI']]\n",
    "commonality_metrics_dti_pls = pd.read_csv('/commonality_analysis/var_explained_dti_full_renamed.csv').rename(columns={'Unnamed: 0':'Modality'})[['Modality', 'Var Exp MRI']]\n",
    "\n",
    "# Commonality metrics stack\n",
    "commonality_metrics_stack = pd.read_csv('/PLS/brain/commonality_analysis/commonality_metrics_stack.csv')[['Modality', 'Var Exp MRI']]\n",
    "\n",
    "## Model performance\n",
    "# Model performance PLS sMRI\n",
    "struct_perf_pls = pd.read_csv('/PLS/brain/performance/struct_perf_pls_mean.csv')[['Modality', 'R2', 'Pearson r']]\n",
    "# Model performance PLS RS\n",
    "rs_perf_pls = pd.read_csv('/PLS/brain/performance/rs_perf_pls_mean.csv')[['Modality', 'R2', 'Pearson r']]\n",
    "# Model performance PLS DTI\n",
    "dti_perf_pls = pd.read_csv('/PLS/brain/performance/dti_perf_pls_mean.csv')[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "\n",
    "# Model performance stack\n",
    "rs_perf_stack = pd.read_csv('/PLS/brain/stacking/RS_IDP_Timeseries_best_metrics_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "rs_perf_stack_rf = rs_perf_stack[rs_perf_stack['Algorithm']=='rf']\n",
    "rs_perf_stack_rf['Modality'] = 'rsMRI'\n",
    "rs_perf_stack_rf = rs_perf_stack_rf[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "dti_perf_stack = pd.read_csv('/PLS/brain/stacking/DTI_All_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "dti_perf_stack_rf = dti_perf_stack[dti_perf_stack['Algorithm']=='rf']\n",
    "dti_perf_stack_rf['Modality'] = 'dwMRI'\n",
    "dti_perf_stack_rf = dti_perf_stack_rf[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "struct_perf_stack = pd.read_csv('/PLS/brain/stacking/T1_T2_whole_brain_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "struct_perf_stack_svr = struct_perf_stack[struct_perf_stack['Algorithm']=='svr']\n",
    "struct_perf_stack_svr['Modality'] = 'T1w/T2w MRI'\n",
    "struct_perf_stack_svr = struct_perf_stack_svr[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "all_mod_perf_stack = pd.read_csv('/PLS/brain/stacking/All_modalities_stacked_five_folds.csv')[['Algorithm', 'MSE', 'MAE', 'R2', 'Pearson r']].groupby(['Algorithm']).mean().round(3).reset_index().sort_values(by='Pearson r', ascending=False)\n",
    "all_mod_perf_stack = all_mod_perf_stack[all_mod_perf_stack['Algorithm']=='xgb']\n",
    "all_mod_perf_stack['Modality'] = 'Stacked MRI'\n",
    "all_mod_perf_stack = all_mod_perf_stack[['Modality', 'R2', 'Pearson r']]\n",
    "\n",
    "## Merge results of predicion and commonality analysis\n",
    "# Stacked\n",
    "stack_perf = pd.concat([rs_perf_stack_rf, dti_perf_stack_rf, struct_perf_stack_svr, all_mod_perf_stack], axis=0)\n",
    "stack_res = pd.merge(stack_perf, commonality_metrics_stack, on = 'Modality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename PLS results to match commonality\n",
    "# Struct\n",
    "struct_rename_dict = {\n",
    "    \"Subcortical volumetric subsegmentation\": \"Subcortical Volumetric Subseg.\",\n",
    "    \"ASEG volume\": \"ASEG Volume\",\n",
    "    \"FSL FAST\": \"FSL FAST\",\n",
    "    \"Whole-brain T1/T2\": \"Whole-brain T1/T2\",\n",
    "    \"ASEG mean intensity\": \"ASEG Mean Thickness\",\n",
    "    \"Desikan grey/white matter intensity\": \"Desikan GM/WM Intensity\",\n",
    "    \"aparc a2009s volume\": \"aparc.a2009s Volume\",\n",
    "    \"aparc a2009s mean thickness\": \"aparc.a2009s Mean Thickness\",\n",
    "    \"Desikan-Killiany-Tourville volume\": \"DKT Volume\",\n",
    "    \"Desikan white matter mean thickness\": \"Desikan WM Mean Thickness\",\n",
    "    \"Desikan-Killiany-Tourville mean thickness\": \"DKT Mean Thickness\",\n",
    "    \"aparc a2009s area\": \"aparc.a2009s Area\",\n",
    "    \"Desikan white matter volume\": \"Desikan WM Volume\",\n",
    "    \"FSL FIRST\": \"FSL FIRST\",\n",
    "    \"Desikan pial\": \"Desikan Pial\",\n",
    "    \"BA ex-vivo volume\": \"BA ex-vivo Volume\",\n",
    "    \"BA ex-vivo mean thickness\": \"BA ex-vivo Mean Thickness\",\n",
    "    \"Desikan-Killiany-Tourville area\": \"DKT Area\",\n",
    "    \"Desikan white matter area\": \"Desikan WM Area\",\n",
    "    \"BA ex-vivo area\": \"BA ex-vivo Area\"\n",
    "}\n",
    "\n",
    "struct_perf_pls[\"Modality\"] = struct_perf_pls[\"Modality\"].map(struct_rename_dict)\n",
    "\n",
    "# RS\n",
    "rs_rename_dict = {\n",
    "    \"Tangent matrices 55 IC\": \"55 IC Func. Connectivity\",\n",
    "    \"Schaefer7n200p MSA I Full correlation\": \"Schaefer200-I Func. Connectivity\",\n",
    "    \"Schaefer7n500p MSA IV Full correlation\": \"Schaefer500-IV Func. Connectivity\",\n",
    "    \"Glasser MSA I Full correlation\": \"Glasser-I Func. Connectivity\",\n",
    "    \"Tangent matrices 21 IC\": \"21 IC Func. Connectivity\",\n",
    "    \"Glasser MSA IV Full correlation\": \"Glasser-IV Func. Connectivity\",\n",
    "    \"aparc MSA I Full correlation\": \"aparc-I Func. Connectivity\",\n",
    "    \"aparc a2009s MSA I Full correlation\": \"aparc.a2009s-I Func. Connectivity\",\n",
    "    \"Amplitudes 55 IC\": \"55 IC Amplitudes\",\n",
    "    \"Amplitudes 21 IC\": \"21 IC Amplitudes\"\n",
    "}\n",
    "rs_perf_pls[\"Modality\"] = rs_perf_pls[\"Modality\"].map(rs_rename_dict)\n",
    "\n",
    "# DTI\n",
    "dti_rename_dict = {\n",
    "    \"aparc MSA I Connectome Streamline Count\": \"aparc-I Streamline Count\",\n",
    "    \"aparc MSA I Connectome SIFT2\": \"aparc-I SIFT2\",\n",
    "    \"aparc a2009s MSA I Connectome Streamline Count\": \"aparc.a2009s-I Streamline Count\",\n",
    "    \"Schaefer7n200p MSA I Connectome Streamline Count\": \"Schaefer200-I Streamline Count\",\n",
    "    \"Schaefer7n200p MSA I Connectome SIFT2\": \"Schaefer200-I SIFT2\",\n",
    "    \"aparc MSA I Connectome FA\": \"aparc-I FA\",\n",
    "    \"aparc a2009s MSA I Connectome FA\": \"aparc.a2009s-I FA\",\n",
    "    \"aparc MSA I Connectome Mean Length\": \"aparc-I Mean Length\",\n",
    "    \"aparc a2009s MSA I Connectome SIFT2\": \"aparc.a2009s-I SIFT2\",\n",
    "    \"L2 TBSS\": \"L2 TBSS\",\n",
    "    \"Schaefer7n200p MSA I Connectome FA\": \"Schaefer200-I FA\",\n",
    "    \"aparc a2009s MSA I Connectome Mean Length\": \"aparc.a2009s-I Mean Length\",\n",
    "    \"Glasser MSA IV Connectome SIFT2\": \"Glasser-IV SIFT2\",\n",
    "    \"Glasser MSA IV Connectome Streamline Count\": \"Glasser-IV Streamline Count\",\n",
    "    \"L3 TBSS\": \"L3 TBSS\",\n",
    "    \"ICVF TBSS\": \"ICVF TBSS\",\n",
    "    \"Glasser MSA IV Connectome FA\": \"Glasser-IV FA\",\n",
    "    \"Schaefer7n500p MSA IV Connectome SIFT2\": \"Schaefer500-IV SIFT2\",\n",
    "    \"L1 TBSS\": \"L1 TBSS\",\n",
    "    \"Schaefer7n500p MSA IV Connectome Streamline Count\": \"Schaefer500-IV Streamline Count\",\n",
    "    \"FA TBSS\": \"FA TBSS\",\n",
    "    \"Glasser MSA I Connectome FA\": \"Glasser-I FA\",\n",
    "    \"Schaefer7n200p MSA I Connectome Mean Length\": \"Schaefer200-I Mean Length\",\n",
    "    \"MD TBSS\": \"MD TBSS\",\n",
    "    \"MD Probabilistic\": \"MD Prob.\",\n",
    "    \"Glasser MSA I Connectome Streamline Count\": \"Glasser-I Streamline Count\",\n",
    "    \"L3 Probabilistic\": \"L3 Prob.\",\n",
    "    \"Schaefer7n500p MSA IV Connectome FA\": \"Schaefer500-IV FA\",\n",
    "    \"Glasser MSA I Connectome SIFT2\": \"Glasser-I SIFT2\",\n",
    "    \"L2 Probabilistic\": \"L2 Prob.\",\n",
    "    \"L1 Probabilistic\": \"L1 Prob.\",\n",
    "    \"ISOVF TBSS\": \"ISOVF TBSS\",\n",
    "    \"Glasser MSA IV Connectome Mean Length\": \"Glasser-IV Mean Length\",\n",
    "    \"Glasser MSA I Connectome Mean Length\": \"Glasser-I Mean Length\",\n",
    "    \"OD TBSS\": \"OD TBSS\",\n",
    "    \"ICVF Probabilistic\": \"ICVF Prob.\",\n",
    "    \"MO TBSS\": \"MO TBSS\",\n",
    "    \"ISOVF Probabilistic\": \"ISOVF Prob.\",\n",
    "    \"FA Probabilistic\": \"FA Prob.\",\n",
    "    \"Schaefer7n500p MSA IV Connectome Mean Length\": \"Schaefer500-IV Mean Length\",\n",
    "    \"OD Probabilistic\": \"OD Prob.\",\n",
    "    \"MO Probabilistic\": \"MO Prob.\"\n",
    "}\n",
    "\n",
    "dti_perf_pls[\"Modality\"] = dti_perf_pls[\"Modality\"].map(dti_rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_pls_res = pd.merge(dti_perf_pls, commonality_metrics_dti_pls, on = 'Modality')\n",
    "rs_pls_res = pd.merge(rs_perf_pls, commonality_metrics_rs_pls, on = 'Modality')\n",
    "struct_pls_res = pd.merge(struct_perf_pls, commonality_metrics_struct_pls, on = 'Modality')\n",
    "\n",
    "pls_res = pd.concat([struct_pls_res, dti_pls_res, rs_pls_res], axis=0)\n",
    "all_res = pd.concat([stack_res, pls_res], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, p_value = pearsonr(pls_res['Pearson r'], pls_res['Var Exp MRI'])\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_ci(bootstrap_distribution, confidence_level=0.95):\n",
    "    \"\"\"Compute confidence intervals with input validation\"\"\"\n",
    "    alpha = 1 - confidence_level\n",
    "    lower = np.percentile(bootstrap_distribution, (alpha / 2) * 100).round(3)\n",
    "    upper = np.percentile(bootstrap_distribution, (1 - alpha / 2) * 100).round(3)\n",
    "    return (lower, upper)\n",
    "\n",
    "\n",
    "def save_dataframe_to_files(df, pathname, filename):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to both CSV and Excel formats in the specified location.\n",
    "    Creates directories if they don't exist.\n",
    "    \"\"\"\n",
    "    # Ensure the filename doesn't include extensions\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(pathname, exist_ok=True)\n",
    "    \n",
    "    # Create full paths\n",
    "    csv_path = os.path.join(pathname, f\"{base_name}.csv\")\n",
    "    excel_path = os.path.join(pathname, f\"{base_name}.xlsx\")\n",
    "    \n",
    "    try:\n",
    "        # Save files\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        df.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "        \n",
    "        print(f\"Files saved successfully:\\n- {csv_path}\\n- {excel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving files: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_pearson_correlation(df, x_col='Pearson r', y_col='Var Exp MRI', n_bootstraps=5000, ci_level=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence intervals for Pearson correlation between two columns\n",
    "    \n",
    "    Parameters:\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe containing the variables\n",
    "    x_col, y_col : str\n",
    "        Column names for the two variables\n",
    "    n_bootstraps : int\n",
    "        Number of bootstrap samples (default: 5000)\n",
    "    ci_level : float\n",
    "        Confidence level (default: 0.95)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    dict : Contains original correlation, p-value, CI, and full bootstrap distribution\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    x = df[x_col].values\n",
    "    y = df[y_col].values\n",
    "    n = len(x)\n",
    "    \n",
    "    # Original correlation\n",
    "    original_r, original_p = pearsonr(x, y)\n",
    "    \n",
    "    # Bootstrap distribution\n",
    "    boot_samples = np.zeros(n_bootstraps)\n",
    "    for i in range(n_bootstraps):\n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        boot_samples[i] = pearsonr(x[idx], y[idx])[0]\n",
    "    \n",
    "    # Calculate CI using your existing function\n",
    "    ci_lower, ci_upper = get_bootstrap_ci(boot_samples, confidence_level=ci_level)\n",
    "    \n",
    "    # Prepare results in your standard format\n",
    "    results = {\n",
    "        'Modality': 'Combined',  # Or specify as needed\n",
    "        'Metric': 'r',\n",
    "        'Mean': np.mean(boot_samples).round(3),\n",
    "        'Median': np.median(boot_samples).round(3),\n",
    "        'Std': np.std(boot_samples).round(3),\n",
    "        'CI_lower': ci_lower,\n",
    "        'CI_upper': ci_upper,\n",
    "        'Original_r': original_r,\n",
    "        'Original_p': original_p,\n",
    "        'bootstrap_dist': boot_samples\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage with your data:\n",
    "bootstrap_results = bootstrap_pearson_correlation(\n",
    "    pls_res,\n",
    "    x_col='Pearson r',\n",
    "    y_col='Var Exp MRI',\n",
    "    n_bootstraps=5000,\n",
    "    ci_level=0.95,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for consistency with your workflow\n",
    "results_df = pd.DataFrame([{\n",
    "    'Modality': bootstrap_results['Modality'],\n",
    "    'Metric': bootstrap_results['Metric'],\n",
    "    'Mean': bootstrap_results['Mean'],\n",
    "    'Median': bootstrap_results['Median'],\n",
    "    'Std': bootstrap_results['Std'],\n",
    "    'CI_lower': bootstrap_results['CI_lower'],\n",
    "    'CI_upper': bootstrap_results['CI_upper']\n",
    "}])\n",
    "\n",
    "# Save results using your existing function\n",
    "save_dataframe_to_files(\n",
    "    results_df,\n",
    "    pathname='/media/hcs-sci-psy-narun/IBu/Articles-Conferences/Articles/eLife/rev1-analysis/bootstrap',\n",
    "    filename='bootstrap_prediction_commonality'\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Bootstrap Results for Pearson r ~ Var Exp MRI ===\")\n",
    "print(f\"Original correlation: {bootstrap_results['Original_r']:.3f}\")\n",
    "print(f\"Original p-value: {bootstrap_results['Original_p']:.4f}\")\n",
    "print(f\"Bootstrapped 95% CI: [{bootstrap_results['CI_lower']:.3f}, {bootstrap_results['CI_upper']:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of correlation\n",
    "colors = []\n",
    "for modality in pls_res['Modality'].to_list():\n",
    "    if modality in struct_pls_res['Modality'].to_list():\n",
    "        colors.append('#DF8F44FF') #DF8F4499\n",
    "    elif modality in dti_pls_res['Modality'].to_list():\n",
    "        colors.append('#5C88DAFF') #00A1D599\n",
    "    elif modality in rs_pls_res['Modality'].to_list():\n",
    "        colors.append('#2C715FFF') #79AF9799\n",
    "    else:\n",
    "        colors.append('black')\n",
    "\n",
    "#'#5C88DAFF', '#2C715FFF', '#DF8F44FF'\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.scatter(pls_res['Pearson r'], pls_res['Var Exp MRI'], s=70, alpha=0.7, linewidth=1, c=colors)\n",
    "slope, intercept = np.polyfit(pls_res['Pearson r'], pls_res['Var Exp MRI'], 1)\n",
    "reg_line = slope * pls_res['Pearson r'] + intercept\n",
    "plt.plot(pls_res['Pearson r'], reg_line, color='red', label='Regression line', linewidth=0.5)\n",
    "plt.xlabel('Performance of a neuroimaging phenotype\\n in predicting cognition (Pearson $r$)', fontsize=18)\n",
    "plt.ylabel('Proportion of the\\ncognition-mental health relationship\\ncaptured by a neuroimaging phenotype (%)', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.text(0.17, 22, '$r$=0.97', fontsize=23, color='black') \n",
    "plt.grid(False)\n",
    "\n",
    "\n",
    "plt.savefig(\"/media/hcs-sci-psy-narun/IBu/Articles-Conferences/Articles/eLife/figures/Fig4a.png\",\n",
    "            bbox_inches =\"tight\", \n",
    "            pad_inches = 1, \n",
    "            transparent = False, \n",
    "            facecolor =\"w\", \n",
    "            edgecolor ='w', \n",
    "            orientation ='landscape',\n",
    "            format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographics distribution in commonality analysis samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dwMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_dti_demo_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_dti_rf = pd.read_csv(f'/PLS/brain/stacking/g/DTI_All_target_pred_2nd_level_rf_test_fold_{fold}.csv')\n",
    "    g_pred_mh = g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_dti_rf, on='eid').merge(demo, on='eid')\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_dti_all_stack_test': 'g_pred_dti'})\n",
    "    all_g_mh_dti_demo_concat.append(all_g)\n",
    "    all_g_mh_dti_demo = pd.concat(all_g_mh_dti_demo_concat, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rsMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_rs_demo_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_rs_idp_rf = pd.read_csv(f'/PLS/brain/stacking/g/RS_IDP_Timeseries_best_metrics_target_pred_2nd_level_rf_test_fold_{fold}.csv')\n",
    "    g_pred_mh = g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_rs_idp_rf, on='eid').merge(demo, on='eid') #.drop(columns=['eid'])\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_rs_idp_ts_best_stack_test': 'g_pred_rs_idp'})\n",
    "    all_g_mh_rs_demo_concat.append(all_g)\n",
    "    all_g_mh_rs_demo = pd.concat(all_g_mh_rs_demo_concat, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_t1_demo_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_t1_svr = pd.read_csv(f'/PLS/brain/stacking/g/T1_T2_whole_brain_target_pred_2nd_level_svr_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_t1_svr, on='eid').merge(demo, on='eid') #.drop(columns=['eid'])\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_mribest_stack_test': 'g_pred_t1'}) #the column was incorrectly names g_pred_mribest_stack_test instead of g_pred_t1_stack_test\n",
    "    all_g_mh_t1_demo_concat.append(all_g)\n",
    "    all_g_mh_t1_demo = pd.concat(all_g_mh_t1_demo_concat, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g_mh_mri_demo_concat = []\n",
    "for fold in folds:\n",
    "    g_pred_mri_all = pd.read_csv(f'/PLS/brain/stacking/g/All_modalities_target_pred_2nd_level_xgb_test_fold_{fold}.csv')\n",
    "    g_pred_mh = pd.concat([pd.read_csv(f'/mental_health/folds/fold_{fold}/g_pred/g_pred_mh_fold_{fold}.csv'), pd.read_csv(f'/mental_health/folds/fold_{fold}/suppl/g_test_matched_id_fold_{fold}.csv')], axis=1)\n",
    "    all_g = pd.read_csv(f'/PLS/g_factor/g_test_with_id_fold_{fold}.csv').merge(g_pred_mh, on='eid').merge(g_pred_mri_all, on='eid').merge(demo, on='eid')\n",
    "    all_g = all_g.rename(columns={'g': 'g_real', 'g_pred_mribest_stack_test': 'g_pred_mri'})\n",
    "    all_g_mh_mri_demo_concat.append(all_g)\n",
    "    all_g_mh_mri_demo = pd.concat(all_g_mh_mri_demo_concat, axis=0) #, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_confounds_min = pd.read_csv('/ML_DATASETS/basic_confounds_min.csv')[['Sex', 'Age when attended assessment centre', 'eid']]\n",
    "age_sex = pd.DataFrame(basic_confounds_min)\n",
    "age_sex.columns = ['Sex', 'Age', 'eid']\n",
    "commonality_dw_age_sex = all_g_mh_dti_demo.copy()\n",
    "commonality_rs_age_sex = all_g_mh_rs_demo.copy()\n",
    "commonality_str_age_sex = all_g_mh_t1_demo.copy()\n",
    "commonality_mri_age_sex = all_g_mh_mri_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sample sizes for each modality\n",
    "#dwMRI\n",
    "print('dwMRI: Sample size', commonality_dw_age_sex.shape[0])\n",
    "print('dwMRI: Mean age', commonality_dw_age_sex['Age'].mean().round(2))\n",
    "print(f\"dwMRI: SD age {commonality_dw_age_sex['Age'].std():.3f}\")\n",
    "print('dwMRI: Age when attended assessment centre, mean:', commonality_dw_age_sex['Age'].mean().round(2), 'SD:', f\"{commonality_dw_age_sex['Age'].std():.3f}\")\n",
    "print('dwMRI: Age max range:', commonality_dw_age_sex['Age'].max())\n",
    "print('dwMRI: Age min range:', commonality_dw_age_sex['Age'].min())\n",
    "print('dwMRI: Proportion of males:', (commonality_dw_age_sex['Sex'].value_counts()[1] / len(commonality_dw_age_sex['Sex']) * 100).round(2))\n",
    "print('dwMRI: Proportion of females:', (commonality_dw_age_sex['Sex'].value_counts()[0] / len(commonality_dw_age_sex['Sex']) * 100).round(2))\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "#rsMRI\n",
    "print('rsMRI: Sample size', commonality_rs_age_sex.shape[0])\n",
    "print('rsMRI: Mean age', commonality_rs_age_sex['Age'].mean().round(2))\n",
    "print(f\"rsMRI: SD age {commonality_rs_age_sex['Age'].std():.3f}\")\n",
    "print('rsMRI: Age when attended assessment centre, mean:', commonality_rs_age_sex['Age'].mean().round(2), 'SD:', f\"{commonality_rs_age_sex['Age'].std():.3f}\")\n",
    "print('rsMRI: Age max range:', commonality_rs_age_sex['Age'].max())\n",
    "print('rsMRI: Age min range:', commonality_rs_age_sex['Age'].min())\n",
    "print('rsMRI: Proportion of males:', (commonality_rs_age_sex['Sex'].value_counts()[1] / len(commonality_rs_age_sex['Sex']) * 100).round(2))\n",
    "print('rsMRI: Proportion of females:', (commonality_rs_age_sex['Sex'].value_counts()[0] / len(commonality_rs_age_sex['Sex']) * 100).round(2))\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "#sMRI\n",
    "print('sMRI: Sample size', commonality_str_age_sex.shape[0])\n",
    "print('sMRI: Mean age', commonality_str_age_sex['Age'].mean().round(2))\n",
    "print(f\"sMRI: SD age {commonality_str_age_sex['Age'].std():.3f}\")\n",
    "print('sMRI: Age when attended assessment centre, mean:', commonality_str_age_sex['Age'].mean().round(2), 'SD:', f\"{commonality_str_age_sex['Age'].std():.3f}\")\n",
    "print('sMRI: Age max range:', commonality_str_age_sex['Age'].max())\n",
    "print('sMRI: Age min range:', commonality_str_age_sex['Age'].min())\n",
    "print('sMRI: Proportion of males:', (commonality_str_age_sex['Sex'].value_counts()[1] / len(commonality_str_age_sex['Sex']) * 100).round(2))\n",
    "print('sMRI: Proportion of females:', (commonality_str_age_sex['Sex'].value_counts()[0] / len(commonality_str_age_sex['Sex']) * 100).round(2))\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "#MRI\n",
    "print('MRI: Sample size', commonality_mri_age_sex.shape[0])\n",
    "print('MRI: Mean age', commonality_mri_age_sex['Age'].mean().round(2))\n",
    "print(f\"MRI: SD age {commonality_mri_age_sex['Age'].std():.3f}\")\n",
    "print('MRI: Age when attended assessment centre, mean:', commonality_mri_age_sex['Age'].mean().round(2), 'SD:', f\"{commonality_mri_age_sex['Age'].std():.3f}\")\n",
    "print('MRI: Age max range:', commonality_mri_age_sex['Age'].max())\n",
    "print('MRI: Age min range:', commonality_mri_age_sex['Age'].min())\n",
    "print('MRI: Proportion of males:', (commonality_mri_age_sex['Sex'].value_counts()[1] / len(commonality_mri_age_sex['Sex']) * 100).round(2))\n",
    "print('MRI: Proportion of females:', (commonality_mri_age_sex['Sex'].value_counts()[0] / len(commonality_mri_age_sex['Sex']) * 100).round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukbiobank_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
